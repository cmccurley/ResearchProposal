\chapter{Introduction}
\vspace{1cm}
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \cite{hackett1990multisensorfusion,zhang2010multisourceremotingsensingfusion}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  For example, consider the scenario described by Du in \cite{Du2017Thesis} where hyperspectral (HSI) and LiDAR (light detection and ranging) sensors are used for scene understanding. Hyperspectral cameras can provide a broad range of spectral information about the materials present in a scene, while LiDAR can provide depth information.  If a road and a building rooftop are built with the same material (e.g. asphalt), hyperspectral information alone may not be able to discriminate between the two. However, incorporating elevation information provided by the corresponding LiDAR depth-map would greatly facilitate the classification problem.  Alternatively, paved and dirt biking trails might lay at the same elevation, and using LiDAR data alone may not be sufficient to distinguish between the two types of roads.  However, the spectral characteristics given by the HSI camera could easily determine an area of ground covered in dirt and another filled by asphalt or cement.  

Additionally, data fusion can provide confidence bounds on sensor's collections \cite{hackett1990multisensorfusion,Mohandes2018ClassifierCombinationTechniquesReview}.  For example, HSI sensors measure the radiance across a broad range of wavelengths at varying ground locations.  The measured radiance is a combination of radiation that is reflected and/or emitted by materials on the ground.  This inherently implies that environmental characteristics such as atmospheric conditions and diurnal cycle affect the amount of radiance that can be captured by the sensor \cite{Zare2008Thesis}.  Therefore, incorporation of meta-data such as time-of-day and weather conditions can provide bounds on the uncertainty contained in a given measurement and reinforce when to trust readings from particular spectral bands (i.e. IR during night).   These examples demonstrate how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce measurement uncertainty or to obtain a more complete understanding of a scene, thus aiding in classification or segmentation decisions.
\newline

Information fusion approaches make two typical assumptions: (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered \cite{Shen2016SpatioTemporalSpectralFusion}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \cite{hackett1990multisensorfusion,Butenuth2007HeterogeneousGeospatialData}.  Assumption (2) states that precise training labels are available for each data point \cite{Du2017Thesis}. 
\newline 

Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial, spectral, or temporal resolutions, it is usually non-trivial to convert all data to the same resolution or to map to the same grid and existing co-registration approaches often result in loss of sensor-specific information \cite{Shen2016SpatioTemporalSpectralFusion,Brigot2016CoregistrationForestRemoteSensingImages}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process since high-level information such as time-of-day, weather, environmental-setting (e.g. urban or forest), etc., would likely be collected at different spatial or temporal rates and would probably not align easily.  Typical approaches would address this problem by throwing out data points to match the lowest sampling rate.  However, this mapping is often noisy and results in loss of sensor-specific information.  This loss is often detrimental since classification improvement may only be achieved if the total information uncertainty in a problem is reduced \cite{Ruta2000OverviewClassifierFusionMethods}.  Moreover, it becomes difficult to distinguish between errors which evolve during the registration process and actual physical differences in a scene \cite{Zitova2003SurveyImageRegistrationMethods}.
\newline 

Additionally, even assuming that there is a noiseless/ lossless way to co-register heterogeneous data, standard supervised learning methods require accurate labels for each training data point.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \cite{Du2018MultiResolutionSensorFusion,Du2019MIChoquetIntegral}.  For example, when attempting to detect explosive ordinance with a hand-held electromagnetic induction sensor (metal detector), it is difficult to estimate the expected response from a target due to variations in target size, soil conditions, etc.   Even if the true size and location of a target is known, estimating the spatial expanse of the response is nontrivial and subject to inaccuracies which could be detrimental to the algorithmic training process.  Additionally, labeling high-resolution imagery on the sample-level is tedious, time-consuming, and costly in terms of resources.  There is often an unrealistic assumption that labelers are domain experts.  Even among experts, label boundaries are often subjective and varying levels of uncertainty in the true class exist between different labelers.  These effects make it very difficult (potentially even impossible) to obtain exact, sample-level labels for a training target, and thus add an additional level of geometric uncertainty to the sensor fusion process.  Therefore, fusion methods should be developed which consider this label ambiguity.  
\newline

One method for fusion of heterogeneous data sources which has gained popularity in previous years is \textit{manifold alignment} (MA) \cite{Hong2019LearnableManifoldAlignment}.  The goal of MA is to match two or more seemingly disparate datasets by mapping them to a joint latent space, while both preserving the qualities of each dataset and highlighting their similarities \cite{Wang2011ManifoldAlignment,Liao2016ManAlignmentHSI,Stanley2019ManAlignmentFeatureCorrespondence}.  This mapping creates a joint feature space which can then be utilized by more traditional learning algorithms \cite{Hong2019LearnableManifoldAlignment}.  The underlying assumption of MA is that, while datasets might capture different qualities presented in a scene, objects under observation should demonstrate similar latent properties independent of modality.  Thus, a single manifold can be discovered from multiple sensors which accurately describes the information present in a scene. \cite{Shen2018ManifoldSensorFusionImageData}.  While MA has been explored for domain adaptation and transfer learning, few studies have investigated MA applied to remote sensing applications \cite{Wang2010MultiscaleManAlignment,Wang2011HeteroDomainAdaptationManAlignment,Yang2016ManifoldAlignmentMultitemporalHSI}.  Furthermore, MA suffers from the same problems mentioned previously.  Traditional methods for MA assume that accurate correspondence between datasets is available \cite{Wang2011ManifoldAlignment}.  While unsupervised MA techniques have been proposed, results demonstrate the advantage of incorporating correspondence points \cite{Wang2010MultiscaleManAlignment, Stanley2019ManAlignmentFeatureCorrespondence, Hong2019LearnableManifoldAlignment}.  It was shown in \cite{Wang2011HeteroDomainAdaptationManAlignment} that MA can be extended to incorporate label correspondence instead of instance-level pairs.  However, in many remote sensing scenarios we do not know the correspondence between data points and accurate labels for supervised learning might not be available. This uncertainty should be incorporated in the model \cite{Damianou2017ManifoldAlignmentDifferentDataView}. In agreement, \cite{Liao2016ManAlignmentHSI} recently noted that completely accurate correspondence might not be needed for MA.  This poses the question, \textit{can we take advantage of weak labels to generate correspondence points?}  Additionally, \textit{if the purpose of fusion is to ultimately perform a classification task, this suggests that label information should be used to drive class representations further from each other in the latent space} \cite{Yang2016ManifoldAlignmentMultitemporalHSI}.
\newline

To address these two points, I propose the following.  During this project, techniques will be explored for use in multi-sensor, multi-resolution information fusion given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for fusion and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, LiDAR and more.  The aim of this project is to develop fusion methods which can address mis-registration between sensor sources as well as uncertainty and imprecision in training data groundtruth while demonstrating robustness towards outlying and adversarial data points. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Can a joint-representation space be developed between multiple sensors through incorporation of weakly and ambiguously labeled data?  A method for generating correspondence points from weak labels will be proposed and a manifold alignment method will be adapted to incorporate the weak correspondences.  How does this data alignment compare to state-of-the-art manifold and graph isomorphism-based alignment methods?
	\item Supervised and semi-supervised manifold learning has proven effective at discovering low-dimensional data representations which provide adequate class separation in the latent space.  However, current manifold learning procedures do not consider data which is weakly or ambiguously labeled.  To address this gap in the literature, a method for weakly-supervised manifold learning will be developed and applied in conjunction to the manifold alignment procedure proposed in Objective 1.  How does this method of manifold construction compare to state-of-the-art manifold learning techniques?
	\item Fusion methods such as the Choquet integral, deep learning and hierarchical mixture of experts often utilize individual processing pipelines on each sensor before combining on the sample, feature or decision level.  Will a single, sensor-agnostic processing pipeline on the unified representation (developed in the previous objectives) obtain comparable detection/ segmentation results to these alternative approaches?
\end{enumerate} 

Experiments will be conducted on both synthetic data and real applications such as target detection and scene understanding in remote sensing imagery, plant phenotyping, and semantic segmentation.  Datasets will include the DSIAC ATR Algorithm Development Database, MUUFL Gulfport, and Danforth Plant Science Center Manifold Learning datasets.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.

