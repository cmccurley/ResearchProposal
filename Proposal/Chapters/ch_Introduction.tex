\chapter{Introduction}
\vspace{1cm}\noindent --(\textbf{Motivation for Sensor Fusion}) \newline
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \textbf{CITE}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  Take the task of automatic target recognition (ATR) as an example, where the goal is to locate armored vehicles in a scene subject to varying environmental conditions.  Imagery can be obtained from multiple sensors such as mid-wave infrared (MWIR) and hyperspectral (HSI) cameras.  Hyperspectral can provide a broad range of spectral information about the materials present in a scene, while MWIR imaging sensors can supply thermal information.  If a building and vehicle are both constructed from the same material (i.e. metal), hyperspectral information alone may not be sufficient to tell them apart.  However, MWIR supplying thermal information can easily distinguish a vehicle whose engine was recently running from a cold building.  Alternatively, MWIR will likely fail during the day when temperatures are elevated for the entire scene, whereas HSI will likely operate better since it can obtain more reflectance.  This example demonstrates how it would be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce the uncertainty in a measurement or obtain a more complete understanding of a scene which can then be used to improve classification or segmentation decisions.
\newline
\vspace{1cm}\noindent --(\textbf{Current Assumptions/ Issues}) \newline
Information fusion approaches make two typical assumptions. (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered (matched together) \textbf{CITE}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \textbf{CITE}.  Assumption (2) states that training labels are available for each data point \textbf{CITE}. 
\newline \vspace{1cm} \noindent --(\textbf{Loss of information, noisy transformations}) \newline
Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial and temporal resolutions, it is not necessarily feasible to convert all data to the same resolution or to map to the same grid \textbf{CITE} and existing co-registration approaches often result in loss of sensor-specific information \textbf{CITE}.  In the previous ATR example, it was observed that the MWIR and HSI sensors would demonstrate varying levels of operation based on environmental conditions.  It is then intuitive that external meta-data should also be incorporated to provide confidence bounds on each sensor's measurement capabilities.  This, of course, adds an additional level of difficulty to the fusion process.  Where just two sensors of varying resolutions were mapped to the same grid, we now have to incorporate time-of-day, weather, ground temperature, etc., all of which likely operate with different spatial or temporal rates and may or may not align easily.  Typical approaches would throw out data points to match the lowest sampling rate \textbf{CITE}.  This mapping is often noisy and results in loss of sensor-specific information.
\newline 
\vspace{1cm}\noindent --(\textbf{Physical limitations, cost of labeling}) \newline
Additionally, even assuming that there is a noiseless/ lossless way to co-register heterogeneous data, standard supervised learning methods require accurate labels for each training data point\textbf{CITE}.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \textbf{CITE}.  For example, when attempting to detect explosive ordinance with a hand-held electromagnetic induction sensor (metal detector), it is difficult to estimate the expected response from a target due to variations in target size, soil conditions, etc.   Additionally, supplementary Global Positioning Systems (GPS) are only accurate on the level of several meters. These effects make it very difficult (potentially even impossible) to obtain exact, sample-level labels for a training target, and thus add an additional level of geometric uncertainty to the sensor fusion process.  
\newline
\vspace{1cm}\noindent --(\textbf{Statement of Work}) \newline
To address these two problems, I propose the following.  During this project, techniques will be explored for use in multi-sensor target detection, target classification, and information fusion given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for fusion and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, as well as LiDaR, ground-penetrating radar and electromagnetic induction sensors.  The aim of this project is to develop fusion methods which can address mis-registration between sensor sources as well as uncertainty and imprecision in training data groundtruth while demonstrating robustness towards outlying and adversarial data points. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Can manifold learning be extended to operate under the multiple-instance framework? If so, what is the ``best'' way to construct the data manifold?  Can manifold learning provide robustness to outlying and adversarial exemplars?
	\item Can we construct a joint-representation space for multiple sensors such that there is no loss of information amongst any of the modalities?  What is the ``best'' way to construct this representation?  Do we use raw data or perform feature extraction before combination?
	\item Can we perform detection/ segmentation using a single, sensor-agnostic processing pipeline on the unified representation?  How do we obtain representative, quantitative evaluation of performance?
\end{enumerate} 

Experiments will be conducted on both synthetic data and real applications such as plant phenotyping, as well as target detection and scene understanding in remote sensing imagery. Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.

%As an example, it is common in automatic target recognition tasks to utilize both mid-wave infrared (MWIR) and visible-spectrum imagery.  MWIR is able to detect heat in a scene, and while this is an appropriate choice for detecting warmed objects in cold weather or during the nighttime, it often fails during the day when an entire scenes' temperature is elevated.  On the other hand, visible-spectrum imagery can supply highly accurate information in the daytime, but typically fails under poor lighting conditions exhibited during the night.  These two sensors provide complementary information which, when used in the correct context, can greatly increase the performance of an ATR system.  Additionally, auxiliary sensors such as light, temperature, time-of-day, etc. can be used to give confidence values for trusting each sensor's collection.  In this example, each sensor operates under different spatial or temporal resolutions, thus nullifying the homogeneity assumption.

%In existing approaches, is extracted through sensor-specific pipelines before fusion on the sample, feature, or decision level.  There is an assumption under this approach that each pipeline extracts information with an adequate level of precision.  However, overall performance is sensitive to each sensors' processing and information is often lost through co-registration.  As an example, consider a self-driving car outfitted with an RGB camera and LiDAR (light detection and ranging) system. The goal of this computer vision system is to understand the scene around the vehicle so it may navigate safely.  Classical approaches would perform pre-processing, feature extraction, and segmentation/ depth-map estimation on each sensor, individually.  Co-registration would then be performed by mapping a single measurement from the dense LiDAR point-cloud to every pixel.  However, if the wrong LiDAR point is assigned to a pixel, even just a small shift, the car's depth perception can be thrown off.  This is information loss we cannot afford.  A more intuitive method for fusion would be to first project each sensors' collection into a combined representation space before running on a unified pipeline such that the entirety of each sensor's collection is utilized.

%Consider a real scene understanding task where an image is segmented into separate regions containing sky, grass, armored vehicle, mountains, etc.  Existing SOA segmentation techniques require pixel-level labels for each image in the training set.  Obtaining this detail of labeling (especially on a large scale) would cost a labeler or team of labelers a significant amount of time, effort, and money.  The field of multiple-instance learning (MIL) relaxes sample-level label requirements to the region or even image level and has potential to alleviate the cost of labeling, as well as handle spatial and temporal inaccuracies in the collection.  To address these two assumptions, the following is proposed.
