\chapter{Introduction}
\vspace{1cm}


%Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \cite{hackett1990multisensorfusion,zhang2010multisourceremotingsensingfusion}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  For example, consider the scenario described by Du in \cite{Du2017Thesis} where hyperspectral (HSI) and LiDAR (light detection and ranging) sensors are used for scene understanding. Hyperspectral cameras can provide a broad range of spectral information about the materials present in a scene, while LiDAR can provide depth information.  If a road and a building rooftop are built with the same material (e.g. asphalt), hyperspectral information alone may not be able to discriminate between the two. However, incorporating elevation information provided by the corresponding LiDAR depth-map would greatly facilitate the classification problem.  Alternatively, paved and dirt biking trails might lay at the same elevation, and using LiDAR data alone may not be sufficient to distinguish between the two types of roads.  However, the spectral characteristics given by the HSI camera could easily determine an area of ground covered in dirt and another filled by asphalt or cement.  
%
%Additionally, data fusion can provide confidence bounds on sensor's collections \cite{hackett1990multisensorfusion,Mohandes2018ClassifierCombinationTechniquesReview}.  For example, HSI sensors measure the radiance across a broad range of wavelengths at varying ground locations.  The measured radiance is a combination of radiation that is reflected and/or emitted by materials on the ground.  This inherently implies that environmental characteristics such as atmospheric conditions and diurnal cycle affect the amount of radiance that can be captured by the sensor \cite{Zare2008Thesis}.  Therefore, incorporation of meta-data such as time-of-day and weather conditions can provide bounds on the uncertainty contained in a given measurement and reinforce when to trust readings from particular spectral bands (i.e. IR during night).   These examples demonstrate how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce measurement uncertainty or to obtain a more complete understanding of a scene, thus aiding in classification or segmentation decisions.
%\newline
%
%Information fusion approaches make two typical assumptions: (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered \cite{Shen2016SpatioTemporalSpectralFusion}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \cite{hackett1990multisensorfusion,Butenuth2007HeterogeneousGeospatialData}.  Assumption (2) states that precise training labels are available for each data point \cite{Du2017Thesis}. 
%\newline 
%
%Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial, spectral, or temporal resolutions, it is usually non-trivial to convert all data to the same resolution or to map to the same grid and existing co-registration approaches often result in loss of sensor-specific information \cite{Shen2016SpatioTemporalSpectralFusion,Brigot2016CoregistrationForestRemoteSensingImages}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process since high-level information such as time-of-day, weather, environmental-setting (e.g. urban or forest), etc., would likely be collected at different spatial or temporal rates and would probably not align easily.  Typical approaches would address this problem by throwing out data points to match the lowest sampling rate.  However, this mapping is often noisy and results in loss of sensor-specific information.  This loss is often detrimental since classification improvement may only be achieved if the total information uncertainty in a problem is reduced \cite{Ruta2000OverviewClassifierFusionMethods}.  Moreover, it becomes difficult to distinguish between errors which evolve during the registration process and actual physical differences in a scene \cite{Zitova2003SurveyImageRegistrationMethods}.
%\newline 
%
%Additionally, even assuming that there is a noiseless/ lossless way to co-register heterogeneous data, standard supervised learning methods require accurate labels for each training data point.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \cite{Du2018MultiResolutionSensorFusion,Du2019MIChoquetIntegral}.  For example, when attempting to detect explosive ordinance with a hand-held electromagnetic induction sensor (metal detector), it is difficult to estimate the expected response from a target due to variations in target size, soil conditions, etc.   Even if the true size and location of a target is known, estimating the spatial expanse of the response is nontrivial and subject to inaccuracies which could be detrimental to the algorithmic training process.  Additionally, labeling high-resolution imagery on the sample-level is tedious, time-consuming, and costly in terms of resources.  There is often an unrealistic assumption that labelers are domain experts.  Even among experts, label boundaries are often subjective and varying levels of uncertainty in the true class exist between different labelers.  These effects make it very difficult (potentially even impossible) to obtain exact, sample-level labels for a training target, and thus add an additional level of geometric uncertainty to the sensor fusion process.  Therefore, fusion methods should be developed which consider this label ambiguity.  
%\newline
%
%One method for fusion of heterogeneous data sources which has gained popularity in previous years is \textit{manifold alignment} (MA) \cite{Hong2019LearnableManifoldAlignment}.  The goal of MA is to match two or more seemingly disparate datasets by mapping them to a joint latent space, while both preserving the qualities of each dataset and highlighting their similarities \cite{Wang2011ManifoldAlignment,Liao2016ManAlignmentHSI,Stanley2019ManAlignmentFeatureCorrespondence}.  This mapping creates a joint feature space which can then be utilized by more traditional learning algorithms \cite{Hong2019LearnableManifoldAlignment}.  The underlying assumption of MA is that, while datasets might capture different qualities presented in a scene, objects under observation should demonstrate similar latent properties independent of modality.  Thus, a single manifold can be discovered from multiple sensors which accurately describes the information present in a scene. \cite{Shen2018ManifoldSensorFusionImageData}.  While MA has been explored for domain adaptation and transfer learning, few studies have investigated MA applied to remote sensing applications \cite{Wang2010MultiscaleManAlignment,Wang2011HeteroDomainAdaptationManAlignment,Yang2016ManifoldAlignmentMultitemporalHSI}.  Furthermore, MA suffers from the same problems mentioned previously.  Traditional methods for MA assume that accurate correspondence between datasets is available \cite{Wang2011ManifoldAlignment}.  While unsupervised MA techniques have been proposed, results demonstrate the advantage of incorporating correspondence points \cite{Wang2010MultiscaleManAlignment, Stanley2019ManAlignmentFeatureCorrespondence, Hong2019LearnableManifoldAlignment}.  It was shown in \cite{Wang2011HeteroDomainAdaptationManAlignment} that MA can be extended to incorporate label correspondence instead of instance-level pairs.  However, in many remote sensing scenarios we do not know the correspondence between data points and accurate labels for supervised learning might not be available. This uncertainty should be incorporated in the model \cite{Damianou2017ManifoldAlignmentDifferentDataView}. In agreement, \cite{Liao2016ManAlignmentHSI} recently noted that completely accurate correspondence might not be needed for MA.  This poses the question, \textit{can we take advantage of weak labels to generate correspondence points?}  Additionally, \textit{if the purpose of fusion is to ultimately perform a classification task, this suggests that label information should be used to drive class representations further from each other in the latent space} \cite{Yang2016ManifoldAlignmentMultitemporalHSI}.
%\newline
%
%Experiments will be conducted on both synthetic data and real applications such as target detection and scene understanding in remote sensing imagery, plant phenotyping, and semantic segmentation.  Datasets will include the DSIAC ATR Algorithm Development Database, MUUFL Gulfport, and Danforth Plant Science Center Manifold Learning datasets.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.


Feature embedding and dimensionality reduction under the multiple instance learning framework has scarcely been explored.  The first true experimentation was performed in \cite{Sun2010MIDR}.  In this work, Sun et al. showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  The authors proposed Multiple Instance Dimensionality Reduction (MIDR) which optimized an objective through gradient descent to discover sparse, orthogonal projection vectors into the latent space.  Their approach relied on fitting a distribution of negative instances to which each instances' probability was evaluated.  This approach was later extended by \cite{Zhu2018MIDRSparsity} in attempt to improve sparsity.  Most existing approaches in the literature extend LDA to distinguish between positive and negative bags \cite{Chai2014MIDA,Zhu2018MIDRSparsity}.  These methods typically rely on costly optimization procedures to maximize an objective for orthogonal and sparse projection vectors.  Additionally, the objective of these approaches is, when given a new bag at test, to provide a bag-level label.  The work in \cite{Xu2011MI_Metric_Learning} investigated metric learning on sets of data (where each bag was a set) to learn an appropriate similarity metric to compare bags.  They do not go as far as to discriminate positive instances within the positive bags, nor do they propose positive target concepts.  While bag-level label prediction is useful in many remote sensing applications, such as region of interest (ROI) proposal for anomaly detection, they limit the the ability of learners to classify on the pixel or sample level.  Xu et al. proposed an importance term to weight samples believed to be true target exemplars, as those instances are more important in determining the bag-level label.  However, to other's knowledge, no work has been done to investigate instance-level discrimination through manifold learning/ dimensionality reduction.  \textit{With this in mind, the goal of this work is to find discriminative instance embeddings which allow for accurate sample-level class discrimination from weakly-labeled data. }

To address these points, I propose the following.  During this project, techniques will be explored for use in instance-level classification given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for discriminative manifold learning and dimensionality reduction and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, LiDAR and more.  The aim of this project is to develop dimensionality reduction methods which promote class discriminability and are simultaneously capable of addressing uncertainty and imprecision in training data groundtruth. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Supervised and semi-supervised manifold learning has proven effective at discovering low-dimensional data representations which provide adequate class separation in the latent space.  However, only a handful of manifold learning procedures consider data which is weakly or ambiguously labeled.  To address this gap in the literature, a method for weakly-supervised manifold learning will be developed. How does this method of manifold construction compare to state-of-the-art manifold learning techniques as well as alternative MI dimensionality reduction methodologies for bag-level label prediction?
	\item Can metric embedding or metric learning be used to improve discriminability among the bags/instances?   Should ranking loss by incorporate within individual bags or across them?  How is this loss incorporated to update the embedding function(s)?
	\item Multiple instance classification procedures such as Multiple Instance Adaptive Cosine/Coherence Estimator, Multiple Instance Random Forest and Multiple Instance Support Vector Machines have shown considerable success in anomaly detection and positive instance classification/concept representation. Do the methods developed in the previous objectives obtain comparable sample-level detection/segmentation results to these alternative approaches?  Or does the discriminative dimensionality reduction technique developed facilitate classification in standard, supervised approaches?
\end{enumerate} 

