\chapter{Introduction}
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \textbf{CITE Xiaoxiao Paper}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  \textbf{Provide intuitive example} 
\newline
Information fusion approaches make two typical assumptions. (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be matched together \textbf{CITE}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \textbf{CITE}.  (2) Training labels are available for each data point \textbf{CITE}.     
\newline \noindent --(\textbf{Loss of information, noisy transformations}) \newline
Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial and temporal resolutions it is not necessarily feasible to convert all data to the same resolution or to map to the same grid \textbf{CITE} and existing co-registration approaches often result in a loss of sensor-specific information \textbf{CITE}.  \textbf{Provide intuitive example}   
\newline \noindent --(\textbf{Physical limitations, cost of labeling}) \newline
Additionally, even assuming that homogeneously registered data is available for fusion or that there is a noiseless way to transform heterogeneous  data, standard supervised learning methods require accurate labels for each training data point\textbf{CITE Xiaoxiao Dissertaion}.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \textbf{CITE Xiaoxiao dissertation reference 10}. \textbf{Provide intuitive example} 
\newline
During this project, techniques will be explored for use in multi-sensor target detection, target classification, and information fusion given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for fusion and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, as well as LiDaR, ground-penetrating radar and electromagnetic induction sensors.  The aim of this project is to develop fusion methods which can address mis-registration between sensor sources as well as uncertainty and imprecision in training data groundtruth while demonstrating robustness towards outlying and adversarial data points. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Can manifold learning be extended to operate under the multiple-instance framework? If so, what is the ``best'' way to construct the data manifold?  Can manifold learning provide robustness to outlying and adversarial exemplars?
	\item Can we construct a joint-representation space for multiple sensors such that there is no loss of information amongst any of the modalities?  What is the ``best'' way to construct this representation?  Do we use raw data or perform feature extraction before combination?
	\item Can we perform detection/ segmentation using a single, sensor-agnostic processing pipeline on the unified representation?  How do we obtain representative quantitative evaluation of performance?
\end{enumerate} 

Experiments will be conducted on both synthetic data and real applications such as plant phenotyping, as well as target detection and scene understanding in remote sensing imagery. Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.

%As an example, it is common in automatic target recognition tasks to utilize both mid-wave infrared (MWIR) and visible-spectrum imagery.  MWIR is able to detect heat in a scene, and while this is an appropriate choice for detecting warmed objects in cold weather or during the nighttime, it often fails during the day when an entire scenes' temperature is elevated.  On the other hand, visible-spectrum imagery can supply highly accurate information in the daytime, but typically fails under poor lighting conditions exhibited during the night.  These two sensors provide complementary information which, when used in the correct context, can greatly increase the performance of an ATR system.  Additionally, auxiliary sensors such as light, temperature, time-of-day, etc. can be used to give confidence values for trusting each sensor's collection.  In this example, each sensor operates under different spatial or temporal resolutions, thus nullifying the homogeneity assumption.

%In existing approaches, is extracted through sensor-specific pipelines before fusion on the sample, feature, or decision level.  There is an assumption under this approach that each pipeline extracts information with an adequate level of precision.  However, overall performance is sensitive to each sensors' processing and information is often lost through co-registration.  As an example, consider a self-driving car outfitted with an RGB camera and LiDAR (light detection and ranging) system. The goal of this computer vision system is to understand the scene around the vehicle so it may navigate safely.  Classical approaches would perform pre-processing, feature extraction, and segmentation/ depth-map estimation on each sensor, individually.  Co-registration would then be performed by mapping a single measurement from the dense LiDAR point-cloud to every pixel.  However, if the wrong LiDAR point is assigned to a pixel, even just a small shift, the car's depth perception can be thrown off.  This is information loss we cannot afford.  A more intuitive method for fusion would be to first project each sensors' collection into a combined representation space before running on a unified pipeline such that the entirety of each sensor's collection is utilized.

%Consider a real scene understanding task where an image is segmented into separate regions containing sky, grass, armored vehicle, mountains, etc.  Existing SOA segmentation techniques require pixel-level labels for each image in the training set.  Obtaining this detail of labeling (especially on a large scale) would cost a labeler or team of labelers a significant amount of time, effort, and money.  The field of multiple-instance learning (MIL) relaxes sample-level label requirements to the region or even image level and has potential to alleviate the cost of labeling, as well as handle spatial and temporal inaccuracies in the collection.  To address these two assumptions, the following is proposed.
