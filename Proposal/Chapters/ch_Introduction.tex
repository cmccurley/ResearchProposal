\chapter{Introduction}
\vspace{1cm}

%%%%%%% \subsection{Causes of ambiguity/ imprecision in groundtruth} %%%%%%%%%%%%%%%%
Target detection is a paramount area of research in the field of remote sensing which aims to locate an object or region of interest while suppressing unrelated objects and information. \citep{Geng2017TargetDetection,Chaudhuri1995TargetDetection}.  Target detection can be formulated as a two-class classification problem where samples belonging to a class of interest are discriminated from a background distribution \citep{Zare2016MIACE}.  The goal of target detection in remote sensing is to correctly classify every true positive instance (TP) in a given scene while indicating no false alarms (FA) (non-target samples predicted as targets).  However, there is always a trade-off in performance, and one might actually choose to miss targets to achieve a lower false-alarm rate \citep{WeinbergerFalseAlarms}, and vice versa. Many remote sensing target detection techniques in the literature involve learning generative models which describe the target and background distributions or learning a labeling function which can differentiate between classes at test \citep{Geng2017TargetDetection,Chaudhuri1995TargetDetection}.  Traditional supervised learning approaches such as these require extensive amounts of highly precise, sample- or pixel-level groundtruth to guide algorithmic training.  However, acquiring large quantities  of accurately labeled training data can be expensive both in terms of time and resources, and in some cases, may even be infeasible to obtain \citep{Xu2014LargeMarginWeaklySupervisedDR}.  To demonstrate these inherent labeling problems, consider the following real-world remote sensing examples, beginning with the hyperspectral target detection scenario described in \citep{Du2017Thesis} and \citep{Bocinsky2019Thesis}:

Hyperspectral (HS) sensors collect spatial and spectral information of a scene by receiving radiance data in hundreds of contiguous wavelengths \citep{Zare2008Thesis}.  Due to their inherent properties, HS cameras can provide a broad range of spectral information about the materials present in a scene, and are thus useful for detecting targets whose spectral signatures vary from the background.  Such examples include airplanes on a tarmac or weeds in a cornfield.  While HS provides nice properties for target detection, there are significant challenges when utilizing this modality.  First, the spatial resolution of HS cameras can be low.  As an example, some HS cameras have spatial resolution of $30m^2$ when capturing scenes from the air.  This implies that objects of interest in a scene, such as an airplane, will actually be contained in a single pixel along with other materials, such as asphalt.  When performing target detection/recognition on that pixel, the HS spectra will not be distinguishable as a single, pure material, but as a sub-pixel mixture where the actual materials present as well as their corresponding proportions are unknown. Second, assuming pure target pixels are available, accurate positioning at the desired resolution may not be.  For example, when analyzing a scene from an airplane or satellite, it is necessary to denote the true locations of targets on the ground using a global positioning system (GPS). It is not uncommon, however, to experience GPS error of greater magnitude than the HS pixel-level spatial resolution.  This implies that a halo of uncertainty potentially surrounds every target pixel in the hyperspectral image (HSI), thus making labeling on the pixel-level difficult.

This example demonstrates inherent infeasibility to obtain accurate sample-level labels due to sensor restrictions on both resolution and accuracy.  Furthermore, label imprecision and ambiguity can often be presented from subjectivity between annotators. Many applications such as medical diagnosis and wildlife identification require domain experts to provide accurate data labels \cite{Cheplygina2019MILSurvey,RuizMunoz2015MILBirdsongClassification}.  However, there might not always be agreement between expert annotators and humans are prone to making mistakes.  For example, when looking at computed tomography (CT) scans for malignant/benign tumors, many doctors would likely determine different pixel-level boundaries denoting a tumor, and in some cases, might even misclassify the detriment of the growth.  Similarly, expert wildlife  ecologists determining the identity of birds solely from their songs might be uncertain of a species due to corruptive background noise in the audio segment.

Finally, consider the scenarios shown in Figures \ref{fig:weak_bbox} and \ref{fig:weak_labels}.  These figures show frames taken from the DSIAC MS-003-DB dataset \citep{DSIACATR} which demonstrates mid-wave infrared (MWIR) video segments of moving military vehicles taken at approximately 30 frames per second (FPS).  Many computer vision algorithms have already been developed to perform target detection using canonical bounding boxes (shown in green in Figure \ref{fig:weak_bbox}) \citep{Redmon2018YOLOV3}.  However, drawing tight boxes around targets in each video frame is extremely tedious and time consuming.  It would be beneficial if an annotator could provide a less-restrictive form of label, such as a relaxed bounding box (shown in blue in Figure \ref{fig:weak_bbox} and bottom left in Figure \ref{fig:weak_labels}) or as a small subset of target pixels such as single dot or scribble as shown in Figure \ref{fig:weak_labels}.  Labeling burden could be reduced even further if a single frame could be labeled at a high level as ``including" or ``excluding" target pixels, as shown in Figure \ref{fig:binary_targets}. 


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{"introduction/weak_bbox"}
		\caption[Example of bounding box imprecision.]{A sample frame from the DSIAC  MS-003-DB MWIR dataset.  Two targets are shown with canonical bounding boxes (green) and relaxed bounding boxes (blue).  Red dots represent the centers of the target objects.}
		\label{fig:weak_bbox}
	\end{figure*}
\end{center}

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{"introduction/weak_labels"}
		\caption[Forms of weak labels.]{Examples of weakly-labeled infrared imagery.  The images demonstrate various forms of weak groundtruth around a pickup truck taken with a mid-wave infrared camera.  The images show spot, scribble, imprecise bounding box and image-level labels, respectively.}
		\label{fig:weak_labels}
	\end{figure*}
\end{center}


\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/target_img"}
		\caption{}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/bg_img"}
		\caption{}
	\end{subfigure}
	\caption[Examples of image-level labels.]{Example of image-level labels for binary target detection.  Image (a) is denoted to contain pixels belonging to the target class somewhere within the image, while image (b) clearly contains samples solely from the background distribution.}
	\label{fig:binary_targets}%
\end{figure*}

Techniques which can address these forms of label ambiguity while achieving comparable or better target detection than standard supervised methods can greatly ease the burdens associated with many remote sensing labeling tasks and allow for the application of pattern recognition techniques which would otherwise be infeasible.

%%%%%%%%%%%%%%%%% \subsection{Multiple Instance Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Multiple Instance Learning}
Learning from uncertain, imprecise and ambiguous data has been an active area of research since the late 1990s and is known as \textit{multiple instance learning} (MIL) or \textit{weak learning} \citep{Bocinsky2019Thesis}.  Supervised learning assumes that each training sample is paired with a corresponding classification label.  In multiple instance learning, however, the label of each sample is not necessarily known.  Instead, MIL approaches learn from groups of data points called \textit{bags}, and each bag concept is paired with a label \citep{Cook2015Thesis}. Under the two-class classification scenario the bags are labeled as \textit{negative} if all data points (or \textit{instances}) are known to belong to the background class (not the class of interest).  While the actual number of positive and negative instances may be unknown, bags are labeled \textit{positive} if \textit{at least one} instance is known to belong to the target class (also called a ``true positive") \citep{Zare2016MIACE} or to contain a proportion of true target.  The goal of learning under the MIL framework is to train a model which can classify a bag as positive or negative (bag-level classification) or to predict the class labels of individual instances (instance-level classification). Consider again the example shown in Figure \ref{fig:binary_targets}.  The figure labeled as ``Target" could be considered as a positive bag because, while the image is not accompanied with pixel-level labels, it is known that a true target pixel exists somewhere within the set.  Additionally, the image denoted as ``Background" could be considered as a negative bag, since it does not contain any pure target pixels. Another way to formulate this problem would be to consider sets of these image patches, where a negative bag would only include samples of background patches, while a positive bag would contain at least one patch that held target or part of a target. Given data of this type, multiple instance learning could be used to discover target and/or background class representatives which could be used for class discrimination, or a classifier could be trained to label and unseen test image as containing or excluding target pixels (bag-level classification) or to label each individual pixel as belonging to the target class (instance-level classification).  As with this example, the MIL problem formulation fits many remote sensing scenarios and is thus an important area of investigation \citep{Du2017Thesis}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%  Problems with Existing Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Multiple instance learning approaches in the literature can be broadly generalized into two categories: learning a generative model which effectively describes the positive and/or negative classes, or training a classifier to discriminate between target and background samples or bags.  The success of both learning frameworks, however, is highly dependent on the feature representations of the data and the metrics used to measure dissimilarity.  Many feature learning approaches in the literature use supervised learning to obtain discriminative feature representations, or use supervised methods to fine-tune unsupervised  feature extraction.  \textit{However, this cannot be done directly in MIL because of the uncertainty on the labels \citep{Carbonneau2016MILSurvey}.}

%%%%%%%%%%%%% \subsection{Manifold Learning and Dimensionality Reduction} %%%%%%%%%%%%%%
%\subsection{Manifold Learning and Dimensionality Reduction}
Another approach to obtain useful feature representations is through the application of  \textit{manifold learning}, also commonly referred to as \textit{dimensionality reduction} (DR), \textit{feature embedding}, \textit{geometric machine learning} or \textit{representation learning} in the literature.  The goal of manifold learning can be posed as discovering intrinsic (often lower-dimensional) features from the data which meet an overarching objective, such as: preserving variance, finding compressed representations of data, maintaining global or local structure or promoting discriminability in the embedded space \citep{VanDerMaaten2009DRReview,Bengio2014RepLearningReview, Geng2005SupNonlinearDimRed, Thorstensen2009ManifoldThesis}.  Manifold learning and dimensionality reduction have been studied extensively in the literature and have been used for visualization, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, combating the curse of dimensionality and obtaining more appropriate measures of dissimilarity \citep{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.


Dimensionality plays a significant role in determining class separability.  Enough features should be incorporated as to adequately describe a class of interest, yet too many features may introduce redundancy, and thus be detrimental to the learning process. The curse of dimensionality states that the number of samples needed to characterize the space spanned by an entity grows exponentially with dimensionality \citep{Murphy2012, Theodoris2008KPCA}.  This fact is overwhelming in the context of remote sensing, as it is often both difficult to acquire large quantities of labeled data and there are often only few samples available to describe the target class (such as in air- or space-born HSI target detection). Additionally, dissimilarity metrics often break down in high dimensional spaces, making application of traditional classifiers difficult.  \textit{Therefore,  it is intuitive that approaches should be developed which can combat the curse of dimensionality and provide more appropriate similarity metrics while also addressing the problems associated with uncertain, imprecise, and ambiguously labeled training data.}

The underlying assumption in using manifold learning for discrimination is that opposing classes either reside on separate manifolds or different regions of a joint, intrinsic manifold, where samples of the same class are close and samples from disparate classes are metrically far.  Essentially, the governing class distributions can be described by hyper-surfaces which follow constraints on properties such as continuity and smoothness \citep{Belkin2004SemiSupLearningRiemannianManifolds}.  The goal of learning in this scenario is to discover embedding functions from the input feature space to a lower-dimensional embedding space where the transformed feature representations allow for improved class discrimination.

%%%%%%%%%%%%%%%%%%%% \subsection{Existing Approaches} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Existing Approaches}
Methods for representation learning have recently gained in popularity because they typically result in high levels of classification accuracy.  Some of these methods  learn features in a supervised manner to obtain more discriminative representations.  As mentioned previously, this learning cannot be done directly in  MIL because of the uncertainty on the labels.  Thus, \textit{adaptation of discriminative  feature learning methods would be beneficial to MIL} \citep{Carbonneau2016MILSurvey}, yet this area of research has scarcely been explored.  The first true experimentation was performed in \citep{Sun2010MIDR}.  In this work, Sun et al. showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  To address these issues, Sun proposed Multiple Instance Dimensionality Reduction (MIDR) which optimized an objective through gradient descent to discover sparse, orthogonal projection vectors in the latent space.  Their approach relied on fitting a distribution of negative instances and applying maximum likelihood estimation.  This approach was later extended in \citep{Zhu2018MIDRSparsity} in attempt to improve sparsity.  Both of these methods rely on finding linear projections which will adequately separate positive and negative bags in the embedding space.  However, these approaches fail when the the target and background instances are metrically similar in the feature space.  For example, consider the data shown in Figure \ref{fig:swiss_roll}.  This image demonstrates the popular Swiss Roll dataset, which is a 2-dimensional manifold folded in 3-dimensions.  Assume that the data classes lie on separate ends of the manifold, and that samples are governed by a smooth labeling function.  In Figure \ref{fig:swiss_roll_3d}, the red samples denote one class while the blue represent another.  If using Euclidean (straight line) distance to measure dissimilarity, it would appear that many red samples are (untruly) close to the blue.  However, if an alternative distance metric such as geodesic distance (around the curve) is used to measure dissimilarity between samples, the true class distributions are better represented.  Figure \ref{fig:swiss_roll_2d} shows the unfolding of the manifold according to geodesic distance.  It can be observed that, after the unfolding, the classification problem was transformed from a nonlinear to a linear one.  Additionally, a dimension of the data was deemed unnecessary.
\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.5in]{"introduction/swiss_roll_3d_view_2"}
		\caption{}
		\label{fig:swiss_roll_3d}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.3in]{"introduction/swiss_roll_2d"}
		\caption{}
		\label{fig:swiss_roll_2d}
	\end{subfigure}
	\caption[Swiss Roll manifold unfolding.]{(a) This dataset is known as the Swiss Roll and it depicts a 2-dimensional manifold embedded in 3 dimensions.  (b) The Swiss Roll unfolded according to the geodesic path around the manifold.}
	\label{fig:swiss_roll}%
\end{figure*}  
It would, therefore, be beneficial to first transform the instance representations into more discriminable forms.  Most existing approaches in the literature apply linear transformations to distinguish between positive and negative bags \citep{Chai2014MIDA,Zhu2018MIDRSparsity}.  However, linear techniques may not be adequate to separate classes in the embedding space if the bags or instances exhibit curved structure.  The work in \citep{Xu2011MI_Metric_Learning} investigated metric learning on sets of data (where each bag was a set) to learn an appropriate similarity metric to compare bags.  They do not go as far as to discriminate positive instances within the positive bags, nor do they propose positive target concepts. 
\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.2in]{"introduction/quad_surface_sep"}
		\caption{Separable 3-dimensional quadratic surfaces.}
		\label{fig:quad_surface_sep}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.3in]{"introduction/quad_surface_non_sep"}
		\caption{Non-separable 3-dimensional quadratic surfaces.}
		\label{fig:quad_surface_non_sep}
	\end{subfigure}
	
	
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2in]{"introduction/quad_surface_lin_sep_le_1d"}
		\caption{1-dimensional embedding of separable quadratic surfaces.}
		\label{fig:quad_surface_sep_le_1d}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2in]{"introduction/quad_surface_non_sep_le_1d"}
		\caption{1-dimensional embedding of non-separable quadratic surfaces.}
		\label{fig:quad_surface_non_sep_le_1d}
	\end{subfigure}
	\caption[Unsupervised embedding of quadratic surfaces]{Unsupervised embedding of quadratic surfaces using Laplacian Eigenmaps with a $K$-nearest neighbors graph.}
	\label{fig:quad_surfaces_no_labels}%
\end{figure*} 
\textit{In fact, virtually all MIL DR methods in the literature are applied, solely, to predict bag-level labels.} While bag-level label prediction is useful in many remote sensing applications, such as region of interest (ROI) proposal for anomaly detection, they limit the the ability of learners to classify on the pixel or sample level.  To this end, methods for instance-level discriminative dimensionality reduction should be developed.  This point is demonstrated in by the embeddings of 3-dimensional quadratic surfaces as shown in Figures \ref{fig:quad_surfaces_no_labels} and \ref{fig:quad_surfaces_with_labels}.  Figures \ref{fig:quad_surface_sep} and \ref{fig:quad_surface_non_sep} demonstrate a set of separable and non-separable quadratic surfaces, respectively. 
 \begin{figure*}[t!]
 	\centering
 	\begin{subfigure}[t]{0.5\textwidth}
 		\centering
 		\includegraphics[height=2.2in]{"introduction/quad_surface_sep_bags"}
 		\caption{Separable 3-dimensional quadratic surfaces and examples potential bag splits.  Blue circles represent negative bags while red circle denote positive bags.}
 		\label{fig:quad_surface_sep_bags}
 	\end{subfigure}%
 	~ 
 	\begin{subfigure}[t]{0.5\textwidth}
 		\centering
 		\includegraphics[height=2.3in]{"introduction/quad_surface_non_sep_bags"}
 		\caption{Non-separable 3-dimensional quadratic surfaces and examples potential bag splits. Blue circles represent negative bags while red circle denote positive bags.}
 		\label{fig:quad_surface_non_sep_bags}
 	\end{subfigure}
 	
 	
 	\begin{subfigure}[t]{0.5\textwidth}
 		\centering
 		\includegraphics[height=2in]{"introduction/quad_surface_lin_sep_sle_1d"}
 		\caption{1-dimensional embedding of separable quadratic surfaces using Supervised Laplacian Eigenmaps.}
 		\label{fig:quad_surface_lin_sep_sle_1d}
 	\end{subfigure}%
 	~ 
 	\begin{subfigure}[t]{0.5\textwidth}
 		\centering
 		\includegraphics[height=2in]{"introduction/quad_surface_non_sep_sle_1d"}
 		\caption{1-dimensional embedding of non-separable quadratic surfaces using Supervised Laplacian Eigenmaps.}
 		\label{fig:quad_surface_non_sep_sle_1d}
 	\end{subfigure}
 	\caption[Supervised embedding of quadratic surfaces]{Embedding of quadratic surfaces using Supervised Laplacian Eigenmaps.}
 	\label{fig:quad_surfaces_with_labels}%
 \end{figure*} 
The use of a traditional, unsupervised dimensionality reduction method to project the data into a 1-dimensional space managed to retain  topological ordering of the samples, but failed to promote class discriminability, as shown in Figures \ref{fig:quad_surface_sep_le_1d} and \ref{fig:quad_surface_non_sep_le_1d}.  However, using a supervised version of the same algorithm demonstrated the enforcement of class separability in the latent embedding space for both the separable and non-separable quadratic surfaces, as shown by Figures \ref{fig:quad_surface_lin_sep_sle_1d} and \ref{fig:quad_surface_non_sep_sle_1d}.  This embedding used sample-level labels.  However, the proposed work will relax the precision of the labels to bags as to comply with the labeling needs of many remote sensing problems.  This notion can be conceptualized by Figures \ref{fig:quad_surface_sep_bags} and \ref{fig:quad_surface_non_sep_bags}, where sets of data samples are used to form bags. 

%%%%%%%%%%%%%%%%%%%%%%%%%%% \subsection{Proposition of Work} %%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Proposition of Work}
To address these points, I propose the following.  During this project, techniques will be explored for use in instance-level classification given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for discriminative manifold/feature representation learning and dimensionality reduction and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyperspectral and multispectral imagery, LiDAR and more.  \textit{The aim of this project is to develop dimensionality reduction methods which promote class discriminability and are simultaneously capable of addressing uncertainty and imprecision in training data groundtruth.}  The motivating idea is to facilitate instance/concept proposition by increasing instance discriminability under the constraints of multiple instance learning. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Supervised and semi-supervised manifold learning have proven effective at discovering low-dimensional data representations which provide adequate class separation in the latent space.  However, only a handful of manifold learning procedures consider data that is weakly or ambiguously labeled.  To address this gap in the literature, a method for weakly-supervised manifold learning will be developed. How does this method of manifold construction compare to state-of-the-art (SOA) manifold learning techniques as well as alternative MI dimensionality reduction methodologies for instance-level label prediction?
	\item In conjunction with dimensionality, the use of metric embedding has been shown to promote class separability in the latent space. However, metric embedding typically requires knowledge of instance-level labels.  Using only weak, bag-level labels, a method for metric embedding will be developed and utilized with the manifold learning approach in Objective 1 to potentially improve class separation of individual instances.  Additionally, a procedure to select the most influential examples for training will be developed.
	\item Do the proposed methods facilitate concept learning/selection?  Using alternative state-of-the-art MIL approaches, are the selected target instances/concepts more discriminable with the proposed methods than without?  How do the proposed methods compare to the alternatives in terms of representation dimensionality, computational complexity, and promotion of discriminability?
\end{enumerate} 

%%%%%%%%%%%%%%%% \subsection{Datasets} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Datasets}
Experiments will be conducted on both synthetic data and real applications such as target detection, scene understanding and semantic segmentation in remote sensing imagery.  Datasets will include the DSIAC MS-003-DB Algorithm Development Database, MUUFL Gulfport, Faces in the Wild (LFW) and benchmark MIL classification datasets \citep{DSIACATR,MUUFL,MUUFLSceneLabels,MUUFLScoringCode,LFW}.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.




