\chapter{Introduction}
\vspace{1cm}

%%%%%%% \subsection{Causes of ambiguity/ imprecision in groundtruth} %%%%%%%%%%%%%%%%
Target detection is a paramount area of research in the field of remote sensing which aims to locate an object or region of interest while suppressing unrelated objects and information. \cite{Geng2017TargetDetection,Chaudhuri1995TargetDetection}.  Target detection can be formulated as a two-class classification problem where samples belonging to a class of interest are discriminated from a global background distribution \cite{Zare2016MIACE}.  The goal of target detection in remote sensing is to correctly classify every true positive instance (TP) in a given scene while indicating as few false alarms (FA) as possible (non-target samples predicted as targets).  Most remote sensing target detection techniques in the literature are variations of constrained energy minimization or maximum likelihood with matched filters \cite{Geng2017TargetDetection,Chaudhuri1995TargetDetection}. The goal of learning in these scenarios is to discover prototypes which represent both the target and background classes that can be used to classify a sample at test.  Traditional supervised learning approaches such as these require extensive amounts of highly precise groundtruth to guide algorithmic training.  However, acquiring large quantities  of accurately labeled training data can be expensive both in terms of time and resources, and in some cases, may even be infeasible to obtain.  Consider the following hyperspectral target detection scenario described in \cite{Du2017Thesis} and \cite{Bocinsky2019Thesis}:  

Hyperspectral (HS) sensors collect spatial and spectral information of a scene by receiving radiance data in hundreds of contiguous wavelengths \cite{Zare2008Thesis}.  Due to their inherent properties, HS cameras can provide a broad range of spectral information about the materials present in a scene, and are thus useful for detecting targets whose spectral signatures vary from the background.  Such examples include airplanes on a tarmac or weeds in a cornfield.  While HS provides nice properties for target detection, there are significant challenges when utilizing this modality.  First, the spatial resolution of HS cameras is typically much lower than traditional digital cameras.  As an example, some HS cameras have spatial resolution of $30m^2$.  This implies that objects of interest in a scene, such as an airplane, will actually be contained in a single pixel along with other materials, such as asphalt.  When performing target detection/recognition on that pixel, the HS spectra will not be distinguishable as a single, pure material, but as a sub-pixel mixture where the actual materials present as well as their corresponding proportions are unknown. Second, assuming pure target pixels are available, accurate positioning at the desired resolution may not be.  For example, when analyzing a scene from an airplane or satellite, it is necessary to denote the true locations of targets on the ground using a global positioning system (GPS). It is not uncommon, however, to experience GPS error in the order of meters.  This implies that a halo of uncertainty potentially surrounds every target pixel in the hyperspectral image (HSI), thus making labeling on the pixel-level difficult.

This example demonstrates inherent infeasibility to obtain accurate sample-level labels due to sensor restrictions on both resolution and accuracy.  Furthermore, label imprecision and ambiguity can often be presented from subjectivity between annotators. Many applications such as medical diagnosis and wildlife identification require domain experts to provide accurate data labels.  However, there might not always be agreement between expert annotators and humans are prone to making mistakes.  For example, when looking at computed tomography (CT) scans for malignant/benign tumors, many doctors would likely determine different pixel-level boundaries denoting a tumor, and in some cases, might even misclassify the detriment of the growth.  Similarly, expert wildlife  ecologists determining the identity of birds solely from their songs might be uncertain of a species due to corruptive background noise in the audio segment.

Finally, consider the scenarios shown in Figures \ref{fig:weak_bbox} and \ref{fig:weak_labels}.  These figures show frames taken from the DSIAC MS-003-DB dataset which demonstrates mid-wave infrared (MWIR) video segments of moving military vehicles taken at approximately 30 frames per second (FPS).  Many computer vision algorithms have already been developed to perform target detection using canonical bounding boxes (shown in green in Figure \ref{fig:weak_bbox}) \cite{Redmon2018YOLOV3}.  However, drawing tight boxes around targets in each video frame is extremely tedious and time consuming.  It would be beneficial if an annotator could provide a less-restrictive form of label, such as a relaxed bounding box (shown in blue in Figure \ref{fig:weak_bbox} and bottom left in Figure \ref{fig:weak_labels}) or as a small subset of target pixels such as single dot or scribble as shown in Figure \ref{fig:weak_labels}.  Labeling burden could be reduced even further if a single frame could be labeled at a high level as ``including" or ``excluding" target pixels, as shown in Figure \ref{fig:binary_targets}. 


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{"introduction/weak_bbox"}
		\caption[Example of bounding box imprecision.]{A sample frame from the DSIAC  MS-003-DB MWIR dataset.  Two targets are shown with canonical bounding boxes (green) and relaxed bounding boxes (blue).  Red dots represent the centers of the target objects.}
		\label{fig:weak_bbox}
	\end{figure*}
\end{center}

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{"introduction/weak_labels"}
		\caption[Forms of weak labels.]{Examples of weakly-labeled infrared imagery.  The images demonstrate various forms of weak groundtruth around a pickup truck taken with a mid-wave infrared camera.  The images show spot, scribble, imprecise bounding box and image-level labels, respectively.}
		\label{fig:weak_labels}
	\end{figure*}
\end{center}


\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/target_img"}
		\caption{}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/bg_img"}
		\caption{}
	\end{subfigure}
	\caption[Examples of image-level labels.]{Example of image-level labels for binary target detection.  Image (a) is denoted to contain pixels belonging to the target class somewhere within the image, while image (b) clearly contains samples solely from the background distribution.}
	\label{fig:binary_targets}%
\end{figure*}

Techniques which can address these forms of label ambiguity while achieving comparable or better target detection than standard supervised methods can greatly ease the burdens associated with many remote sensing labeling tasks and allow for the application of pattern recognition techniques which would otherwise be infeasible.

%%%%%%%%%%%%%%%%% \subsection{Multiple Instance Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Learning from uncertain, imprecise and ambiguous data has been an active area of research since the late 1990s and is known as multiple instance learning (MIL) \cite{Bocinsky2019Thesis}.  Supervised learning assumes that each training sample is paired with a corresponding classification label.  In multiple instance learning, however, the label of each sample is not necessarily known.  Instead, MIL approaches learn from groups of data points called \textit{bags}, and each bag concept is paired with a label \cite{Cook2015Thesis}. Under the two-class classification scenario the bags are labeled as \textit{negative} if all data points (or \textit{instances}) are known to belong to the background class (not the class of interest).  While the actual number of positive and negative instances may be unknown, bags are labeled \textit{positive} if \textit{at least one} instance is known to belong to the target class (also called a ``true positive") \cite{Zare2016MIACE} or to contain a proportion of true target.  The goal of learning under the MIL framework is to train a model which can classify a bag as positive or negative (bag-level classification) or to predict the class labels of individual instances (instance-level classification). Consider again the example shown in Figure \ref{fig:binary_targets}.  The figure labeled as ``Target" could be considered as a positive bag because, while the image is not accompanied with pixel-level labels, it is known that a true target pixel exists somewhere within the set.  Additionally, the image denoted as ``Background" could be considered as a negative bag, since it does not contain any pure target pixels.  Given data of this type, multiple instance learning could be used to discover target and/or background class representatives which could be used for class discrimination, or a classifier could be trained to label and unseen test image as containing or excluding target pixels (bag-level classification) or to label each individual pixel as belonging to the target class (instance-level classification).  As with this example, the MIL problem formulation fits many remote sensing scenarios and is thus an important area of investigation \cite{Du2017Thesis}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%  Problems with Existing Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Multiple instance learning approaches in the literature can be broadly generalized into two categories: learning a concept(s) which effectively describes the positive and/or negative classes, or training a classifier to discriminate bags or individual instances.  Existing approaches in the literature have two primary limitations:  (1) MIL methods assume that target instances are already separable in the feature space or that a signature(s) can be learned which represents the target class well, while poorly representing the entirety of the negative class, regardless of potential distribution overlap \cite{Carbonneau2016MILSurvey}. (2) Current methods are unable to address learning complications associated with high-dimensionality.

%%%%%%%%%%%%% \subsection{Manifold Learning and Dimensionality Reduction} %%%%%%%%%%%%%%
These two detrimental limitations can potentially be resolved with the application of \textit{manifold learning}, also commonly referred to as \textit{dimensionality reduction} (DR), \textit{feature embedding}, \textit{geometric machine learning} or \textit{representation learning} in the literature.  The goal of manifold learning can be posed as discovering intrinsic (often lower-dimensional) features from the data which meet an overarching objective, such as: preserving variance, finding compressed representations of data, maintaining global or local structure or promoting discriminability in the embedded space \cite{VanDerMaaten2009DRReview,Bengio2014RepLearningReview, Geng2005SupNonlinearDimRed, Thorstensen2009ManifoldThesis}.  Manifold learning and dimensionality reduction have been studied extensively in the literature and have been used for visualization, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and arguably the most important application, combating the curse of dimensionality \cite{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.


Dimensionality plays a significant role in determining class separability.  Enough features should be incorporated as to adequately describe a class of interest, yet too many features may introduce redundancy, and thus be detrimental to the learning process. The Curse of Dimensionality  states that the number of samples needed to characterize the space spanned by an entity grows exponentially with dimensionality \cite{Murphy2012, Theodoris2008KPCA}.  This fact is overwhelming in the context of remote sensing, as it is often both difficult to acquire large quantities of labeled data and there are often only few samples available to describe the target class (such as in air- or space-born HSI target detection). Additionally, dissimilarity metrics often break down in high dimensional spaces, making application of traditional classifiers difficult.  \textit{Therefore,  it is intuitive that approaches should be developed which can combat the Curse  of Dimensionality while also addressing the problems associated with uncertain, imprecise, and ambiguously labeled training data.}

The underlying assumption in using manifold learning for discrimination is that opposing classes either reside on separate manifolds or different regions of a joint, intrinsic manifold, where samples of the same class are close and samples from disparate classes are metrically far.  Essentially, the governing class distributions can be described by hyper-surfaces which follow constraints on properties such as continuity and smoothness \cite{Belkin2004SemiSupLearningRiemannianManifolds}.  The goal of learning in this scenario is to discover embedding functions from the input feature space to a lower-dimensional embedding space where the transformed feature representations allow for improved class discrimination.

%%%%%%%%%%%%%%%%%%%% \subsection{Existing Approaches} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Methods for representation learning have recently gained in popularity because they typically result in high levels of classification accuracy.  Some of these methods  learn features in a supervised manner to obtain more discriminative representations.  However, this learning cannot be done directly in  MIL because of the uncertainty on the labels.  Thus, \textit{adaptation of discriminative  feature learning methods would be beneficial to MIL} \cite{Carbonneau2016MILSurvey}, yet this area of research has scarcely been explored.  The first true experimentation was performed in \cite{Sun2010MIDR}.  In this work, Sun et al. showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  To address these issues, Sun proposed Multiple Instance Dimensionality Reduction (MIDR) which optimized an objective through gradient descent to discover sparse, orthogonal projection vectors into the latent space.  Their approach relied on fitting a distribution of negative instances and applying maximum likelihood estimation.  This approach was later extended in \cite{Zhu2018MIDRSparsity} in attempt to improve sparsity.  Both of these methods rely on fitting a distribution of negative instances and determining positive instances as samples which have low likelihood of belonging to the distribution.  However, these approaches fail when the the target and background instances are similar.  It would, therefore, be beneficial to first transform the instance representations into more discriminable forms.  Most existing approaches in the literature extend LDA to distinguish between positive and negative bags \cite{Chai2014MIDA,Zhu2018MIDRSparsity}.  These methods typically rely on costly optimization procedures to maximize an objective for orthogonal and sparse projection vectors.  The work in \cite{Xu2011MI_Metric_Learning} investigated metric learning on sets of data (where each bag was a set) to learn an appropriate similarity metric to compare bags.  They do not go as far as to discriminate positive instances within the positive bags, nor do they propose positive target concepts.  In fact, virtually all MIL DR methods in the literature are applied, solely, to predict bag-level labels. While bag-level label prediction is useful in many remote sensing applications, such as region of interest (ROI) proposal for anomaly detection, they limit the the ability of learners to classify on the pixel or sample level.  Xu et al. proposed an importance term to weight samples believed to be true target exemplars, as those instances are more important in determining the bag-level label \cite{Xu2011MI_Metric_Learning}.  However, to the author's knowledge, no work has been done to investigate instance-level discrimination through manifold learning/ dimensionality reduction.  \textit{With this in mind, the goal of this work is, using only bag-level labels, to find discriminative instance embeddings which allow for accurate sample-level classification.}

%%%%%%%%%%%%%%%%%%%%%%%%%%% \subsection{Proposition of Work} %%%%%%%%%%%%%%%%%%%%%%%%%%
To address these points, I propose the following.  During this project, techniques will be explored for use in instance-level classification given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for discriminative manifold/feature representation learning and dimensionality reduction and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyperspectral and multispectral imagery, LiDAR and more.  \textit{The aim of this project is to develop dimensionality reduction methods which promote class discriminability and are simultaneously capable of addressing uncertainty and imprecision in training data groundtruth.}  The motivating idea is to facilitate instance/concept proposition by increasing instance discriminability under the constraints of multiple instance learning. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Supervised and semi-supervised manifold learning have proven effective at discovering low-dimensional data representations which provide adequate class separation in the latent space.  However, only a handful of manifold learning procedures consider data that is weakly or ambiguously labeled.  To address this gap in the literature, a method for weakly-supervised manifold learning will be developed. How does this method of manifold construction compare to state-of-the-art manifold learning techniques as well as alternative MI dimensionality reduction methodologies for instance-level label prediction?
	\item Metric embedding has been shown to promote class separability through dimensionality reduction.  Without instance-level labels, a method for ranking loss embedding will be developed in conjunction with Objective 1 to improve class separation of individual instances.  Additionally, a procedure to select the most influential examples for training will be developed.
	\item Do the proposed methods facilitate concept learning/selection?  Using alternative state-of-the-art MIL approaches, are the selected target instances/concepts more discriminable with the proposed methods than without?  How do the proposed methods compare to the alternatives in terms of representation dimensionality, computational complexity, and promotion of discriminability?
\end{enumerate} 

%%%%%%%%%%%%%%%% \subsection{Datasets} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Experiments will be conducted on both synthetic data and real applications such as target detection, scene understanding and semantic segmentation in remote sensing imagery.  Datasets will include the DSIAC MS-003-DB Algorithm Development Database, MUUFL Gulfport, Faces in the Wild (LFW) and benchmark MIL classification datasets \cite{DSIACATR,MUUFL,MUUFLSceneLabels,MUUFLScoringCode,LFW}.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.




