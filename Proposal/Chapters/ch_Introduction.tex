\chapter{Introduction}

During this project, manifold learning methodologies will be explored for use in multi-sensor target detection, target classification, and information fusion given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for fusion and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, as well as LiDaR, ground-penetrating radar and electromagnetic induction sensors.  The aim of this project is to develop fusion methods which can address mis-registration between sensor sources as well as uncertainty and imprecision in training data groundtruth, while demonstrating robustness towards outlying and adversarial data points. \newline
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities individually \textbf{CITE Xiaoxiao}.  The advantages of fusing multiple sensor modalities are numerous.  First, sensors are subject to noise and measurement errors which stem from physical limitations or products of the collection environment.  Therefore, the consensus of multiple modalities which can represent the same property may be leveraged to reduce uncertainty.  Additionally, the output of a single sensor might be ambiguous or misleading.  For example, a self-driving car with a single RGB camera might be able to determine what is in a particular scene, but would have a difficult time determining the physical distances to objects in that scene.  In this case, incorporating a depth sensor such as LiDaR would allow the car to obtain a higher level of understanding about the entire scene which it could then use to make more informed decisions. \textbf{CITE Hackett}  Fusion techniques have been applied successfully to a variety of applications, including object detection, recognition, classification, object tracking, change detection, and more \textbf{CITE Zhang}.  Specifically, sensor and data fusion approaches have been applied to remote sensing applications in surveillance, explosive hazard detection, medical imaging and diagnosis, and more.
\newline
A prime challenge with developing accurate fusion methodologies for remote sensing applications is the acquisition of precise groundtruth. 
\newline
How this will expand knowledge

\newpage
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \textbf{CITE Xiaoxiao Paper}.  Fusion of multiple sensor sources solves two problems.  First, 

Traditionally, information is extracted through sensor-specific pipelines before fusion on the sample, feature, or decision level.  There is an assumption under this approach that each pipeline extracts information with an adequate level of precision.  However, overall performance is sensitive to each sensors' processing and information is often lost through co-registration.  As an example, consider a self-driving car outfitted with an RGB camera and LiDAR (light detection and ranging) system. The goal of this computer vision system is to understand the scene around the vehicle so it may navigate safely.  Classical approaches would perform pre-processing, feature extraction, and segmentation/ depth-map estimation on each sensor, individually.  Co-registration would then be performed by mapping a single measurement from the dense LiDAR point-cloud to every pixel.  However, this down-sampling causes a potentially devastating loss of information since inaccurate LiDAR mappings can greatly mis-represent the location of objects.  A more intuitive method for fusion would be to first project each sensors' collection into a combined representation space and then run on a unified pipeline. 
\newline
Additionally, even assuming that homogeneously registered data is available for fusion or that there is a noiseless way to transform heterogeneous  data, standard supervised learning methods require accurate labels for each training data point\textbf{CITE Xiaoxiao Dissertaion}.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \textbf{CITE Xiaoxiao dissertation reference 10}. 
\newline
To address these two problems,
