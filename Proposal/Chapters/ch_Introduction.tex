\chapter{Introduction}
\vspace{1cm}

%Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \cite{hackett1990multisensorfusion,zhang2010multisourceremotingsensingfusion}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  For example, consider the scenario described by Du in \cite{Du2017Thesis} where hyperspectral (HSI) and LiDAR (light detection and ranging) sensors are used for scene understanding. Hyperspectral cameras can provide a broad range of spectral information about the materials present in a scene, while LiDAR can provide depth information.  If a road and a building rooftop are built with the same material (e.g. asphalt), hyperspectral information alone may not be able to discriminate between the two. However, incorporating elevation information provided by the corresponding LiDAR depth-map would greatly facilitate the classification problem.  Alternatively, paved and dirt biking trails might lay at the same elevation, and using LiDAR data alone may not be sufficient to distinguish between the two types of roads.  However, the spectral characteristics given by the HSI camera could easily determine an area of ground covered in dirt and another filled by asphalt or cement.  
%
%Additionally, data fusion can provide confidence bounds on sensor's collections \cite{hackett1990multisensorfusion,Mohandes2018ClassifierCombinationTechniquesReview}.  For example, HSI sensors measure the radiance across a broad range of wavelengths at varying ground locations.  The measured radiance is a combination of radiation that is reflected and/or emitted by materials on the ground.  This inherently implies that environmental characteristics such as atmospheric conditions and diurnal cycle affect the amount of radiance that can be captured by the sensor \cite{Zare2008Thesis}.  Therefore, incorporation of meta-data such as time-of-day and weather conditions can provide bounds on the uncertainty contained in a given measurement and reinforce when to trust readings from particular spectral bands (i.e. IR during night).   These examples demonstrate how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce measurement uncertainty or to obtain a more complete understanding of a scene, thus aiding in classification or segmentation decisions.
%\newline
%
%Information fusion approaches make two typical assumptions: (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered \cite{Shen2016SpatioTemporalSpectralFusion}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \cite{hackett1990multisensorfusion,Butenuth2007HeterogeneousGeospatialData}.  Assumption (2) states that precise training labels are available for each data point \cite{Du2017Thesis}. 
%\newline 
%
%Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial, spectral, or temporal resolutions, it is usually non-trivial to convert all data to the same resolution or to map to the same grid and existing co-registration approaches often result in loss of sensor-specific information \cite{Shen2016SpatioTemporalSpectralFusion,Brigot2016CoregistrationForestRemoteSensingImages}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process since high-level information such as time-of-day, weather, environmental-setting (e.g. urban or forest), etc., would likely be collected at different spatial or temporal rates and would probably not align easily.  Typical approaches would address this problem by throwing out data points to match the lowest sampling rate.  However, this mapping is often noisy and results in loss of sensor-specific information.  This loss is often detrimental since classification improvement may only be achieved if the total information uncertainty in a problem is reduced \cite{Ruta2000OverviewClassifierFusionMethods}.  Moreover, it becomes difficult to distinguish between errors which evolve during the registration process and actual physical differences in a scene \cite{Zitova2003SurveyImageRegistrationMethods}.
%\newline 
%
%Additionally, even assuming that there is a noiseless/ lossless way to co-register heterogeneous data, standard supervised learning methods require accurate labels for each training data point.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \cite{Du2018MultiResolutionSensorFusion,Du2019MIChoquetIntegral}.  For example, when attempting to detect explosive ordinance with a hand-held electromagnetic induction sensor (metal detector), it is difficult to estimate the expected response from a target due to variations in target size, soil conditions, etc.   Even if the true size and location of a target is known, estimating the spatial expanse of the response is nontrivial and subject to inaccuracies which could be detrimental to the algorithmic training process.  Additionally, labeling high-resolution imagery on the sample-level is tedious, time-consuming, and costly in terms of resources.  There is often an unrealistic assumption that labelers are domain experts.  Even among experts, label boundaries are often subjective and varying levels of uncertainty in the true class exist between different labelers.  These effects make it very difficult (potentially even impossible) to obtain exact, sample-level labels for a training target, and thus add an additional level of geometric uncertainty to the sensor fusion process.  Therefore, fusion methods should be developed which consider this label ambiguity.  
%\newline
%
%One method for fusion of heterogeneous data sources which has gained popularity in previous years is \textit{manifold alignment} (MA) \cite{Hong2019LearnableManifoldAlignment}.  The goal of MA is to match two or more seemingly disparate datasets by mapping them to a joint latent space, while both preserving the qualities of each dataset and highlighting their similarities \cite{Wang2011ManifoldAlignment,Liao2016ManAlignmentHSI,Stanley2019ManAlignmentFeatureCorrespondence}.  This mapping creates a joint feature space which can then be utilized by more traditional learning algorithms \cite{Hong2019LearnableManifoldAlignment}.  The underlying assumption of MA is that, while datasets might capture different qualities presented in a scene, objects under observation should demonstrate similar latent properties independent of modality.  Thus, a single manifold can be discovered from multiple sensors which accurately describes the information present in a scene. \cite{Shen2018ManifoldSensorFusionImageData}.  While MA has been explored for domain adaptation and transfer learning, few studies have investigated MA applied to remote sensing applications \cite{Wang2010MultiscaleManAlignment,Wang2011HeteroDomainAdaptationManAlignment,Yang2016ManifoldAlignmentMultitemporalHSI}.  Furthermore, MA suffers from the same problems mentioned previously.  Traditional methods for MA assume that accurate correspondence between datasets is available \cite{Wang2011ManifoldAlignment}.  While unsupervised MA techniques have been proposed, results demonstrate the advantage of incorporating correspondence points \cite{Wang2010MultiscaleManAlignment, Stanley2019ManAlignmentFeatureCorrespondence, Hong2019LearnableManifoldAlignment}.  It was shown in \cite{Wang2011HeteroDomainAdaptationManAlignment} that MA can be extended to incorporate label correspondence instead of instance-level pairs.  However, in many remote sensing scenarios we do not know the correspondence between data points and accurate labels for supervised learning might not be available. This uncertainty should be incorporated in the model \cite{Damianou2017ManifoldAlignmentDifferentDataView}. In agreement, \cite{Liao2016ManAlignmentHSI} recently noted that completely accurate correspondence might not be needed for MA.  This poses the question, \textit{can we take advantage of weak labels to generate correspondence points?}  Additionally, \textit{if the purpose of fusion is to ultimately perform a classification task, this suggests that label information should be used to drive class representations further from each other in the latent space} \cite{Yang2016ManifoldAlignmentMultitemporalHSI}.
%\newline
%
%Experiments will be conducted on both synthetic data and real applications such as target detection and scene understanding in remote sensing imagery, plant phenotyping, and semantic segmentation.  Datasets will include the DSIAC ATR Algorithm Development Database, MUUFL Gulfport, and Danforth Plant Science Center Manifold Learning datasets.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.

%%%%%%% \subsection{Causes of ambiguity/ imprecision in groundtruth} %%%%%%%%%%%%%%%%
Target detection is a paramount area of research in the field of remote sensing which aims to locate an object or region of interest while suppressing background \cite{Geng2017TargetDetection,Chaudhuri1995TargetDetection}.  Target detection can be formulated as a two-class classification problem where samples belonging to a class of interest are discriminated from a global background distribution \cite{Zare2016MIACE}.  The goal of target detection in remote sensing is to correctly classify every true positive instance (TP) in a given scene while denoting as few false alarms (FA) as possible (non-target samples predicted as targets).  Most remote sensing target detection techniques in the literature are variations of constrained energy minimization or maximum likelihood with matched filters \cite{Geng2017TargetDetection,Chaudhuri1995TargetDetection}. The goal of learning in these scenarios is to discover prototypes which represent both the target and background classes that can be used to classify a sample at test.  Traditional supervised learning approaches such as these require extensive amounts of highly precise groundtruth to guide algorithmic training.  However, acquiring large quantities  of accurately labeled training data can be expensive both in terms of time and resources, and in some cases, may even be infeasible to obtain.  Consider the following hyperspectral target detection scenario described in \cite{Du2017Thesis} and \cite{Bocinsky2019Thesis}:  

Hyperspectral (HSI) sensors collect spatial and spectral information of a scene by receiving radiance data in hundreds of contiguous wavelengths \cite{Zare2008Thesis}.  Due to their inherent properties, HSI cameras can provide a broad range of spectral information about the materials present in a scene, and are thus useful for detecting targets whose material makeups vary from the background.  Such examples include airplanes on a tarmac or weeds in a cornfield.  While HSI provides nice properties for target detection, target detection with hyperspectral data poses unique challenges.  First, the spatial resolution of HSI cameras is typically much lower than traditional digital cameras.  This implies that objects of interest in a scene can be mixed at the sub-pixel level, where the true materials present as well as corresponding proportions are unknown. Second, assuming pure target pixels are available, accurate positioning at the desired resolution may not be.  For example, when analyzing a scene from an airplane or satellite, it is necessary to denote the true locations of targets on the ground using a global positioning system (GPS). It is not uncommon, however, to experience GPS error in the order of meters.  This implies that a halo of uncertainty potentially surrounds every target pixel in the HSI image, thus making labeling on the pixel-level difficult.

This example demonstrates inherent in-feasibility to obtain accurate sample-level labels due to sensor restrictions on both resolution and accuracy.  Furthermore, label imprecision and ambiguity can often be presented from subjectivity between annotators. Many applications such as medical diagnosis and wildlife identification require domain experts to provide accurate data labels.  However, there might not always be agreement between expert annotators and humans are prone to making mistakes.  For example, when looking at computed tomography (CT) scans for malignant/benign tumors, many doctors would likely determine different pixel-level boundaries denoting a tumor, and in some cases, might even mis-classify the detriment of the growth.  Similarly, expert wildlife  ecologist determining the identity of birds solely from their songs might be uncertain of a species due to corruptive background noise in the audio segment.

Finally, consider the scenarios shown in Figures \ref{fig:weak_bbox} and \ref{fig:weak_labels}.  These figures show frames taken from the DSIAC MS-003-DB dataset which demonstrates mid-wave infrared (MWIR) video segments of moving military vehicles taken at approximately 30 frames per second (FPS).  Many computer vision algorithms have already been developed to perform target detection using canonical bounding boxes (shown in green in Figure \ref{fig:weak_bbox}) \cite{Redmon2018YOLOV3}.  However, drawing tight boxes around targets in each video frame is extremely tedious and time consuming.  It would be beneficial if an annotator could provide a less-restrictive form of label, such as a relaxed bounding box (shown in blue in Figure \ref{fig:weak_bbox} and bottom left in Figure \ref{fig:weak_labels}) or as a small subset of target pixels such as single dot or scribble as shown in Figure \ref{fig:weak_labels}.  Labeling burden could be reduced even further if a single frame could be labeled at a high level as ``including" or ``excluding" target pixels, as shown in Figure \ref{fig:binary_targets}. 


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{"introduction/weak_bbox"}
		\caption{A sample frame from the DSIAC  MS-003-DB MWIR dataset.  Two targets are shown with canonical bounding boxes (green) and relaxed bounding boxes (blue).  Red dots represent the centers of the target objects.}
		\label{fig:weak_bbox}
	\end{figure*}
\end{center}

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{"introduction/weak_labels"}
		\caption{Examples of weakly-labeled infrared imagery.  The images demonstrate various forms of weak groundtruth around a pickup truck taken with a mid-wave infrared camera.  The images show spot, scribble, imprecise bounding box and image-level labels, respectively.}
		\label{fig:weak_labels}
	\end{figure*}
\end{center}


\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/target_img"}
		\caption{}
	\end{subfigure}%
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=1.2in]{"introduction/bg_img"}
		\caption{}
	\end{subfigure}
	\caption{Example of image-level labels for binary target detection.  Image (a) is denoted to contain pixels belonging to the target class somewhere within the image, while image (b) clearly contains samples solely from the background distribution.}
	\label{fig:binary_targets}%
\end{figure*}

Techniques which can address these forms of label ambiguity while achieving comparable or better target detection than standard supervised methods can greatly ease the burdens associated with many remote sensing labeling tasks and allow for the application of pattern recognition techniques which would otherwise be infeasible.

%%%%%%%%%%%%%%%%% \subsection{Multiple Instance Learning} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Learning from uncertain, imprecise and ambiguous data has been an active area of research since the late 1990s and is known as multiple instance learning (MIL) \cite{Bocinsky2019Thesis}.  Supervised learning assumes that each training sample is paired with a corresponding classification label.  In multiple instance learning, however, the label of each sample is not necessarily known.  Instead, MIL approaches learn from groups of data points called \textit{bags}, and each bag concept is paired with a label \cite{Cook2015Thesis}. Under the two-class classification scenario the bags are labeled as \textit{negative} if all data points (or \textit{instances}) are known to belong to the background class (not the class of interest).  While the actual number of positive and negative instances may be unknown, bags are labeled \textit{positive} if \textit{at least one} instance is known to belong to the target class (also called a ``true positive") \cite{Zare2016MIACE} or to contain a proportion of true target.  The goal of learning under the MIL framework is to train a model which can classify a bag as positive or negative (bag-level classification) or to predict the class labels of individual instances (instance-level classification). This problem formulation fits many remote sensing scenarios and is thus an important area of investigation \cite{Du2017Thesis}.  

%%%%%%%%%%%%%%%%%%%%%%%%%%  Problems with Existing Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Multiple instance learning approaches in the literature can be broadly generalized into two categories: learning a single or set of concepts which effectively describe the positive and/or negative class, or training a classifier to discriminate bags or individual instances.  Existing approaches in the literature have two primary limitations:  (1) MIL methods assume that target instances are already separable in the feature space or that a signature(s) can be learned which represents the target class well, while poorly representing the entirety of the negative class, regardless of potential distribution overlap \textbf{CITE}. (2) These methods fail to address learning complications associated with high-dimensionality.

%%%%%%%%%%%%% \subsection{Manifold Learning and Dimensionality Reduction} %%%%%%%%%%%%%%
These two detrimental assumptions can potentially be resolved with the application of \textit{manifold learning}, also commonly referred to as \textit{dimensionality reduction} (DR), \textit{feature embedding} or \textit{representation learning}.  The goal of manifold learning can be posed as discovering intrinsic (often lower-dimensional) features from the data which meet an overarching objective, such as: preserving variance, finding compressed representations of data, maintaining global or local structure or promoting discriminability in the embedded space \cite{VanDerMaaten2009DRReview,Bengio2014RepLearningReview, Geng2005SupNonlinearDimRed}.  Manifold learning and dimensionality reduction have been studied extensively in the literature and have been used for visualization, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and arguably the most important application, combating the Curse of Dimensionality \cite{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.


Dimensionality plays a significant role in determining class separability.  Enough features should be incorporated as to adequately describe a class of interest, yet too many features may introduce redundancy, and thus be detrimental to the learning process. The Curse of Dimensionality  states that the number of samples needed to characterize the space spanned by an entity grows exponentially with dimensionality \cite{Murphy2012, Theodoris2008KPCA}.  This fact is overwhelming in the context of remote sensing, as it is often both difficult to acquire large quantities of labeled data and there are often only few samples available to describe the target class (such as in air- or space-born HSI target detection). Additionally, dissimilarity metrics often break down in high dimensional spaces, making application of traditional classifiers difficult.  \textit{Therefore,  it is intuitive that approaches should be developed which can combat the Curse  of Dimensionality while also addressing the problems associated with uncertain, imprecise, and ambiguously labeled training data.}

The underlying assumption in using manifold learning for discrimination is that opposing classes either reside on separate manifolds or different regions of a joint, intrinsic manifold, where samples of the same class are close and samples from disparate classes are metrically far.  Essentially, the governing class distributions can be described by hyper-surfaces which follow constraints on properties such as continuity and smoothness \cite{Belkin2004SemiSupLearningRiemannianManifolds}.  The goal of learning in this scenario is to discover embedding functions from the input feature space to a lower-dimensional embedding space where the transformed feature representations allow for improved class discrimination.

%%%%%%%%%%%%%%%%%%%% \subsection{Existing Approaches} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Methods for representation learning have recently gained in popularity because they typically result in high levels of classification accuracy.  Some of these methods  learn features in a supervised manner to obtain more discriminative representations.  However, this learning cannot be done directly in  MIL because of the uncertainty on the labels.  Thus, \textit{adaptation of discriminative  feature learning methods would be beneficial to MIL} \cite{Carbonneau2016MILSurvey}, yet this area of research has scarcely been explored.  The first true experimentation was performed in \cite{Sun2010MIDR}.  In this work, Sun et al. showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  To address these issues, Sun proposed Multiple Instance Dimensionality Reduction (MIDR) which optimized an objective through gradient descent to discover sparse, orthogonal projection vectors into the latent space.  Their approach relied on fitting a distribution of negative instances and applying maximum likelihood estimation.  This approach was later extended in \cite{Zhu2018MIDRSparsity} in attempt to improve sparsity.  Both of these methods rely on fitting a distribution of negative instances and determining positive instances as samples which have low likelihood of belonging to the distribution.  However, these approaches fail when the the target and background instances are similar.  It would, therefore, be beneficial to first transform the instance representations into more discriminable forms.  Most existing approaches in the literature extend LDA to distinguish between positive and negative bags \cite{Chai2014MIDA,Zhu2018MIDRSparsity}.  These methods typically rely on costly optimization procedures to maximize an objective for orthogonal and sparse projection vectors.  The work in \cite{Xu2011MI_Metric_Learning} investigated metric learning on sets of data (where each bag was a set) to learn an appropriate similarity metric to compare bags.  They do not go as far as to discriminate positive instances within the positive bags, nor do they propose positive target concepts.  In fact, virtually all MIL DR methods in the literature are applied, solely, to predict bag-level labels. While bag-level label prediction is useful in many remote sensing applications, such as region of interest (ROI) proposal for anomaly detection, they limit the the ability of learners to classify on the pixel or sample level.  Xu et al. proposed an importance term to weight samples believed to be true target exemplars, as those instances are more important in determining the bag-level label \cite{Xu2011MI_Metric_Learning}.  However, to author's knowledge, no work has been done to investigate instance-level discrimination through manifold learning/ dimensionality reduction.  \textit{With this in mind, the goal of this work is, using only bag-level labels, to find discriminative instance embeddings which allow for accurate sample-level classification.}

%%%%%%%%%%%%%%%%%%%%%%%%%%% \subsection{Proposition of Work} %%%%%%%%%%%%%%%%%%%%%%%%%%
To address these points, I propose the following.  During this project, techniques will be explored for use in instance-level classification given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for discriminative manifold/feature representation learning and dimensionality reduction and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyperspectral and multispectral imagery, LiDAR and more.  \textit{The aim of this project is to develop dimensionality reduction methods which promote class discriminability and are simultaneously capable of addressing uncertainty and imprecision in training data groundtruth.}  The motivating idea is to facilitate instance/concept proposition by increasing instance discriminability under the constraints of multiple instance learning. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item Supervised and semi-supervised manifold learning have proven effective at discovering low-dimensional data representations which provide adequate class separation in the latent space.  However, only a handful of manifold learning procedures consider data that is weakly or ambiguously labeled.  To address this gap in the literature, a method for weakly-supervised manifold learning will be developed. How does this method of manifold construction compare to state-of-the-art manifold learning techniques as well as alternative MI dimensionality reduction methodologies for instance-level label prediction?
	\item Metric embedding has been shown to promote class separability through dimensionality reduction.  Without instance-level labels, a method for ranking loss embedding will be developed in conjunction with Objective 1 to improve class separation of individual instances.  Additionally, a procedure to select the most influential examples for training will be developed.
	\item Do the proposed methods facilitate concept learning/selection?  Using alternative, state-of-the-art MIL approaches, are the selected target instances/concepts more discriminable with the proposed methods than without?  How do the proposed methods compare to the alternatives in terms of representation dimensionality, computational complexity, and promotion of discriminability?
\end{enumerate} 

%%%%%%%%%%%%%%%% \subsection{Datasets} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Experiments will be conducted on both synthetic data and real applications such as target detection, scene understanding and semantic segmentation in remote sensing imagery.  Datasets will include the DSIAC ATR Algorithm Development Database, MUUFL Gulfport, Faces in the Wild (LFW) and benchmark MIL classification datasets \textbf{CITE Datasets}.  Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.




