\chapter{Introduction}
\vspace{1cm}\noindent --(\textbf{Motivation for Sensor Fusion}) \newline
Multi-sensor fusion methods aim to amalgamate data collected from multiple information sources to reduce uncertainty and provide a greater level of understanding than can be obtained from the modalities, individually \textbf{CITE}.  Fusion of multiple sensor sources providing complimentary or reinforcing information is often paramount to the success of remote sensing applications.  For example, consider the scenario described by Du in \textbf{CITE} where hyperspectral (HSI) and LiDaR (light detection and ranging) sensors are used for scene understanding. Hyperspectral cameras can provide a broad range of spectral information about the materials present in a scene, while LiDaR can provide depth information.  If a road and a building rooftop are built with the same material (say, asphalt), hyperspectral information alone may not be able to discriminate between the two. However, incorporating elevation information provided by the corresponding LiDaR depth-map (in conjunction with spectral information) would greatly facilitate the classification problem.  Alternatively, a paved and dirt biking trail might lay at the same elevation and using LiDAR data alone may not be sufficient to distinguish between the two types of roads.  However, the spectral characteristics given by the HSI camera could easily determine an area of ground covered in dirt and another filled by asphalt or cement.  This example demonstrates how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce the uncertainty in a measurement or to obtain a more complete understanding of a scene which can then be used to improve classification or segmentation decisions.
\newline
\vspace{1cm}\noindent --(\textbf{Current Assumptions/ Issues}) \newline
Information fusion approaches make two typical assumptions. (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered (matched together) \textbf{CITE}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \textbf{CITE}.  Assumption (2) states that training labels are available for each data point \textbf{CITE}. 
\newline \vspace{1cm} \noindent --(\textbf{Loss of information, noisy transformations}) \newline
Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial and temporal resolutions, it is not necessarily feasible to convert all data to the same resolution or to map to the same grid \textbf{CITE} and existing co-registration approaches often result in loss of sensor-specific information \textbf{CITE}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process.  Instead of just mapping two sensors of varying resolutions to the same grid, we now have to incorporate time-of-day, weather, high-level scene information, etc., all of which likely operate with different spatial or temporal rates and probably do not align easily.  Typical approaches would throw out data points to match the lowest sampling rate \textbf{CITE}.  However, this mapping is often noisy and results in loss of sensor-specific information.
\newline 
\vspace{1cm}\noindent --(\textbf{Physical limitations, cost of labeling}) \newline
Additionally, even assuming that there is a noiseless/ lossless way to co-register heterogeneous data, standard supervised learning methods require accurate labels for each training data point \textbf{CITE}.  However, data-point specific labels are often unavailable or difficult and expensive to obtain \textbf{CITE}.  For example, when attempting to detect explosive ordinance with a hand-held electromagnetic induction sensor (metal detector), it is difficult to estimate the expected response from a target due to variations in target size, soil conditions, etc.   Additionally, supplementary Global Positioning Systems (GPS) are only accurate on the level of several meters. These effects make it very difficult (potentially even impossible) to obtain exact, sample-level labels for a training target, and thus add an additional level of geometric uncertainty to the sensor fusion process.  
\newline
\vspace{1cm}\noindent --(\textbf{Statement of Work}) \newline
To address these two problems, I propose the following.  During this project, techniques will be explored for use in multi-sensor target detection, target classification, and information fusion given uncertain and imprecise groundtruth.  These methods will be developed as universal approaches for fusion and will be evaluated on a variety of sensor modalities, including: mid-wave IR, visible, hyper-spectral and multi-spectral imagery, as well as LiDaR, ground-penetrating radar and electromagnetic induction sensors.  The aim of this project is to develop fusion methods which can address mis-registration between sensor sources as well as uncertainty and imprecision in training data groundtruth while demonstrating robustness towards outlying and adversarial data points. Roughly, the following research questions will be addressed during the scope of this project:
\begin{enumerate}
	\item At least one method for manifold construction will be developed under the multiple-instance learning framework.  How does this method(s) compare to both traditional and state-of-the-art manifold learning approaches in terms of outlier detection?
	\item Can a joint-representation space be developed between multiple sensors such that there is less sensor-specific information lost, when compared to current fusion approaches? Is there an appropriate metric to measure loss of information through fusion?  If not, what is an appropriate measure?
	\item Traditional fusion methods such as the choquet integral, deep learning and hierarchical mixture of experts utilize individual processing pipelines on each sensor before combining on the sample, feature or decision level.  Will a single, sensor-agnostic processing pipeline on the unified representation (developed in Objective 2) obtain comparable detection/ segmentation results to these alternative approaches?
\end{enumerate} 

Experiments will be conducted on both synthetic data and real applications such as target detection and scene understanding in remote sensing imagery, plant phenotyping, and semantic segmentation. Initial results demonstrate the aptitude of the proposed approaches and suggest further development and evaluation of these methods.

%As an example, it is common in automatic target recognition tasks to utilize both mid-wave infrared (MWIR) and visible-spectrum imagery.  MWIR is able to detect heat in a scene, and while this is an appropriate choice for detecting warmed objects in cold weather or during the nighttime, it often fails during the day when an entire scenes' temperature is elevated.  On the other hand, visible-spectrum imagery can supply highly accurate information in the daytime, but typically fails under poor lighting conditions exhibited during the night.  These two sensors provide complementary information which, when used in the correct context, can greatly increase the performance of an ATR system.  Additionally, auxiliary sensors such as light, temperature, time-of-day, etc. can be used to give confidence values for trusting each sensor's collection.  In this example, each sensor operates under different spatial or temporal resolutions, thus nullifying the homogeneity assumption.

%In existing approaches, is extracted through sensor-specific pipelines before fusion on the sample, feature, or decision level.  There is an assumption under this approach that each pipeline extracts information with an adequate level of precision.  However, overall performance is sensitive to each sensors' processing and information is often lost through co-registration.  As an example, consider a self-driving car outfitted with an RGB camera and LiDAR (light detection and ranging) system. The goal of this computer vision system is to understand the scene around the vehicle so it may navigate safely.  Classical approaches would perform pre-processing, feature extraction, and segmentation/ depth-map estimation on each sensor, individually.  Co-registration would then be performed by mapping a single measurement from the dense LiDAR point-cloud to every pixel.  However, if the wrong LiDAR point is assigned to a pixel, even just a small shift, the car's depth perception can be thrown off.  This is information loss we cannot afford.  A more intuitive method for fusion would be to first project each sensors' collection into a combined representation space before running on a unified pipeline such that the entirety of each sensor's collection is utilized.

%Consider a real scene understanding task where an image is segmented into separate regions containing sky, grass, armored vehicle, mountains, etc.  Existing SOA segmentation techniques require pixel-level labels for each image in the training set.  Obtaining this detail of labeling (especially on a large scale) would cost a labeler or team of labelers a significant amount of time, effort, and money.  The field of multiple-instance learning (MIL) relaxes sample-level label requirements to the region or even image level and has potential to alleviate the cost of labeling, as well as handle spatial and temporal inaccuracies in the collection.  To address these two assumptions, the following is proposed.
