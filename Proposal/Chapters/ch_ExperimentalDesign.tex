\chapter{Experimental Design}
In this chapter, a summary is provided of initial and future experiments using the proposed method for discriminative manifold learning and dimensionality reduction. 

\subsection{Overview}
Two approaches were compared in terms of classification accuracy for nonlinear discriminative dimensionality reduction with weak labels.  Experiments compared Supervised Laplacian Eigenmaps and Supervised Enhanced Isomap  across three parameters of variation: 1.) dimensionality of the embedding space and 2.) feature representation and 3.) amount of label imprecision.  Previous work has explored linear methods for dimensionality reduction under the multiple instance learning paradigm \citep{Sun2010MIDR, Ping2010MILDRMaxMargin, Kim2010LocalDRMIL}.  However, no MIL methods have employed a nonlinear manifold learning approach.  The objective of this work was to demonstrate proof of concept for discriminative nonlinear dimensionality reduction using weak labels.  To prove this point, tests were conducted to embed image chips abstracted from a real-world remote sensing task of vehicle detection in infrared imagery.  Each chip (bag) was provided a positive label if at least one pixel in the image belonged to a vehicle.  A negative label was provided if the chip contained only background pixels.  The objective was to determine if classification accuracy with a K-nearest neighbor classifier was improved in the embedding space, as compared to the high-dimensional input feature space.  The classification performance of each algorithm was tested across a range of embedding dimensionalities using traditional gradient-based features and convolutional neural network features.  These tests were repeated for increasing amounts of label imprecision.  Experiments are detailed in the following.   

\subsection{Description of Data}
The DSIAC MS-003-DB Algorithm Development Database \citep{DSIACATR} contains a publicly-available collection of mid-wave infrared MWIR imagery collected by the US Army Night Vision and Electronic Sensors Directorate (NVESD). The system for this collection was the L3 Cincinnati Electronics Night Conqueror MWIR imager. The system used a fixed field of view (FOV) 300mm lens, resulting in a 3.4x2.6 FOV.  Thus, frames consisted of 640x480 pixels  The objective of the data collection was to capture a set of targets at a minimum of 72 unique aspect angles in range steps of 500 meters from 500 to 5000 meters during both day and night.  This was achieved by marking a circle with a diameters of approximately 100 meters at each range.  Vehicle drivers tracked the circle at around 10 mph. Imagery was taken in real-time for one minute using the  MWIR camera at a frame rate of 30 HZ, resulting in approximately 1800 still frames for each target of interest at each range and light scenario.  The collection used in this work consisted of MWIR video segments of three military and civilian vehicle classes, namely, pickup, SUV and BTR70, at ranges of 1000, 1500 and 2000 meters.  To account for sensor noise and lighting variations,  each frame was clipped at 0.5\% and 98\% before applying MAD normalization.
 
Initial experimentation was performed for bag-level classification.  In this work, bags were represented as image chips extracted from the training frames.  A chip was given a positive label if it contained at least one pixel on target, and a chip was given a negative label if it contained only background pixels.   Figure \ref{fig:chip_extraction} demonstrates the process used for image chip extraction.  The level of imprecision was measured by the amount of non-target pixels contained in a bag.  Essentially, four levels of imprecision were tested.  As seen in Figure \ref{fig:chip_uncertainty}, an uncertainty ratio of 0 denotes that positive bags are constructed from canonical bounding boxes (the tightest possible bounding box on the target), meaning the majority of pixels in the bag lie on target.  This scenario can be considered as supervised learning, and is thus used to provide baseline performance on the dataset. For the remaining levels of label imprecision, chips were extracted in ratios of 51x121 pixels, which corresponds to the largest canonical bounding box present in the dataset across all ranges.  Besides increasing the number of background pixels allowed in each bag, the boxes were  allowed to shift, so long as at least half of the target was contained in the box.  This effectively allowed for potential decrease in the number of pixels on target.  Each chip was up-sampled with bicubic interpolation to 510x720 to maintain consistency amongst the dataset.  To construct the dataset, 10 background chips and 10 chips containing a portion of the target were extracted from every 50th frame in the videos of interest.


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Image Chip Extraction]{}
		\label{fig:chip_extraction}
	\end{figure*}
\end{center}

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Image Chip Extraction]{}
		\label{fig:chip_uncertainty}
	\end{figure*}
\end{center}

\subsection{Feature Extraction}
Two types of features were explored for high-dimensional bag representation in this work, namely, hand-crafted Histogram of Oriented Gradients (HOG) and features pulled from a convolutional neural network (CNN).  

\subsubsection{HOG Features}

\textit{Histogram of Oriented Gradients} (HOG) features are popular object representations in the computer vision literature.  HOG features are essentially descriptors of local object appearance and shape that are characterized by intensity gradients and edge directions \citep{Dalal2005HOG}.  In practice, HOG descriptors are extracted by dividing an image into small spatial windows and accumulating local 1-D histograms of gradient directions or edge orientations over the pixels in the windows.  The combined histogram representations over all the windows form the combined feature vector for the image.  Figure \ref{fig:hog_features} demonstrates the HOG gradient information presented in a target image chip.  It can be observed that the descriptor cues on edge information, which is likely important for the MWIR target detection problem.  HOG descriptors were extracted for each chip in the dataset, thus providing \textbf{INPUT NUMBER}-dimensional input features for each bag.  

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[HOG features]{}
		\label{fig:hog_features}
	\end{figure*}
\end{center}

\subsubsection{CNN Features}
As mentioned in Section \ref{sec:mil_deep_learning}, deep learning has recently been explored under the MIL framework.  The primary assumption is that networks can extract useful feature representations using only bag-level label information \citep{Ghaffarzadegan2018MILVAE}.  This is, typically, at the cost of large quantities of data.  However, methods for representation learning with deep learning typically result in high levels of classification accuracy \citep{Bengio2014RepLearningReview}.  Therefore, we tested the dimensionality reduction frameworks with CNN extracted features.  To extract features, we fine-tuned ResNet18 \citep{He2015ResNet} pre-trained on ImageNet \citep{Deng2009ImageNet} in PyTorch for 2000 epochs in mini-batches of 20 chips.  Image chips were input to the network, which was tasked with predicting the corresponding bag-level labels.  After training, the entire dataset was fed through the network and the corresponding convolutional feature maps were extracted before the first max-pooling layer in order to provide every pixel a corresponding feature vector.  For bag-level classification, the feature maps were concatenated to create a single high-dimensional representation for each bag.  The features extracted from the CNN were \textbf{INPUT NUMBER} dimensional.

\subsection{Algorithm Parameters}
parameters

\subsection{Training and Testing Procedures}

Training

Out of sample testing


\subsection{Evaluation Metrics}


\subsection{Future Experiments}

The work by Wei et al. in \cite{Wei2016ImageBagGenerators} suggested that  for image classification tasks, certain formulations of MIL were better suited than others.  Algorithms such as miGraph, MIBoosting and miFV which assume non-i.i.d samples or take advantage of aggregating properties of bags tend to work better than those which adopt the standard assumption.  The authors of this work recommend miGraph with LBP bag generation or MIBoosting with Single Blob generation for image classification.  Additionally, classification performance tended to increase as the number of instances increased.

\subsubsection{Synthetic Data}

\subsubsection{Comparison to other methods}