\chapter{Experimental Design}
In this chapter, a summary is provided of initial experiments used to show proof of concept for the proposed methods for discriminative manifold learning and dimensionality reduction. 

\subsection{Overview}
Two approaches were compared in terms of classification accuracy for nonlinear discriminative dimensionality reduction with weak labels.  Experiments compared Supervised Laplacian Eigenmaps and Supervised Enhanced Isomap  across three parameters of variation: 1.) dimensionality of the embedding space and 2.) feature representation and 3.) amount of label imprecision.  Previous work has explored linear methods for dimensionality reduction under the multiple instance learning paradigm \citep{Sun2010MIDR, Ping2010MILDRMaxMargin, Kim2010LocalDRMIL}.  However, no MIL methods have employed a nonlinear manifold learning approach.  The objective of this work was to demonstrate proof of concept for discriminative nonlinear dimensionality reduction using weak labels.  To prove this point, tests were conducted to embed image chips abstracted from a real-world remote sensing task of vehicle detection in infrared imagery.  Each chip (bag) was provided a positive label if at least one pixel in the image belonged to a vehicle.  A negative label was provided if the chip contained only background pixels.  The objective was to determine if classification accuracy with a K-nearest neighbor classifier was improved in the embedding space, as compared to the high-dimensional input feature space.  The classification performance of each algorithm was tested across a range of embedding dimensionalities using traditional gradient-based features and convolutional neural network features.  These tests were repeated for increasing amounts of label imprecision.  Experiments are detailed in the following.   

\subsection{Description of Data}
The DSIAC MS-003-DB Algorithm Development Database \citep{DSIACATR} contains a publicly-available collection of mid-wave infrared MWIR imagery collected by the US Army Night Vision and Electronic Sensors Directorate (NVESD). The system for this collection was the L3 Cincinnati Electronics Night Conqueror MWIR imager. The system used a fixed field of view (FOV) 300mm lens, resulting in a 3.4x2.6 FOV.  Thus, frames consisted of 640x480 pixels  The objective of the data collection was to capture a set of targets at a minimum of 72 unique aspect angles in range steps of 500 meters from 500 to 5000 meters during both day and night.  This was achieved by marking a circle with a diameter of approximately 100 meters at each range.  Vehicle drivers tracked the circle at around 10 mph. Imagery was taken in real-time for one minute using the  MWIR camera at a frame rate of 30 HZ, resulting in approximately 1800 still frames for each target of interest at each range and lighting scenario.  The collection used in this work consisted of MWIR video segments of three military and civilian vehicle classes, namely, pickup, SUV and BTR70, at ranges of 1000, 1500 and 2000 meters.  To account for sensor noise and lighting variations,  each frame was clipped at 0.5\% and 98\% before applying MAD normalization.
 
Initial experimentation was performed for bag-level classification.  In this work, bags were represented as image chips extracted from the training frames.  A chip was given a positive label if it contained at least one pixel on target, and a chip was given a negative label if it contained only background pixels.   Figure \ref{fig:chip_extraction} demonstrates the process used for image chip extraction.  The level of imprecision was measured by the amount of non-target pixels contained in a bag.  Essentially, four levels of imprecision were tested.  As seen in Figure \ref{fig:chip_uncertainty}, an uncertainty ratio of 0 denotes that positive bags are constructed from canonical bounding boxes (the tightest possible bounding box on the target), meaning the majority of pixels in the bag lie on target.  This scenario can be considered as supervised learning, and is thus used to provide baseline performance on the dataset. For the remaining levels of label imprecision, chips were extracted in ratios of 51x121 pixels, which corresponds to the largest canonical bounding box present in the dataset across all ranges.  Besides increasing the number of background pixels allowed in each bag, the boxes were  allowed to shift, so long as at least half of the target was contained in the box.  This effectively allowed for potential decrease in the number of pixels on target.  Each chip was up-sampled with bicubic interpolation to 510x720 to maintain consistency amongst the dataset.  To construct the dataset, 10 background chips and 10 chips containing a portion of the target were extracted from every 50th frame in the videos of interest.


\begin{figure*}[t!]
	\hfill
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[height=2.8in]{"data_chips/ratio1/cegr01923_0001_0_patch_scope_p1"}
	\end{subfigure}%
	\centering
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.8in]{"data_chips/ratio1/cegr01923_0001_0_patch_scope_n0"}
	\end{subfigure}
	
	
	\begin{subfigure}[t]{0.5\textwidth}
		\includegraphics[height=2.6in]{"data_chips/ratio1/cegr01923_0001_0_patch_p_2"}
	\end{subfigure}%
	\centering
	~ 
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
		\includegraphics[height=2.6in]{"data_chips/ratio1/cegr01923_0001_0_patch_n_1"}
	\end{subfigure}%
	\caption[Image Chip Extraction]{The top row shows proposed target (left) and background (right) 51x121 image chips for a label uncertainty ratio of 1. The bottom row shows the corresponding chips after being up-sampled with bicubic interpolation to 510x720.  The image containing the truck would be labeled as a positive bag, while the image of foliage would be considered a negative bag. }
	\label{fig:chip_extraction}%
\end{figure*}


\begin{figure*}[t!]
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[height=2.2in]{"data_chips/ratio0/scope01"}
		\caption{Uncertainty 0}
	\end{subfigure}%
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[height=2.2in]{"data_chips/ratio1/cegr01923_0001_0_patch_scope_p2"}
		\caption{Uncertainty 1}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.3\textwidth}
		\includegraphics[height=2.2in]{"data_chips/ratio3/cegr01923_0001_550_patch_scope_p5"}
		\caption{Uncertainty 3}
	\end{subfigure}%
	
	\caption[Image Chip Uncertainty]{Proposed positive image chips as denoted by the green and blue boxes.  The left image corresponds to a label uncertainty of 0, which is the canonical bounding box around the target.  The middle represents a random target chip of base size 51x121 and the right image is a proposed chip of label uncertainty 3, or 3 times the base chip.}
	\label{fig:chip_uncertainty}%
\end{figure*}



\subsection{Feature Extraction}
Two types of features were explored for high-dimensional bag representation in this work, namely, hand-crafted Histogram of Oriented Gradients (HOG) and features pulled from a convolutional neural network (CNN).  

\subsubsection{HOG Features}

\textit{Histogram of Oriented Gradients} (HOG) features are popular object representations in the computer vision literature.  HOG features are essentially descriptors of local object appearance and shape that are characterized by intensity gradients and edge directions \citep{Dalal2005HOG}.  In practice, HOG descriptors are extracted by dividing an image into small spatial windows and accumulating local 1-D histograms of gradient directions or edge orientations over the pixels in the windows.  The combined histogram representations over all the windows form the combined feature vector for the image.  Figure \ref{fig:hog_features} demonstrates the HOG gradient information presented in a target and background image chips.  It can be observed that the descriptor cues on edge information, which is likely an important feature for the MWIR target detection problem.  HOG descriptors were extracted for each chip in the dataset, thus providing 11160-dimensional input features for each bag.  

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{"data_chips/hog_triplets_target_anchor"}
		\caption[HOG features]{Example image chips and corresponding HOG gradient magnitude and direction features over the chip.  In the left column, the chips labeled as ``Anchor" and ``Positive" correspond to positive bags, while th image labeled ``Negative" represents a negative bag because it only contains background pixels.  The images in the right column demonstrate edge direction and magnitude calculated over 16x16 windows across the images.}
		\label{fig:hog_features}
	\end{figure*}
\end{center}

\subsubsection{CNN Features}
As mentioned in Section \ref{sec:mil_deep_learning}, deep learning has recently been explored under the MIL framework.  The primary assumption is that networks can extract useful feature representations using only bag-level label information \citep{Ghaffarzadegan2018MILVAE}.  This is, typically, at the cost of large quantities of data.  However, methods for representation learning with deep learning typically result in high levels of classification accuracy \citep{Bengio2014RepLearningReview}.  Therefore, we tested the dimensionality reduction frameworks with CNN extracted features.  To extract features, we fine-tuned ResNet18 \citep{He2015ResNet} pre-trained on ImageNet \citep{Deng2009ImageNet} in PyTorch for 2000 epochs in mini-batches of 20 chips.  Image chips were input to the network, which was tasked with predicting the corresponding bag-level labels.  After training, the entire dataset was fed through the network and the corresponding convolutional feature maps were extracted before the first max-pooling layer in order to provide every pixel a corresponding feature vector.  For bag-level classification, the feature maps were concatenated to create a single high-dimensional representation for each bag.  The features extracted from the CNN were \textbf{INPUT NUMBER} dimensional.

\subsection{Algorithm Parameters}
Algorithm hyper-parameters were selected by 5-fold cross validation.  It was found that for S-LE, 

The parameters of SE-Isomap were set at

A K-NN classifier was trained for each test scenario with a fixed $k=3$ for simplicity.  

\subsection{Training and Testing Procedures}

\paragraph{Training}
Training times for manifold embedding were inhibited by the sizes of the datasets.  Thus, to make training more manageable, the data was split into 5 folds.  Each algorithm was trained on four folds, validated on the fifth, and tested on a hold-out test set.  This method of testing allowed us to obtain a measure of performance across varying class balances in the dataset.  Algorithms were also trained with the entire balanced dataset at each level of imprecision across a range of embedding-space dimensions.  For each level of label imprecision and embedding dimensionality, image chips corresponding to ranges of 1000m and 2000m were used for training and validation, while chips at ranges of 1500m were used exclusively for testing.


\paragraph{Out-of-Sample Testing}
A known difficulty with nonlinear manifold learning methods is the embedding of \textit{out-of-sample} datapoints.  These points are samples that were not included in the training  dataset.  While methods have been developed to perform this embedding which consider class-label information of the training data or learn the embedding function through the universal function approximation abilities of a neural network \citep{Vural2016OutOfSampleSupManifoldLearning,Mendoza2016ELMOutOfSample}, this work adopts a simple linear reconstruction method for out of sample embedding.  Essentially, a low-dimensional coordinate for a test point is found as a linear combination of the low-dimensional representations of its $k=5$ nearest neighbors in the input feature space.  A bias term is also added to improve reconstruction ability.  


\paragraph{Evaluation Metrics}
Two metrics were used to evaluate algorithmic performance.  The first is simply the total classification accuracy, or the number of correctly labeled bags divided by the total number of bags.  The second is the \textit{Receiver-Operating Characteristic Curve} (ROC).  A ROC curve is a tool used in binary classification, where the x-axis shows the false-positive rate for detection and the y-axis shows the true-positive rate.  True positive rate (TPR) is the measure of true positives over the number of true positive plus false negatives, and false positive rate (FPR) is the number  of false negatives over the number of false negatives plus the number of true positives, or one minus TPR.  A perfect ROC curve would be a vertical line, where 100\% of true positive bags or instances are detected with zero false alarms.


