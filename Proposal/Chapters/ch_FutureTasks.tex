\chapter{Future Tasks}
This chapter provides a table of the planned experiments for the duration of the work proposed.

These are the things I would still like to investigate, specifically, instance-level classification...

\subsection{Future Experiments}

The work by Wei et al. in \cite{Wei2016ImageBagGenerators} suggested that  for image classification tasks, certain formulations of MIL were better suited than others.  Algorithms such as miGraph, MIBoosting and miFV which assume non-i.i.d samples or take advantage of aggregating properties of bags tend to work better than those which adopt the standard assumption.  The authors of this work recommend miGraph with LBP bag generation or MIBoosting with Single Blob generation for image classification.  Additionally, classification performance tended to increase as the number of instances increased.

Choquet integral
Can we use to preference learning to discern instances which are likely to be positive and negative, then use these rankings for embedding?

Metric embedding is often realized through weakly-supervised learning, where instead of labels, the data is accompanied with sets of preferences. While this embedding could, conceptually be easy to implement at the bag level, enforcing instance embeddings is difficult due to the uncertainty on the labels.  However, one could (theoretically) use instance ranking as discussed in Section \ref{sec:MI_Ranking} to provide estimated preference sets on the instances for use in embedding.

\subsubsection{Comparison to other methods}

\begin{table}[H]
	\caption[List of Research Tasks]{List of research tasks and their corresponding locations in the dissertation document.}
	\label{tab:future_tasks}
	\begin{longtable}{ |p{10cm}|p{4cm}|  } 
		\hline
		\multicolumn{2}{|c|}{\textbf{Research Tasks}} \\
		\hline
		\hline
		\textbf{Tasks} & \textbf{Approximate Date of Completion}\\
		\hline
		\multicolumn{2}{|l|}{\textbf{Multiple Instance Manifold Learning}} \\
		\hline
		\begin{enumerate}
%			\setcounter{enumi}{2}
			\item Nonlinear DR method
			\item Method for selecting positive instances
			\begin{enumerate}
				\item Clustering (refer to Bocinsky2019Thesis)
				\item Diverse Density-based
				\item Maximum Likelihood/ KDE of negative instances
			\end{enumerate}
		\end{enumerate} & text\\
		\hline
		\multicolumn{2}{|l|}{\textbf{Multiple Instance Metric Embedding}} \\
		\hline
		\begin{enumerate}
			%			\setcounter{enumi}{2}
			\item Contrastive Siamese Network
			\item Triplet Siamese Network
			\item How to construct pairs/triplets for bags and instances (rely on previous work) 
		\end{enumerate} & text\\
		\hline
		\multicolumn{2}{|l|}{\textbf{Comparison to SOA}} \\
		\hline
		\begin{enumerate}
%			\setcounter{enumi}{2}
			\item Code SOA MI dimensionality reduction methods, namely: MIDR, MidLABS, MIDA and MI-FEAR
			\item Compare to SOA MI dimensionality reduction methods in terms of bag-level and instance-level classification performance (with $k$-NN and SVM), optimal embedding dimensionality and computational complexity.
			\item  Datasets: DSIAC, Gulfport, LFW, instance-level MIL dataset
		\end{enumerate} & text\\
		\hline 
	\end{longtable}
\end{table}

%\begin{enumerate}
%	\item Entry action
%	\item next steps
%\end{enumerate} & text\\