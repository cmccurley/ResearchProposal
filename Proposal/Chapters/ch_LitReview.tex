\chapter{Background}

This chapter provides a literature review of the Multiple Instance Learning framework for learning from weak and ambiguous annotations.  A review is provided on Manifold Learning, including classic approaches, supervised and semi-supervised methods and uses of manifolds for functional regularization. Additionally, this chapter reviews the existing literature on metric embedding, focusing heavily on the utilization of contrastive and triplet-based loss evaluation.  Reviews describe basic terminology and definitions.  Foundational approaches are elaborated and advances are addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

Multiple Instance Learning (MIL) was originally proposed in \citep{Dietterich1996AxisParallelRectangles} as a method to handle inherent observation difficulties associated with drug activity prediction.  This problem, among many others, fits well into the framework of MIL where training labels are associated with sets of data points, called \textit{bags} instead of each individual data points, or \textit{instances}.  Under the \textit{standard MIL assumption}, a bag is given a ``positive" label if it is known that  \textit{at least one} sample in the set represents pure or partial target.  Alternatively, a bag is labeled as ``negative" if does not contain any positive instances \citep{Carbonneau2016MILSurvey}.  Let $\bm{X}=[\bm{x}_1,\dots, \bm{x}_N] \in \mathbb{R}^{D \times N}$ be training data where $D$ is the dimensionality of an instance, $\bm{x}_n$, and $N$ is the total number of training instances.  The data is grouped into K \textit{bags}, $\bm{B} = \{\bm{B}_1, \dots, \bm{B}_K\}$, with associated binary bag-level labels, $\mathcal{L} = \{L_1, \dots, L_K \}$ where 
\begin{align}
	L_k = \begin{cases} 
	1, & \exists \bm{x}_{kn} \in \bm{B}^{+}_{k} \ni  l_{kn} = 1\\
	-1, & l_{kn} = -1 \quad \forall \bm{x}_{kn} \in \bm{B}^{-}_{k} 
	\end{cases}
\end{align} and $\bm{x}_{kn}$ denotes the $n^{th}$ instance in positive bag $\bm{B}^{+}_{k}$ or negative bag $\bm{B}^{-}_{k}$ \citep{Zare2016MIACE} and $l_{kn}$ denotes the instance-level label on instance $\bm{x}_{kn}$.  Figure \ref{fig:bag_eg} demonstrates the concept of MIL bags.  The objective of learning under MIL is, given only bag-level label information, to fit a model which can classify bags as either being positive or negative (bag-level classification) or to predict the class labels of individual instances (instance-level classification).

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[Multiple instance learning bags.]{Placeholder for examples of positive and negative bag concepts}
		\label{fig:bag_eg}
	\end{figure*}
\end{center}

\subsection{Multiple Instance Space Paradigm}
Multiple Instance Learning via Embedded Instance Selection

\subsection{Multiple Instance Concept Learning}

\subsection{Multiple Instance Classification}

\subsection{Multiple Instance Boosting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Learning}

Real-world remote sensing data such as hyperspectral imagery, ground-penetrating radar scans and sonar signals are naturally represented by high-dimensional feature vectors.  However, in order to handle such real-world data adequately, its dimensionality usually needs to be reduced \citep{VanDerMaaten2009DRReview,Belkin2004SemiSupLearningRiemannianManifolds}. The problem considered in this work is discovering feature representations that promote class discriminability for target or anomaly detection.  This is typically achieved in one of two ways.  First, features can be projected into a high-dimensional space (such as a Kernel Hilbert Space) using a kernel function. The second option, which is the focus of this work, is to transform the data into a new (often lower-dimensional) coordinate system which optimizes feature representations for discrimination \citep{Vural2018StudySupervisedManifoldLearning}.


The application of \textit{dimensionality reduction} (DR) has proven useful in myriad applications in the literature, such as: visualization of high-dimensional data, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and reducing the effects of the Curse of Dimensionality \citep{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.  In classification of object entities, it is often assumed that classes can be described by an \textit{intrinsic} subset of representative features which demonstrate geometrical structure \citep{Belkin2006ManReg}. These structures are called intrinsic \textit{manifolds}, and they represent the generating distributions of class objects exactly by the number of degrees of freedom in a dataset \citep{Thorstensen2009ManifoldThesis, Belkin2004SemiSupLearningRiemannianManifolds}.     Consider the example shown in Figure \ref{fig:manifold_eg}.  This classic example demonstrated in \citep{Thorstensen2009ManifoldThesis} shows samples from a pose-estimation dataset \textbf{CITE}.  While each individual image is represented by a vector of features (pixel intensities in this case) in $\mathbb{R}^{4096}$, the dataset only exhibits three degrees of freedom: 1 light variation parameter and 2 rotation angles.  Thus, it is intuitive that the dataset lies on a smooth, intrinsic submanifold spanning three dimensions which inherently capture the degrees of freedom in the data.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[Example pose data manifold.]{Placeholder for example of high D data lying on a low-dimensional sub-manifold.}
		\label{fig:manifold_eg}
	\end{figure*}
\end{center}

The goal of manifold Learning is then to discover embedding functions which take data from the input feature space and transform it into a lower-dimensional (ideally intrinsic) coordinate system (also called a \textit{latent space} in the literature) which captures the ``useful" properties of the data, while enforcing constraints such as smoothness (the transformation function should not produce sporadic images), continuity (no discontinuous points on the hyper-surface), topological ordering (neighbors in the input space should also be neighbors in the embedded space ) or class separability (samples from the same class should fall metrically close to each other in the embedded space and disparate classes should be distinctly far) \citep{Vural2018StudySupervisedManifoldLearning}.

This dissertation focuses on investigating the use of manifold learning to increase instance discriminability in the latent space, where labels are solely provided at the bag-level.   While there is an expansive literature in unsupervised manifold learning methods, this document will pay special attention to both strictly- and semi-supervised methods, since they are typically adaptations of unsupervised approaches, as well as manifold learning under the MIL framework.

\subsection{Definition and General Notation}
Most studies perform classification or regression after applying unsupervised dimensionality reduction.  However, it has been shown that there are advantages of learning the low-dimensional representations and classification/regression models simultaneously \citep{Chao2019RecentAdvancesSupervisedDimRed,Rish2008SupDimRedGLM}.  Considering classification as the main goal of dimensionality reduction, this section provides a summary of the current literature in the area. \newline

Given a data matrix $\bm{X} = [\bm{x}_1, \dots, \bm{x}_N]\in \mathbb{R}^{D \times N}$ where $N$ is the total number of samples and $D$ is the dimensionality of the input feature space, general dimensionality reduction seeks to find a representation $\bm{Z} \in \mathbb{R}^{d \times N}$ with $d \ll D$ that enhances the between-class separation while preserving the intrinsic geometric structure of the data\citep{Vural2018StudySupervisedManifoldLearning}.  In other words, it is assumed that the data lie on a smooth manifold $\mathcal{X}$, which is the image of some parameter domain $\mathcal{Z} \subset \mathbb{R}^{d}$ under a smooth mapping $\Psi : \mathcal{Z} \rightarrow \mathbb{R}^{D}$.  The goal of manifold learning is to discover an inverse mapping to the low-dimensional pre-image coordinates $\bm{z}_n \in \bm{z}$ corresponding to points $\bm{x}_n \in \bm{X}$.  The data matrices $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}]$ and $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ are of size $N \times D$ and $N \times d$, respectively.  Since these low-dimensional data representations are unknown, they are often referred to as \textit{latent} vectors and the span in $\mathbb{R}^d$ is sometimes called the \textit{latent feature space} or \textit{latent space} for brevity \citep{Murphy2012}. The primary difference between traditional, unsupervised manifold learning and supervised approaches is that, in supervised manifold learning, data matrix $\bm{X} $ is accompanied with a corresponding label vector $\bm{l} = [l_1, \dots, l_N]$ indicating the corresponding class labels of each sample in $\bm{X}$. \newline

Manifold learning methods can be subdivided into a wide taxonomy of approaches, with \textit{linear} and \textit{nonlinear} at the root. Nonlinear approaches can be further divided into purely global methods and approaches that capture global structure solely from local information.  We begin with a review of popular linear manifold learning techniques before moving into the realm of nonlinear approaches. Base, unsupervised methods are reviewed along with corresponding supervised and semi-supervised adaptations. 

\subsection{Comparison Table of Manifold Learning Methods}
\subsection{Geneology Image of Manifold Learning Methods from van der Maaten 2009, page  2}

\subsection{Linear Manifold Learning}
A review of linear manifold learning approaches is provided.  Linear approaches are advantageous over nonlinear because they allow for out-of-sample extensions.  In other words, linear transformation matrices are learned which can be easily applied on data not included in the training set.  However, linear approaches are limited in their abilities to capture irregular data surfaces \citep{Kegl2008PrincipalManifoldsTextbook}.  Principal Component Analysis (PCA), Multi-dimensional Scaling (MDS) and Fisher's Linear Discriminant Analysis (LDA) are reviewed.  General approaches are discussed and supervised as well as nonlinear extensions are elaborated. Special focus is given to (LDA), as it is the only inherently supervised technique out of the included approaches.

\subsubsection{Principal Component Analysis (PCA)} \label{sec:PCA}

\paragraph{Unsupervised PCA}
Principal Component Analysis (PCA) is arguably the most popular (and best-studied) technique for dimensionality reduction and manifold learning.  It attempts to learn an orthogonal projection of the input data into a lower-dimensional space, known as the principal subspace, such that the variance of the projected data is maximized \citep{Chao2019RecentAdvancesSupervisedDimRed}.  In other words, each \textit{principal axis}, or \textit{principal component}, of the learned coordinate system is orthogonal to the other principal components.  In summary, the problem of PCA is to discover basis vectors which linearly combine to reconstruct the data.  In practice, data in the input feature space are projected into a new coordinate system of $d$ dimensions, such that the variance along each principal axis is maximized and the reconstruction errors of the data are minimized in the mean-square sense \citep{Thorstensen2009ManifoldThesis}.  Let $V$ be a $d$-dimensional subspace of $\mathbb{R}^{D}$ and let  $\bm{w}_1, \dots, \bm{w}_D$ be an orthonormal basis of $\mathbb{R}^{D}$ such that $\bm{w}_1, \dots, \bm{w}_d$ is a basis of $V$.  The goal of PCA is to find an orthogonal set of basis vectors $\bm{w}_n \in \mathbb{R}^{D}$ and corresponding latent coordinates $\bm{x}_n \in \mathbb{R}^{d}$ such that the average reconstruction error is minimized \citep{Murphy2012}
\begin{align}
	J(\bm{W}, \bm{Z}) = \frac{1}{N}\sum_{n=1}^{N} ||\bm{x}_n - \hat{\bm{x}}_n ||^{2}
\end{align}

\noindent
where $\hat{\bm{x}}_n = \bm{W}\bm{z}_{n}$, subject to the constraint that $\bm{W}$ is \textit{orthonormal}, or that $\bm{w}_{i}^{T}\bm{w}_{j}=0,\forall i \neq j$ and $\bm{w}_{i}^{T}\bm{w}_{i}=1 $.  This is equivalently written as 
\begin{align}
		J(\bm{W}, \bm{Z}) = ||\bm{X} - \bm{W}\bm{Z} ||^{2}_{F}
\end{align}

\noindent
where $\bm{Z}$ is a $N \times d$ matrix with the $\bm{z}_{n}$ in its rows and $||\bm{A}||_{F}$ is the \textit{Frobenius norm} of matrix $\bm{A}$, defined by 
\begin{align}
	||\bm{A}||_{F} &= \sqrt{\sum_{m=1}^{M}\sum_{n=1}^{N}a^{2}_{mn}} = \sqrt{tr(\bm{A}^{T}\bm{A})} = ||\bm{A}(:)||_{2}
\end{align} 

\noindent
As noted by Murphy \citep{Murphy2012}, the optimal solution is obtained by setting $\hat{\bm{W}} = \bm{U}_{d}$, where $\bm{U}_{d}$ contains the eigenvectors corresponding to the $d$ largest eigenvalues of the mean-subtracted, empirical data covariance matrix, $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}(\bm{x}_{n}-\hat{\bm{\mu}})(\bm{x}_{n}-\hat{\bm{\mu}})^{T}$, where $\hat{\bm{\mu}}$ is the empirical data mean.  Therefore, the low-dimensional encoding of the data is given by $\bm{z}_n = \hat{\bm{W}}^{T}\bm{x}_n$, which is the orthogonal, linear projection of the data onto the column space spanned by the eigenvectors of the $d$ largest eigenvalues of the empirical data covariance.  Alternatively, 


Assumes Gaussian distributions
Typically, the data is standardized before applying PCA, as it can be misled by directions in which the variance is high simply because of the measurement scale.

The example shown in figure \ref{fig:pca_eg} demonstrates the projection of 2-dimensional data onto the first principal axis.  As can be seen from the figure, the first principal axis corresponds to the direction of maximal variance of the data.  PCA from the viewpoint of variance maximization is often called the \textit{analysis view} of PCA \citep{Murphy2012}.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[PCA Example.]{Placeholder for PCA projection example.}
		\label{fig:pca_eg}
	\end{figure*}
\end{center}


PCA has been successfully applied to a large  number of domains such as face recognition, coin classification and seismic series analysis \citep{VanDerMaaten2009DRReview}. However PCA suffers from a few drawbacks.  First, the dimensionality of the covariance matrix is proportional to the dimensionality of the data points.  As a result, the computation of the eigenvectors may be infeasible or untrustworthy (singularity) for high-dimensional data.  Additionally, PCA focuses mainly on preserving large pairwise distances between data samples instead of retaining local relationships, which may be important in certain applications \textbf{SHOW SWISS ROLL EXAMPLE AGAIN?}.

Many extensions and alternative viewpoints have been made to PCA, such as creating a nonlinear version, a supervised version and looking at it as a factor analysis problem \textbf{CITE}.  A few adaptations to PCA are discussed in later sections.




\subsubsection{Multi-Dimensional Scaling (MDS)} 

\paragraph{Unsupervised MDS} \label{sec:MDS}
Most modern manifold learners have theoretical and algorithmic roots in one of three basic dimensionality reduction techniques: PCA, K-means and \textit{Multidimensional Scaling} (MDS). Whereas PCA looks for linear projection bases which are constructed from the eigenvectors of a data covariance or scatter matrix, MDS tries to find a linear projection that preserves pairwise distances as well as possible. This idea is demonstrated by Figure \ref{fig:mds_example}, where the pairwise distances between samples in the 3-dimensional space are preserved in the 2-dimensional embedding space. While MDS does not construct an embedded manifold explicitly, it holds the status of being the grandfather of ``one-shot" (non-iteratve) manifold learners, such as Isomap and Locally Linear Embedding (LLE), which are discussed later in this literature review \citep{Kegl2008PrincipalManifoldsTextbook}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Example of MDS distance preservation.]{Example of MDS distance preservation.}
		\label{fig:mds_example}
	\end{figure*}
\end{center}

The steps of MDS correspond exactly to those of PCA except that, instead of a scatter matrix $\bm{S}=\frac{1}{N}\bm{X}\bm{X}^{T}$, MDS operates with a positive semi-definite, dissimilarity matrix $\bm{D} \in \mathbb{R}^{N \times N}$, where $N$ is the number of data samples and a real, symmetric Gram matrix $\bm{K} = \bm{X}^{T}\bm{X}$ (inner-product matrix) where $\bm{K}_{mn}$ is the inner product between $\bm{x}_{m}$ and $\bm{x}_{n}$.  The problem of MDS is posed as finding $d$-dimensional Euclidean coordinates for each sample $\bm{x}_{n}$ in dataset $\bm{X}$ such that the Euclidean distances in the low-dimensional embedding space are proportional to the pairwise distances in the input space \citep{Thorstensen2009ManifoldThesis,Sorzano2014DRReview}. While the literature poses several cost functions for this task, this review focuses on classical MDS, which is described as follows:

First, the pairwise distance matrix $\bm{D}$ is computed such that 
\begin{align}
	\bm{D}_{mn} = \mathcal{D}_{\mathcal{X}}(\bm{x}_{m},\bm{x}_{n}) = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} = (\bm{x}_{m} - \bm{x}_{n})^{T}(\bm{x}_{m} - \bm{x}_{n}) 
\end{align}
\noindent
where $\mathcal{D}_{\mathcal{X}}(\cdot,\cdot)$ is a chosen dissimilarity metric (Euclidean distance for classical MDS).  Then, the double-centered Gram matrix $\bm{K}$ is computed by
\begin{align}
	\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}
\end{align}
\noindent
where 
\begin{align}
	\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}
\end{align}
\noindent
with $\mathbb{I}_{N}$ denoting the $N \times N$ identity matrix and $\bm{e}= (1, \dots, 1)^{T}$ the $N \times 1$ column vector of all ones.  Multiplying $\bm{D}$ on both sides by $\bm{H}$ performs \textit{double centering}, which subtracts the row and column means from $\bm{D}$ (and adds back the global mean which gets subtracted twice), so that both the row and column means of $\bm{K}$ are equal to zero.  The objective is to find $\bm{Z}= \{\bm{z}_{1}, \dots, \bm{z}_{N}\} \in \mathbb{R}^{d}$ which minimizes the objective 
\begin{align}
	J(\bm{D}_{\bm{X}},\bm{D}_{\bm{Z}}) = ||\bm{K}_{\bm{X}} - \bm{D}_{\bm{Z}} ||^{2} = \left |\left|-\frac{1}{2} \bm{H} (\bm{D}_{\bm{X}} - \bm{D}_{\bm{Z}}) \bm{H} \right |\right|^{2}
\end{align}
Similarly to PCA, this can be solved by a generalized eigenvalue problem 
\begin{align}
	\bm{K}v = \lambda v
\end{align}
\noindent
such that 
\begin{align}
	\bm{Z} = \bm{V}\bm{\Lambda}^{\frac{1}{2}}
\end{align}
\noindent
with $\bm{\Lambda}^{\frac{1}{2}} = diag(\sqrt{\lambda_{1}},\dots,\sqrt{\lambda_N})$ being a diagonal matrix with entries equal to the square roots of the eigenvalues of $\bm{K}$ sorted from largest to smallest ($\lambda_{1} \geq  \lambda_{2} \geq \dots \geq 0$), and $\bm{V} = \{ \bm{v}_{1}, \dots, \bm{v}_{N} \}$ the corresponding eigenvectors.  The low-dimensional embedding coordinates $\bm{Z} \in \mathbb{R}^{N \times d}$ are obtained by $\bm{Z} = \{\sqrt{\lambda_{1}}\bm{v_{1}}, \dots, \sqrt{\lambda_{d}}\bm{v_{d}} \}$ \citep{Chao2019RecentAdvancesSupervisedDimRed}.

It has been proven that the eigenvalues of Gram matrix $\bm{K}$ and covariance $\bm{S}$ are the same, and that the space spanned by MDS and PCA are the identical for any $d \leq rank(\bm{K}) = rank(\bm{S})$.  This implies that a rotation matrix $\bm{A}$ could be found such that $\bm{A}^{T}\bm{Z}_{MDS} = \bm{Z}_{PCA}$ \citep{Sorzano2014DRReview}.  Additionally, MDS can be computed even if the data observation matrix $\bm{X}$ is unknown.  All that is needed is the Gram matrix or a dissimilarity matrix.  This feature potentially allows MDS to be applied in a variety of data-sensitive and privacy-concerned scenarios. 

A pitfall of MDS is that it focuses on retaining global pairwise distances as opposed to local distances, which are typically much more important for capturing the geometry of the data \citep{VanDerMaaten2009DRReview}.  Several MDS variants have been proposed to address this weakness.  A popular variant is known as \textit{Sammon Mapping} and is discussed in Section \ref{sec:sammon_mapping}. 

\paragraph{Supervised MDS}
As with most manifold learning methods in the literature, MDS does not inherently consider class information when learning the embedding function.  In attempt to promote class separability in the  low-dimensional embedding space, Witten et al. proposed a \textit{Supervised Multidimensional Scaling} (SMDS) \citep{Witten2011SuperMDS}.  this method follows the idea of traditional MDS where the goal is to find low-dimensional coordinate or \textit{configuration points} $\bm{z}_{n} \in \mathbb{R}^{d}$, such that pairwise distances in the input feature space are preserved in the  embedding space.  Incorporating class label information, the goal of SMDS is to not only preserve distances, but ensure the coordinate values $z_{mk} > z_{nk}$ when $l_{m} > l_{n}, \quad \forall k = 1, \dots, d$, where $l$ are the instance-level labels and $d$ is the dimensionality of the embedding space.  Considering the binary target classification case, SMDS can be formulated as
\begin{align}
	\min_{\bm{Z}} \quad \frac{1}{2}(1-\alpha)\sum_{m=1}^{N}\sum_{n=1}^{N}(\bm{D}_{mn} - ||\bm{z}_{m} - \bm{z}_{n} ||^{2}) + \alpha \sum_{m:l_{m}=1} \sum_{n:l_n=2} \sum_{k=1}^{d}\left(\frac{\bm{D_{mn}}}{\sqrt{d}} - (z_{nk} - z_{mk})^{2} \right)
\end{align}
This objective has two terms.  The first is the traditional metric MDS \textit{stress}.  This term attempts to ensure that the Euclidean distances of two points in the embedding space is the same as the dissimilarity between the points in the input feature space.  The second term is the supervised term which enforces that each dimension of the embedded configuration points be larger if belonging to the class with the larger label, and smaller if belonging to the class with a smaller-valued label.  The term $\alpha \in [0,1]$ is a tuning parameter.  When $\alpha = 0$, the objective reduces to the MDS stress function.  As $\alpha$ increases, however, the objective becomes increasingly more supervised, focused on ensuring class separation of the training data.

A least square regression was applied to estimate the embedding function for out-of-sample  test points.  SMDS was successfully applied to tasks in data visualization, bipartite ranking and classification of prostate data and USPS handwritten digits.

\subsubsection{Non-negative Matrix Factorization (NMF)}

\subsubsection{Fisher's Linear Discriminant Analysis (LDA)} \label{sec:LDA}

\paragraph{Classical LDA}
\textit{Linear Discriminant Analysis} (LDA) is a popular method for supervised, linear dimensionality reduction.  LDA currently forms the basis for Multiple Instance Learning dimensionality reduction methods exhibited in the literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity,Xu2011MI_Metric_Learning}. Whereas PCA tries to project data into a space which maximizes variance, LDA considers class label information and tries to find a transformation which both maximizes between-class (inter-class) dissimilarity and minimizes between-class (intra-class) compactness \citep{Yan2007GeneralGraphEmbeddingFramework,Chao2019RecentAdvancesSupervisedDimRed, Sun2010MIDR, Murphy2012}.   This is done by maximizing the ratio between the inter-class $\bm{S}_{b}$ and intra-class $\bm{S}_{w}$ scatter matrices, defined as:
\begin{align}
	\bm{S}_{w} = \sum_{k=1}^{K}\bm{S}_{k}
\end{align}
\begin{align}
	\bm{S}_{k} = \sum_{n \in C_{k}}(\bm{x}_{n} - \hat{\bm{\mu}_{k}})(\bm{x}_{n} - \hat{\bm{\mu}_{k}})^{T}
\end{align}
\begin{align}
	\bm{S}_{b} = \sum_{k=1}^{K}N_{k}(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})^{T}
\end{align}
\noindent
Here, $\bm{S}_{w}$ is the global within-class scatter matrix which is defined as the sum over each individual class' scatter matrix $\bm{S}_{k}$, and $\bm{S}_{k}$ is essentially an outer product between all samples belonging to class $C_{k}$ after subtracting the respective empirical class mean $\hat{\bm{\mu}_{k}}$.  This scatter matrix would be the class covariance if it was normalized by the number of samples $N_{k}$ in class $C_{k}$.  However, this normalization constant does not affect the final solution and can thus be ignored.  The between-class scatter $\bm{S}_{b}$ is defined by the sum of outer products of the differences between the empirical class means $\hat{\bm{\mu}_{k}}$ and the global data mean $\hat{\bm{\mu}}$, weighted by the number of samples in each class.  The objective of LDA is then to solve for $\bm{W}^{*}$ which maximizes the ratio $J(\bm{W})$:
\begin{align}
	\bm{W}^{*} = \arg\min_{\bm{W}} J(\bm{W}) =  \arg\max_{\bm{W}} \frac{|\bm{W}^{T}\bm{S}_{b}\bm{W}|}{|\bm{W}^{T}\bm{S}_{w}\bm{W}|}
\end{align}
It has been shown that the optimal projection matrix $\bm{W}^{*}$ is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of the generalized eigenvalue problem 
\begin{align}
	\bm{S}_{b}\bm{w}=\lambda \bm{S}_{w}\bm{w} \Rightarrow \bm{S}_{w}^{-1}\bm{S}_{b}\bm{w} = \lambda\bm{w}
\end{align}
Since $\bm{S}_{b}$ is the sum of $K$ matrices of rank $\leq 1$, this implies that $\bm{S}_{b}$ will be of rank $(K-1)$ or less and only $(K-1)$ of the eigenvalues $\lambda$ will be non-zero.  
A low-dimensional coordinate representation $\bm{z}_{n} \in \mathbb{R}^{(K-1)}$ of sample $\bm{x}_{n} \in \mathbb{R}^{D}$ is given by the linear projection of $\bm{x}_{n}$ onto the hyper-plane parameterized by $\bm{W}^{*}$, $\bm{z}_{n} = \bm{W}^{*T}\bm{x}_{n}$  It should be noted that for LDA, the dimensionality of the latent space is not a free-parameter, but is always fixed at $d=(K-1)$, or one less than the number of classes present in the dataset.  Equivalently, LDA can be derived by maximum likelihood for normal class-conditional densities where the covariances for each class are assumed to be equivalent \citep{Murphy2012}.  For the special case of binary target classification, the LDA transformation will place every sample onto a single line in 1-dimension, and thus the LDA solution can be simplified:
\begin{align}
	\bm{w}^{*} = \arg\max_{\bm{w}} \frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}} = \bm{S}_{w}^{-1}(\hat{\bm{\mu}_{2}} - \hat{\bm{\mu}_{1}})
\end{align}  
where $\hat{\bm{\mu}_{1}}$ and $\hat{\bm{\mu}_{2}}$ are the empirical means for classes $1$ and $2$, respectively.  Figure \ref{fig:lda_example} demonstrates the differences between PCA and LDA.  While PCA projects data onto the axes exhibiting the maximal variation, LDA projects the data into a space which attempts to simultaneously enforce between-class separation and within-class compactness. 

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[LDA Example.]{LDA example.}
		\label{fig:lda_example}
	\end{figure*}
\end{center}

Although LDA is the basis for large number of discriminative dimensionality reduction approaches, it does not guarantee class separation in the embedding space.  For example, LDA projects data into a space of at most $(K-1)$ dimensions, however, more features may be necessary for adequate class discrimination.  Additionally, LDA is a parametric method which assumes unimodal Gaussian likelihoods.  This implies that it may not be able to preserve complex data structure.  Finally, LDA will fail if the discriminatory information is contained in the variance of the data instead of the mean.  Despite these pitfalls, LDA  has been successfully applied to object detection and recognition tasks \citep{Wang2016OrthogonalLDA}  Many variations of LDA have been developed, such as Non-parametric LDA \citep{Fukunaga1983NonparametricLDA}, Orthonormal LDA \citep{Wang2016OrthogonalLDA}, Generalized LDA \citep{Baudat2000GeneralizedDiscriminantAnalysis} and Multilayer Perceptrons \citep{Webb1990MLPLDA}.  Additionally, LDA serves as the foundation for all of the Multiple Instance Learning dimensionality reduction approaches in the current literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity}.

\subsubsection{Locality Preserving Projection (LPP)}



\subsection{Nonlinear Manifold Learning}

Linear methods such as PCA and MDS are convenient for projecting out-of-sample test points into the embedding space.  However, they are unable to capture the structure of data that are sampled from nonlinear manifolds \citep{Kegl2008PrincipalManifoldsTextbook}. This section will discuss a variety of nonlinear dimensionality reduction and manifold learning approaches.  All methods reviewed assume the data is distributed along a $d$-dimensional sub-manifold $\mathcal{X}$ embedded in $\mathbb{R}^{D}$.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Example of a nonlinear manifold.]{Example of a nonlinear manifold.}
		\label{fig:nonlinear_manifold}
	\end{figure*}
\end{center}

\subsubsection{Kernelization}
Although each of the manifold learning techniques previously discussed are inherently linear, nonlinear adaptations have been made.  One easily extendable approach is to utilize kernel functions as means to provide nonlinearity in the embeddings. 

\paragraph{Kernels} A \textit{kernel function} is a real-valued function of two arguments $\kappa(\bm{x},\bm{x}') \in \mathbb{R}$, for $\bm{x},\bm{x}' \in \mathcal{X}$, which maps vectors from the input feature space to a single value in $\mathbb{R}$.  The function is typically symmetric (i.e.  $\kappa(\bm{x},\bm{x}') =  \kappa(\bm{x}',\bm{x})$) and non-negative (i.e.  $\kappa(\bm{x},\bm{x}') \geq 0$), which implies that it can be interpreted as a measure of similarity \citep{Murphy2012}.  The notion of kernels is very useful in certain applications where data representation is not straightforward, such as representing text documents or molecular structures which can have variable length.  Additionally, this allows algorithms to operate directly on the kernel representations.  This is useful in data-sensitive scenarios where direct access to the data may not be available.

A popular choice of kernel in manifold learning is the \textit{radial basis function} (RBF) kernel, defined as:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \exp \left( - \frac{|| \bm{x} - \bm{x}' ||^{2}}{\beta} \right)
\end{align}
\noindent
where $\beta$ is the bandwidth of the isotropic function.  Another popular kernel for text classification is \textit{cosine similarity}, defined by:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \frac{\bm{x}^{T}\bm{x}'}{||\bm{x}||_{2}||\bm{x}'||_{2}} 
\end{align}
\noindent
This kernel measures the cosine of the angle between vectors $\bm{x}$ and  $\bm{x}'$ after scaling them onto the unit hyper-sphere.  If  $\bm{x}$ and  $\bm{x}'$ are strictly positive vectors (counts in the bag-of-words model, for example), then the kernel provides values in $[0,1]$, where a value of $0$ means the feature vectors are orthogonal and, therefore, have no features in common, and a value of $1$ means the vectors are the same. 

Some of the nonlinear manifold learning methods in the literature require the kernel function to satisfy the requirement that the \textit{Gram matrix}
\begin{align}
	\bm{K} = 	
	\begin{bmatrix}
		\kappa(\bm{x}_{1},\bm{x}_{1}) &  \dots & \kappa(\bm{x}_{1},\bm{x}_{N}) \\
		 & \vdots  & \\
		\kappa(\bm{x}_{N},\bm{x}_{1}) & \dots & \kappa(\bm{x}_{N},\bm{x}_{N})
	\end{bmatrix}
\end{align}
\noindent
be positive definite for any set of inputs $\{\bm{x}_{n}\}_{n=1}^{N}$.  This type of kernel is called a \textit{Mercer kernel} or \textit{positive definite kernel}.  The importance of the Mercer Kernel is the following result, known as \textit{Mercer's theorem}.  This theorem states that if the Gram matrix is positive definite, its eigenvector decomposition can be written as
\begin{align}
	\bm{K} = \bm{U}^{T}\bm{\Lambda}\bm{U}
\end{align}
As derived by Murphy and Liu et al. \citep{Murphy2012,Liu2010KernelAdaptiveFiltering}, it then follows that each entry of $\bm{K}$ can be computed as 
\begin{align}
	\kappa(\bm{x},\bm{x}') = \phi(\bm{x})^{T}\phi(\bm{x}') = <\phi(\bm{x}),\phi(\bm{x}')>
	\label{eq:kernel_inner_product}
\end{align}
meaning that the entries of the kernel matrix can be defined by the inner product of some feature vectors that are implicitly defined by the eigenvectors $\bm{U}$.  If the kernel is Mercer, then there exists a function $\phi$ which maps $\bm{x} \in \mathcal{X}$ to $\mathbb{R}^{D}$ such that Equation \ref{eq:kernel_inner_product} holds.  Additionally, $\phi$ depends on the eigenfunctions of $\kappa$, meaning that $D$ is a potentially infinite dimensional space.
Additionally, instead of representing feature vectors in terms of kernels $\phi(\bm{x}) = [\kappa(\bm{x},\bm{x}_{1}), \dots, \kappa(\bm{x},\bm{x}_{N})]$, algorithms can instead work with the input feature vectors $\bm{x}$ by replacing all inner products $<\bm{x},\bm{x}'>$ with a call to the kernel function $\kappa(\bm{x},\bm{x}')$.  This is called the \textit{kernel trick}, and it turns out that many algorithms can be kernelized in this way.

Kernel functions play an important role in the dimensionality reduction literature for both applying nonlinearity to inherently linear problems and in defining similarity measures for graph-based manifold learning methods.  The kernelization of three traditionally linear manifold learning methods, namely: PCA, MDS and LDA, is briefly described in the following.

\paragraph{Kernel PCA}

Section \ref{sec:PCA} showed how PCA could be used to compute linear low-dimensional embeddings of data.  This process involved finding the eigenvectors of the empirical data covariance matrix $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}\hat{\bm{x}}_{n}\hat{\bm{x}}_{n}^{T} = \frac{1}{N}\hat{\bm{X}}^{T}\hat{\bm{X}}$, where $\hat{\bm{x}}_{n} = \bm{x}_{n} - \hat{\bm{\mu}}$ is the mean subtracted feature vector.  However, PCA can also be computed by finding the eigenvectors of the inner product matrix $\bm{X}\bm{X}^{T}$.  This allows the production of nonlinear embeddings by taking advantage of the kernel trick.  This approach is known as Kernel PCA (Schoelkopf 1998). 

\paragraph{Kernel MDS}

\paragraph{Kernel FDA (KDA)}

\subsubsection{Graph-based Methods}
Nonlinear dimensionality reduction methods typically rely on the use of computational graphs.  These graphs represent data structure pooled from local neighborhoods of samples.  \textit{Spectral graph theory} focuses on constructing, analyzing and manipulating graphs.  It has proved useful for object representation, graph visualization, spectral clustering, dimensionality reduction and numerous other applications in chemistry, physics, signal processing and computer science \citep{Shuman2013SignalProcessingGraphs, Bengoetxea2002ThesisGraphMatching}.   An overview of computational graphs as well as prominent methods for graph construction in manifold learning are presented. Additionally, geodesic distance approximation from pairwise distances is reviewed.

\paragraph{Terminology}
Many dimensionality reduction methods in the literature are interested in analyzing relationships between samples defined on an undirected, weighted graph $G = \{ \mathcal{V}, \mathcal{E}, \bm{W} \}$, which consists of a finite set of \textit{vertices}  $\mathcal{V}$ (also called \textit{nodes} or \textit{points}) with cardinality $\mathcal{V}=N$, a set of \textit{edges} $\mathcal{E} \subset \mathcal{V} \times \mathcal{V} = [\mathcal{V}]^{2}$ (also known as \textit{arcs} or \textit{lines}) and a weighted \textit{adjacency} or \textit{affinity} matrix $\bm{W}$ \citep{Shuman2013SignalProcessingGraphs, Livi2013GraphMatchingProblem, Bengoetxea2002ThesisGraphMatching}.  The the size or \textit{order} of a graph is defined by the number of nodes $|\mathcal{V}|$ and edges $|\mathcal{E}|$.  If two vertices in $G$, say $\bm{u},\bm{v} \in \mathcal{V}$, are connected by an edge $e \in \mathcal{E}$, this is denoted by $e=(\bm{u},\bm{v})$ and the two vertices are said to be \textit{adjacent} or \textit{neighbors}.  When edges do not have a direction, they are coined as undirected.  A graph solely containing this type of connection is termed as an \textit{undirected graph}.  When all edges have directions, meaning $(\bm{u},\bm{v})$ and $(\bm{v},\bm{u})$ are distinguishable, the graph is said to be \textit{directed}.  In the literature, the term \textit{arc} is typically used to denote connections between nodes in directed graphs, while \textit{edge} is used when they are undirected.  The graph-based methods included in this literature review focus on analyzing affinities between data samples in undirected graphs.  Moreover, a \textit{path} between any two nodes in $\bm{u},\bm{u'} \in \mathcal{V}$ is a non-empty sequence of $k$ different vertices $<\bm{v}_{0}, \bm{v}_{1}, \dots, \bm{v}_{k}>$ where $\bm{u}=\bm{v}_{0},\bm{u}'=\bm{v}_{k}$ and $(\bm{v}_{i-1},\bm{v}_{i})\in \mathcal{E}$, $i=1,2,\dots,k$.  Additionally, a graph is said to be \textit{acyclic} if there are no cycles between its edges, regardless of whether it is directed or undirected.  

When using graphs for dimensionality reduction, vertices usually represent features of individual samples, and edges express relationships between them.  The most straight-forward way to construct a graph is to instantiate edges between every vertex in the graph, where each edge is weighted by the distance between the vertices it connects according to a pre-defined metric.  This type of graph is called \textit{full mesh}. Weights on edges are captured in the graph adjacency matrix $\bm{W}$.  When weights are not naturally defined by an application, a common way to the define the weight of an edge connecting vertices $\bm{u} \sim \bm{u}'$ is by a symmetric affinity function $W_{\bm{u},\bm{u}'} = K(\bm{u};\bm{u}')$; typically a \textit{radial basis function (RBF)} or \textit{heat kernel}, defined as:
\begin{align}
\bm{W}_{\bm{u},\bm{u}'}= w_{\bm{u},\bm{u}'} = \exp \left ( - \frac{|| \bm{u} - \bm{u}' ||^{2}}{\beta}  \right )
\end{align}
\noindent
where $\beta$ is the non-negative \textit{bandwidth} of the kernel.  Vertices will have a nonzero weight only if they fall within the nonzero mapping domain of the kernel. Additionally, a threshold could be set to truncate the weights of neighbors far from individual samples.


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Examples of graphs.]{Examples of K-nearest neighbor and $\epsilon$-ball graphs}
		\label{fig:examples_of_graphs}
	\end{figure*}
\end{center}


\paragraph{$\bm{K}$-Nearest Neighbor Graph}
In a $K$-nearest neighbor graph, every data point (vertex) $\bm{x}_n \in \bm{X}$ is connected by edges to its $K$-nearest neighbors, where $K \in \mathbb{Z}^{+}$ is fixed. An example of a $K$-nearest neighbor graph is depicted in a of Figure \ref{fig:examples_of_graphs}. The downside of this graph is that it might impose edges between neighbors that should not actually be connected, as in the case where a sample is metrically distant from all of its nearest neighbors. Although, this feature may actually be useful in domains such as outlier detection, where low adjacency weights indicate that the sample is far form the sampling distribution.  Two alternative $K$-nearest neighbor graphs, a  symmetric and mutual neighbors, might instead by utilized.  In the symmetric $K$-nearest neighbors graph, two vertices $\bm{u}$ and $\bm{u}'$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{or} $\bm{u}'$ is among the neighbors of $\bm{u}$.  The mutual $K$-nearest neighbors graph, however, only connects vertices $(\bm{u},\bm{u}')$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{and} $\bm{u}'$ is among the $K$-nearest neighbors of $\bm{u}$.  The weights on each edge are provided as the similarity of the adjacent nodes.

\paragraph{$\bm{\epsilon}$-Neighborhood Graph}
Another method for graph construction is to use $\epsilon$-neighborhoods (or $\epsilon$-balls).  In this graph, two vertices $(\bm{u},\bm{u}')$ are connected by an edge if and only if the distance between them is equal to or smaller than some value $\epsilon$, $\mathcal{D}_{\mathcal{U}}(\bm{u},\bm{u}') \leq \epsilon$.  This idea is represented in b of Figure \ref{fig:examples_of_graphs}.  In both the $K$-nearest and $\epsilon$-neighborhood graphs, a parameter controlling the number of edges in the graph, $K$ or $\epsilon$, must be chosen.   These parameters are highly influential for graph construction and can thus greatly affect dimensionality reduction quality.  Contrary to the $K$-nearest neighbor graph, an $\epsilon$-neighborhood will not create connections between distant vertices.  However, when the data is sampled sparsely from a highly-curved manifold, the $\epsilon$-neighbor graph will not be able to appropriately capture the geometry \citep{Thorstensen2009ManifoldThesis}.

\paragraph{Geodesic Distance Approximation}
The ultimate goal of manifold learning is to uncover an underlying low-dimensional sub-manifold which is embedded in $\mathbb{R}^{D}$.  Many dimensionality reduction methods in the literature discover projections of data into a low-dimensional space which preserve topological ordering of the data \citep{Kegl2008PrincipalManifoldsTextbook}.  These processes require a notion of distance between samples.  \textit{Euclidean distance} is a popular metric which captures the straight-line disparity between two points. As shown in Figure \ref{fig:geodesic_distance}, however, samples that are actually distant on the manifold may appear deceptively close in the high-dimensional input feature space, as measured by Euclidean distance \citep{Tenenbaum2000Isomap}. \textit{Geodesic distance}, also called \textit{curvilinear} or \textit{shortest-path distance}, Figure \ref{fig:geodesic_distance}, on the other hand, follows the curvature of a manifold and may provide a better measure of dissimilarity between data samples.  Geodesic distance can be estimated by the shortest path through a graph constructed by assuming the distances between neighbors is locally Euclidean \citep{Sorzano2014DRReview}.  This can be conceptualized by a simple example.  The Earth is a sphere and naturally has curvature.  Two people standing in a room, however, would estimate the distance between themselves by a straight line.  Thus, in a very local region on the Earth, the measure of curvature would be negligible and the true distances between objects could be estimated with Euclidean distance. The same concept is true for manifolds where, if data is sampled densely enough, geodesic distance can be approximated by the shortest-path through a neighborhood graph where the dissimilarities between neighbors is assumed to be locally Euclidean.  Geodesic distance can be estimated efficiently by methods such as Dijskstra's or Floyd's shortest-path algorithms \citep{Tenenbaum2000Isomap}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Demonstration of geodesic distance]{Demonstration of geodesic distance}
		\label{fig:geodesic_distance}
	\end{figure*}
\end{center}

\subsubsection{General Graph Embedding Framework}

\subsubsection{Isomap}
\paragraph{Traditional Isomap}
While MDS has proven to be successful in a variety of applications, it suffers from the fact that is solely aims to retain pairwise Euclidean distances and does not consider the distributions of neighboring samples.  This implies that MDS is not able to capture the geometry of high-dimensional data which lies on or near to a curved manifold, such as the Swiss roll dataset \citep{VanDerMaaten2009DRReview,Chao2019RecentAdvancesSupervisedDimRed}. Isometric Feature Mapping (Isomap) \citep{Tenenbaum2000Isomap} is a technique which resolves this problem by attempting to preserve pairwise geodesic distances between datapoints.  Isomap can be considered as a generalization of classical MDS in which the pairwise distance matrix is replaced by a matrix of pairwise geodesic distances approximated by distances in the graph \citep{Thorstensen2009ManifoldThesis}.  The classic, unsupervised algorithm consists of a few steps:

\begin{enumerate}
\item Given a set of input data $\bm{X} = \{\bm{x}_{n}\}^{N}_{n=1} \subset \mathbb{R}^{D}$, construct a sparse neighborhood graph (such as the $K$-nearest  or $\epsilon$-ball graphs discussed previously) where each edge is weighted by the Euclidean distance between the neighbors it connects:
\begin{align}
	\bm{W}_{mn} = w_{mn} = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} 
\end{align}
where $\bm{W}$ is the graph adjacency matrix.  

\item Next, the geodesic distances between all pairs of samples is computed by finding the shortest paths between the points through the graph.  This is commonly done with Dijkstra's or Flyod's shortest-path algorithms \citep{Tenenbaum2000Isomap}.  

\item These geodesic distances form a pairwise distance matrix which is substituted into classical MDS as described in Section \ref{sec:MDS}.  This provides the low-dimensional embedding coordinates $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}] \in \mathbb{R}^{N \times d}$ of high-dimensional input data  $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}] \in \mathbb{R}^{N \times D}$, where $d \ll D$.

\end{enumerate}

While Isomap has been successfully applied in the areas of financial analysis \citep{Ribeiro2008SupervisedIsomap}, facial and object recognition \citep{Zhang2018IsomapMultiManifold}, visualization and classification tasks \citep{Vlachos2002NonlinearDRClassification}, a few important weaknesses are prevalent.  First, Isomap may be topologically unstable.  That is, it may construct erroneous connections in the neighborhood graph.  This is known as short-circuiting, and it can severely impair the performance  of Isomap.  Several approaches have been proposed to nullify the short-circuiting problem, such as removing datapoints with large total flows or by removing nearest neighbors that violate local linearity of the neighborhood graph \citep{VanDerMaaten2009DRReview}.  Another weakness of Isomap is that it may not perform correctly if there are holes  in the manifold, as this causes the geodesic distances of some samples to appear further on the manifold than  they truly are.  A third weakness is that Isomap can fail if the manifold is non-convex.  Therefore, we see that Isomap can perform very well due to theoretical guarantees on qualities such as convergence, as long as the manifold  is isometric to a convex open set of $\mathbb{R}^{d}$, $\mathcal{D}_{\mathcal{X}}(\bm{u},\bm{u}') = \mathcal{D}_{\mathcal{Y}}(f(\bm{u}),f(\bm{u}')) $, meaning that the geodesic distances in the graph are almost equal to the Euclidean distances in the embedding space $\mathbb{R}^{d}$.  Continuing, an additional drawback of Isomap is the fact that it requires the decomposition of a large, dense Gram matrix which scales with the number of training data points.  If the dataset grows too large, a solution will no longer be tractable.  Furthermore, the constraint on $\mathcal{X}$ to be isometric to a convex open set of $\mathbb{R}^{d}$ is rarely met.  As mentioned in \citep{Thorstensen2009ManifoldThesis}, these problems may be circumvented by sparsifying  large datasets using landmarks, as with Landmark Isomap \citep{deSilva2002IsomapReview} and looking at conformal maps, as is done in Conformal Isomap \citep{deSilva2002ConformalIsomap}. Finally, as with most nonlinear manifold learning techniques, it is nontrivial to embed out-of-sample data points into the lower dimensional feature space.  

\paragraph{Supervised Isomap Approaches}
As with most traditional manifold learning methods in the literature, Isomap is not inherently well-suited for classification tasks.  However, supervised approaches which consider class label information have been adopted to increase class separability in the latent embedding space. The work by Vllachos et al. \citep{Vlachos2002NonlinearDRClassification} was the first to investigate a supervised adaptation of Isomap.  Two supervised Isomap procedures were proposed which combine Isomap with a nearest neighbor classifier. These methods, Iso+Ada and WeightedIso take label information into consideration to scale the computed Euclidean distances utilized by Isomap by a constant factor according to class label. The idea is to make points closer in the embedding space if they have the same class label and farther if they have opposing class labels. Ribeiro et al. \citep{Ribeiro2008SupervisedIsomap} proposed an enhanced supervised Isomap (ES-Isomap) in which the dissimilarity matrix is weighted according to rules which consider class label information.  The dissimilarity matrix (considered as the adjacency matrix), $\bm{W}$, which was the same used in the Supervised Isomap method \citep{Geng2005SupNonlinearDimRed}, is defined as:
\begin{align}
	\bm{W}(\bm{x}_{m},\bm{x}_{n}) = \begin{cases} \sqrt{1 - \exp{\frac{-\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}}, \quad l_m = l_n \\  \sqrt{ \exp{\frac{\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}} - \alpha, \quad l_m \neq l_n\end{cases}
	\label{eq:supervised_isomap_dissimilarity}
\end{align}
\noindent
where $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ denotes the distance measure between samples $\bm{x}_{m}$ and $\bm{x}_{n}$, $\beta$ is used to prevent $\bm{W}(\bm{x}_{m},\bm{x}_{n})$ from increasing too quickly when $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ is large and is typically set according to the density of the data, $\alpha$ is a constant in $[0,1]$ which controls the dissimilarity between points in different classes and keeps the graph from becoming disconnected, and $l_{m}$ and $l_{n}$ are the corresponding class labels of samples  $\bm{x}_{m}$ and $\bm{x}_{n}$, respectively.  In Equation \ref{eq:supervised_isomap_dissimilarity}, the dissimilarity between two points is greater than or equal to one if their class labels are different and less than 1 if the points have the same class label.  Therefore, the between-class dissimilarity will always be larger than the within-class, which is an important property for classification tasks.

Li and Guo proposed Supervised Isomap with Explicit mapping (SE-Isomap) in \citep{Li2006SupervisedIsomap}.  SE-Isomap enforces discriminability on the matrix of geodesic distances, as compared to the Euclidean distance matrix used in the aforementioned approaches, to learn an explicit mapping to the low-dimensional embedding space.  Finally, Zhang et al. \citep{Zhang2018IsomapMultiManifold} developed a semi-supervised Isomap to utilize both labeled and unlabeled data points in training.  This method aims at minimizing pairwise distances of within-class samples in the same manifold while maximizing the distances over different manifolds.

\subsubsection{Locally Linear Embedding (LLE)}

\subsubsection{Laplacian Eigenmaps (LE)}
\paragraph{Classical LE}
Similar to LLE, Laplacian Eigenmaps, or \textit{Spectral Embedding}, is a nonlinear dimensionality reduction technique which aims to preserve local structure of data \citep{Raducanu2012SupervisedNonlinearDimReduction,VanDerMaaten2009DRReview}.  Using \textit{spectral graph theory}, LE computes low-dimensional representations of data in which the dissimilarities between datapoints and their neighbors (according to an affinity measure) are minimized.  The name \textit{Laplacian Eigenmaps} is derived by the use of Laplacian regularization in the optimization procedure \citep{Thorstensen2009ManifoldThesis}. Given a set of $N$ samples $\bm{X} = \{\bm{x}_n\}^{N}_{n=1} \subset \mathbb{R}^{D}$, the first step of LE is to define a \textit{neighborhood graph on the samples}.  This graph, also called an \textit{affinity} or \textit{adjacency} matrix can be constructed in a variety of ways, such as $K$-nearest neighbor, $\epsilon$-ball, full mesh, or by weighting each edge $\bm{x}_m \sim \bm{x}_n$ by a symmetric affinity function $W_{mn} = K(\bm{x}_m;\bm{x}_n)$, typically a radial basis or heat kernel:
\begin{align}
	\bm{W}_{mn}= w_{mn} = \exp \left ( - \frac{|| \bm{x}_m - \bm{x}_n ||^{2}}{\beta}  \right )
\end{align}

\noindent
where the kernel bandwidth $\beta$ is typically set as the variance of the dataset \citep{Raducanu2012SupervisedNonlinearDimReduction,Thorstensen2009ManifoldThesis}.

The goal is to uncover the latent data representations $\{ \bm{z}_n \}^{N}_{n=1} \subset \mathbb{R}^{d}$ where $d \ll D$ which minimizes the objective 
\begin{align}
	J(\bm{W},\bm{Z}) = \frac{1}{2} \sum_{m,n}^{}||\bm{z}_{m} - \bm{z}_{n} ||^{2}w_{mn} = tr(\bm{Z}^{T}\bm{L}\bm{Z})
\end{align}

\noindent
with $\bm{W}$ denoting the symmetric affinity matrix, $\bm{D}$ the diagonal weight matrix whose entries are the sum of the rows (or columns since $\bm{W}$ is symmetric) of $\bm{W}$ (i.e. $d_{mm} = \sum_{n}w_{mn}$, and is $0$ otherwise).  The graph Laplacian matrix is provided as $\bm{L} = \bm{D} - \bm{W}$.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is the $N \times d$ embedding matrix and $tr(.)$ denotes the trace of a matrix. The $n^{th}$ row of matrix $\bm{Z}$ provides the vector $\bm{z}_n$, which is the latent representation of sample  $\bm{x}_n$. This objective discourages projecting similar points in the input feature space to disparate regions of the embedding space by enforcing heavy penalization. 

The latent sample coordinates $\bm{Z}$ are found as the solution to the optimization problem:
\begin{align}
	\min_{\bm{Z}} tr(\bm{Z}^{T}\bm{L}\bm{Z}) \quad s.t. \quad \bm{Z}^{T}\bm{D}\bm{Z} = \bm{I}, \bm{Z}^{T}\bm{L}\bm{e} = \bm{0}
\end{align}

\noindent
where $\bm{I}$ is the identity matrix and $\bm{e} = (1, \dots, 1)^{T}$.  The first constraint eliminates the trivial solution $\bm{Z} = \bm{0}$ (scaling) and the second constraint avoids the trivial solution $\bm{Z} = \bm{e}$ (uniqueness). By applying the Langrange multiplier method and using the fact that $\bm{L}\bm{e} = \bm{0}$, the low-dimensional data representations can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{L}\bm{v} = \lambda \bm{D} \bm{v} \label{eq:trad_le_eig}
\end{align}
\noindent
The column vectors $\bm{v}_{1}, \dots, \bm{v}_{N}$ are the solutions of Equation \ref{eq:trad_le_eig}, ordered to the corresponding eigenvalues, in ascending order, $\lambda_{1} = 0 \leq \lambda_{2} \leq \dots \leq \lambda_{N} $. The embedding of the input samples given by the matrix $\bm{Z}$, is obtained by concatenating the eigenvectors of the $d$ smallest non-zero eigenvalues.  $\bm{Z}$ is a $N \times d$ matrix, where $d < N$ is the dimensionality of the embedded space.  From observation, it is clear that the embedding dimensionality is limited by the number of samples $N$.

\paragraph{Supervised LE (S-LE)}
In order to adopt LE for classification,Raducanu and Dornaika \citep{Raducanu2012SupervisedNonlinearDimReduction} proposed a supervised LE which minimizes the margin between samples with similar class labels and maximizes the margin between samples with opposing class labels.  Supervised LE utilizes discriminative information contained in the class labels when finding the nonlinear embedding (spectral projection). 

In order to discover both geometrical and discriminative manifold structure, supervised LE splits the global graph into two components: the within-class graph $G_{w}$ and the between-class graph $G_{b}$.  To define the margin, they define two subsets, $N_{w}(\bm{x}_{n})$ and $N_{b}(\bm{x}_{n})$ for each sample $\bm{x}_{n}$.  These two subsets contain the neighbors of $\bm{x}_{n}$ sharing the same label and having different labels, respectively, which have a similarity higher than the average.
\begin{align}
	N_{w}(\bm{x}_n) = \{\bm{x}_m |l_{m} = l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_within_neighbor}
\end{align}

\begin{align}
N_{b}(\bm{x}_n) = \{\bm{x}_m |l_{m} \neq l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_between_neighbor}
\end{align}

\noindent
where $AS(\bm{x}_{n}) = \frac{1}{N} \sum_{n=1}^{N} \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$ denotes the average similarity of the sample  $\bm{x}_{n}$ to the rest of the data.  From Equations \ref{eq:s_le_within_neighbor} and \ref{eq:s_le_between_neighbor} it is clear that the neighborhoods for each data sample are not necessarily the same size.  As a result, this function constructs the affinity graph according to both the local density and similarity between data samples in the input feature space.

With the two sets defined, the within-class and between-class weight matrices $\bm{W}_{w}$ and $\bm{W}_{b}$ are formed from the adjacency graphs $G_{w}$ and $G_{b}$, respectively.  These weight matrices are defined as:
\begin{align}
	W_{w,mn} =
	\begin{cases}
		\exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right), \text{ if } \bm{x}_{n} \in N_{w}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{w}(\bm{x}_{n})  \\
		0, \text{ otherwise}
	\end{cases}
\end{align}

\begin{align}
	W_{b,mn} =
	\begin{cases}
		1, \text{ if } \bm{x}_{n} \in N_{b}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{b}(\bm{x}_{n}) \\
	0, \text{ otherwise}
	\end{cases}
\end{align}
\noindent
and the global affinity matrix, $\bm{W}$, can be written as:
\begin{align}
	\bm{W} = \bm{W}_{w} + \bm{W}_{b}
\end{align}

In order to obtain the low-dimensional representations $\bm{z}_n$ of the input data $\bm{x}_{n}$, the following objective functions can be optimized for $\bm{Z}$:
\begin{align}
	\min \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{w,mn} = tr(\bm{Z}^{T}\bm{L}_{w}\bm{Z})
\end{align}
\begin{align}
\max \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{b,mn} = tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z})
\end{align}
\noindent
where $\bm{L}_{w} = \bm{D}_{w} - \bm{W}_{w}$ and $\bm{L}_{b} = \bm{D}_{b} - \bm{W}_{b}$ indicate the corresponding graph Laplacians of the within-class and between-class affinity graphs, respectively.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ contains the low-dimensional representations of the input samples in its rows.

By merging the two objective functions, the final optimization problem is formulated as:
\begin{align}
	\arg\max_{\bm{Z}} \left \{  \gamma tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z}) + (1- \gamma)tr(\bm{Z}^{T}\bm{W}_{w}\bm{Z}) \right \} \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}
The term $\gamma$ is a scalar value in $[0,1]$ which determines the trade-off between pulling similar samples toward each other in the latent space and pushing heterogeneous points away.  A value of $\gamma = 1$ forces the objective to solely focus on maximizing the margin between dissimilar points.  Alternatively, a value of $\gamma = 0$ priorities the objective on embedding homogeneous samples in close spatial proximity. By defining matrix $\bm{B} = \gamma \bm{L}_{b} + (1 - \gamma)\bm{W}_{w}$, the problem becomes:
\begin{align}
	\arg\max_{\bm{Z}} \left ( \bm{Z}^{T}\bm{B}\bm{Z}  \right ) \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}

The low-dimensional embedding matrix $\bm{Z}$ can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v} \label{eq:sup_le_eig}
\end{align}

The column vectors $\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{N}$ are the generalized eigenvectors of 
Equation \ref{eq:sup_le_eig} arranged by descending eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \dots \lambda_{d}$.  Then the $N \times d$ embedding matrix $\bm{Z} =  [ \bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is provided by concatenating the obtained eigenvectors $\bm{Z} = [\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{d}]$.

The primary difference between the classic LE and S-LE is that traditional LE solely attempts to preserve the spatial relationships between samples, and thus, does not consider label information when learning the embeddings.  Alternatively, S-LE aims at aiding discriminant analysis by collapsing the distance between samples with the same label that are in close spatial proximity and pushing away spatial neighbors with differing class labels.  This is done through the utilization of two affinity graphs: the within-class and between-class graphs.  As with most graph-based methods, LE results vary highly according to the choice of neighborhood size.  However, choosing the size of $K$ or $\epsilon$ in advance can be very difficult. S-LE does not require user-defined graph parameters, other than those associated with the chosen affinity measure.  Instead, graph edges are chosen according to an adaptive neighborhood for each sample.   Both methods, however, suffer from inherent difficulties associated with nonlinear manifold learning, namely, selecting the intrinsic embedding dimensionality and handling out-of-sample extensions.

Despite these nuances, LE (and its variants) have been successfully applied in nonlinear dimensionality reduction tasks for facial recognition, spectral clustering and object classification \cite{VanDerMaaten2009DRReview}.

Apart from S-LE, other methods have been explored to integrate label information into Laplacian Eigenmaps.  A review of supervised dimensionality reduction methods by Chao et al. \citep{Chao2019RecentAdvancesSupervisedDimRed} explains that author's have optimized the affinity matrix using label information after constructing from spatial proximity, proposed deep learning-based approaches to achieve supervised LE and integrated label information into the affinity matrix construction process.  

The special feature exhibited by all Laplacian Eigenmap methods is the use of laplacian regularization, which enforces properties such as smoothness and provides a level of resistance toward the influences of outliers.  This useful feature has been applied in a variety of supervised and semi-supervised tasks, such as hyperspectral and synthetic aperture radar remote sensing classification \citep{Ratle2010ManRegHSI, Ren2017ManRegSAR}, classification of synthetic data \citep{Tsang2007ManifoldRegularization}, zero-shot learning \citep{Meng2018ManRegZeroShot} and reinforcement learning \citep{Li2015ManRegReinforcementLearning}.

\subsubsection{Hessian Eigenmaps}

\subsubsection{Diffusion Maps}

\subsubsection{Sammon Mapping} \label{sec:sammon_mapping}

\subsubsection{Maximum Variance Unfolding (MVU)}

\subsection{Latent Variable Models}

\subsubsection{General Latent Variable Model (GLVM)}

\paragraph{Factor Analysis (FA)}

\paragraph{Probabilistic PCA (PPCA)}
PCA can also be analyzed from the viewpoint of factor analysis (FA), which is discussed later in this literature review.  The basic idea, however, is that data observations $\bm{x}_{n} \in \mathbb{R}^{D}$ are realizations of a probability distribution with a prior on the lower-dimensional latent variable $\bm{z}_n \in \mathbb{R}^{d}$.  A typical choice on this model is a Gaussian-Gaussian conjugate prior pair where the mean of the data likelihood is a linear function of the latent inputs.  As an example, the prior over the hidden data representations can be expressed as Gaussian distribution

\begin{align}
p(\bm{z}_n) = \mathcal{N}(\bm{z}_n|\bm{\mu}_0, \bm{S}_0)
\end{align}

\noindent
and the data likelihood is denoted as a multivariate Gaussian

\begin{align}
p(\bm{x}_n|\bm{z}_n,\bm{\theta}) = \mathcal{N}(\bm{W}\bm{z}_n + \bm{\mu}, \bm{\Psi})
\end{align}

\noindent
where $\bm{W}$ is a $D \times d$ \textit{factor loading matrix}, and $\bm{\Psi}$ is a $D \times D$ covariance matrix.  The objective of FA is to compute the posterior over the latent factors in hopes that they will reveal something interesting about the data \citep{Murphy2012}.  Classical PCA assumes the data covariance to be $\bm{\Psi} = \sigma^2\bm{I}$ with $\sigma^2 \rightarrow 0$, thus the model is deterministic.  Alternatively, when $\sigma^2 > 0$,the projection is no longer orthogonal since it is shrunk toward the prior mean.  The trade-off is that the reconstructions of $\bm{x}_n$ with be closer to the data mean.


While the typical approach for fitting PCA is to use the method of eigenvectors of Singular Value Decomposition (SVD), it can also be fit with Expectation-Maximization (EM).  This formulation may be more computationally efficient for high-dimensional data.  By using Gaussian Processes, PPCA may also be extended to learn nonlinear mappings between the input and latent feature spaces \citep{VanDerMaaten2009DRReview}.

\paragraph{Supervised PCA}

Supervision has also been applied to PCA.  Two such examples, Supervised PCA and Discriminative Supervised PCA, were provided by Murphy in \citep{Murphy2012}.  The two are briefly described. 

\subparagraph{Supervised PCA (Latent Factor Regression)}
\textit{Supervised PCA} or \textit{Bayesian factor regression} is a model like PCA, except that the target variable (or label), $l_n$ is taken into account when learning the low-dimensional embedding.  For the case of binary classification, the Bayesian model can be decomposed into the following elements:
\begin{align}
	p(\bm{z}_n) = \mathcal{N}(0,\bm{I}_d)
\end{align}
\begin{align}
	p(l_n|\bm{z}_n) = \text{Ber}(\text{sigm}(\bm{w}^{T}_{l}\bm{z}_n))
\end{align}
\begin{align}
	p(\bm{x}_n|\bm{z}_n) = \mathcal{N}(\bm{W}_{x}\bm{z}_{n} + \bm{\mu}_{x}, \sigma^{2}\bm{I}_D)
\end{align}

\subparagraph{Discriminative Supervised PCA} 


\subsubsection{Generative Topographic Mapping (GTM)}

\subsection{Competitive Hebbian Learning}

\subsection{Deep Learning}

\subsection{Current State of the Art}
\subsection{UMAP}

\subsection{Stochastic Neighbor Embedding (SNE and t-SNE)}

\subsection{NCA}

\subsection{Multiple Instance Learning on Manifolds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Metric Embedding %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metric Embedding}

The concepts of ``near" and ``far" are very powerful and useful utilities in everyday life.  They classify the relationship between two ``primitives" as being similar or dissimilar, as well as the degree of compatibility \citep{Thorstensen2009ManifoldThesis}. As an example, a medical doctor might consider a machine learning researcher and a software engineer as being similar (near), because they both perform research for computer applications.  However, the same researcher and engineer would likely consider their jobs as being very disparate (far) based on the details of their work.  In order to capture this abstraction of distance, a \textit{metric space} is defined as a mathematical construction  of this vague generality.

 \theoremstyle{definition}
 \begin{definition}{Metric Space}
 A \textit{metric space} is an ordered pair $(\mathcal{X},\mathcal{D})$ where $\mathcal{X}$ is a set and $\mathcal{D}$ is a metric on $\mathcal{X}$, or $\mathcal{D}:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ such that $\forall \bm{x},\bm{y},\bm{z}\in\mathcal{X}$, the following holds:

 \end{definition}
 	\begin{enumerate}
	\item Non-negativity: $\mathcal{D}(\bm{x},\bm{y}) \geq 0$
	\item Identity: $\mathcal{D}(\bm{x},\bm{y}) = 0 \iff \bm{x} = \bm{y} $
	\item Symmetry: $\mathcal{D}(\bm{x},\bm{y}) = \mathcal{D}(\bm{y},\bm{x})$
	\item Triangle Inequality:  $\mathcal{D}(\bm{x},\bm{z}) \leq \mathcal{D}(\bm{x},\bm{y}) + \mathcal{D}(\bm{y},\bm{z})$
	\end{enumerate}


\textbf{Explain more about these properties...} \newline
The goal of \textit{metric embedding learning} it to learn a function $f_{\theta}(\bm{x}):\mathbb{R}^{D} \rightarrow \mathbb{R}^{d}$ which maps semantically similar points from the data input feature space of $\mathbb{R}^{D}$ onto \textit{metrically close} points in $\mathbb{R}^{d}$.  Similarly, $f_{\theta}$ should map semantically different points in $\mathbb{R}^{D}$ onto metrically distant points in $\mathbb{R}^{d}$.  The function $f_{\theta}$ is parameterized by $\theta$ and can be anything ranging from a linear transformation to a complex non-linear mapping as in the case of deep artificial neural networks \citep{Hermans2017DefenseTripletLoss}.  Let $\mathcal{D}(\bm{x},\bm{ y}): \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a metric function measuring similarity or dissimilarity in the embedded space.  For succinctness, $\mathcal{D}_{i,j} = \mathcal{D}(f_{\theta}(\bm{x_{i}}),f_{\theta}(\bm{x_{j}}))$
defines the dissimilarity between samples $\bm{x}_{i}$ and $\bm{x}_{j}$, after being embedded.

\subsubsection{Metric Learning}
Thorstensen thesis, Raducanu S-LE

	\subsection{Ranking Loss}
	
	\subsubsection{Pairwise Loss}
	
	\subsubsection{Contrastive Loss}
	Definition of Contrastive Loss:
	
	
	\subsubsection{Triplet Loss}
	
	Definition of Triplet Loss:
	
	Triplet loss was extended in \citep{Sohn2016NPairLoss} to simultaneously optimize against N negative classes.

		\subsubsection{Large-Margin K-Nearest Neighbors (LMNN)}
		
		\subsubsection{FaceNet}
		
		FaceNet is a convolutional neural network which learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity \citep{Schroff2015FaceNet}. 
		
		\begin{align}
			||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} + \alpha < ||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2}, \quad \forall (f(x^{a}_{i}),f(x^{p}_{i}),f(x^{n}_{i})) \in \mathcal{T}
		\end{align} 
	
		\begin{align}
			\mathcal{L} = ||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} -||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2} + \alpha
		\end{align}
		
		
		\subsubsection{Siamese Neural Networks}
	
	\subsection{Manifold Regularization}
	
	\subsection{Multiple Instance Metric Learning}



