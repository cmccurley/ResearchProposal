\chapter{Background}

This chapter provides a comprehensive review of literature pertinent to the work proposed in this document.  First, a review is given in Section \ref{sec:Multiple_Instance_Learning} on the multiple instance learning framework for learning from weak and ambiguous annotations.  Notation is given and summaries of methods used in bag and instance-level classification and ranking are discussed.  Review is then provided in Section \ref{sec:Manifold_Learning} on manifold learning, including classic approaches as well as supervised and semi-supervised adaptations.  This is followed by an entire section dedicated to reviewing manifold learning and dimensionality reduction techniques specifically tailored for use with weak labels (Section \ref{sec:weakly_sup_dim_reduction}).  The last part of this chapter (Section \ref{sec:MetricEmedding}) reviews the existing literature on metric embedding, focusing heavily on the utilization of contrastive and triplet-based loss evaluation.  Reviews describe basic terminology and definitions.  Foundational approaches are elaborated and advances are addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning} \label{sec:Multiple_Instance_Learning}

Multiple Instance Learning (MIL) was originally proposed by \cite{Dietterich1996AxisParallelRectangles} as a method to handle inherent observation difficulties associated with drug activity prediction.  This problem, among many others, fits well into the framework of MIL where training labels are associated with sets of data points, called \textit{bags} instead of each individual data points, or \textit{instances}.  Under the \textit{standard MIL assumption}, a bag is given a ``positive" label if it is known that  \textit{at least one} sample in the set represents pure or partial target.  Alternatively, a bag is labeled as ``negative" if does not contain any positive instances \citep{Carbonneau2016MILSurvey}.  Let $\bm{X}=[\bm{x}_1,\dots, \bm{x}_N] \in \mathbb{R}^{D \times N}$ be training data where $D$ is the dimensionality of an instance, $\bm{x}_n$, and $N$ is the total number of training instances.  The data is grouped into K \textit{bags}, $\bm{B} = \{\bm{B}_1, \dots, \bm{B}_K\}$, with associated binary bag-level labels, $\mathcal{L} = \{L_1, \dots, L_K \}$ where 
\begin{align}
	L_k = \begin{cases} 
	+1, & \exists \bm{x}_{kn} \in \bm{B}^{+}_{k} \ni  l_{kn} = +1\\
	-1, & l_{kn} = -1 \quad \forall \bm{x}_{kn} \in \bm{B}^{-}_{k} 
	\end{cases}
\end{align} and $\bm{x}_{kn}$ denotes the $n^{th}$ instance in positive bag $\bm{B}^{+}_{k}$ or negative bag $\bm{B}^{-}_{k}$  and $l_{kn} \in \{ -1, +1\}$ denotes the instance-level label on instance $\bm{x}_{kn}$.  Figure \ref{fig:bag_eg} demonstrates the concept of MIL bags.  The objective of learning under MIL is, given only bag-level label information, to fit a model which can perform one of the following tasks: classification, regression, ranking or clustering \citep{Carbonneau2016MILSurvey}.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{"lit_review/MIL/bag_examples"}
		\caption[Multiple instance learning bags.]{Illustration of example bags under the multiple instance learning framework.  Red ``plus signs" denote positive instances and blue ``negative signs" represent negative instances.  The two bags on the top row are labeled ``negative" because they only contain negative instances.  The three bags on the bottom row are ``positive" because they each contain at least one positive instance.}
		\label{fig:bag_eg}
	\end{figure*}
\end{center}

\noindent

\subsection{Multiple Instance Learning with Manifold Bags}
It should be noted that while the aforementioned MIL formulation is standard in the literature, \cite{Babenko2011MILManifoldBags} introduced the framework of MIL using manifold bags  .  Their work claimed that instead of finite sets of instances, bags are inherently better represented as low-dimensional manifolds in a high-dimensional feature space.  They considered the scenario of classifying whether or not an image contained a face.  In this scenario, the entire image was considered to be a bag and patches of the image represented individual instances.  It was assumed that the collection of instances collectively formed a low-dimensional manifold.  The representation of bags as low-dimensional manifolds over the domain of instances is related to the \textit{Multiple-Instance Learning via Embedded Instance Selection} (MILES) algorithm \citep{Chen2006MILES}, which embeds bags into a feature space defined by similarities between instances.  By giving more importance to the features representing samples in local neighborhoods around the most-likely-cause concept, MILES can be interpreted as representing bags according to local, manifold approximations centered on the instances which most-likely provide the bag-level labels.  While related to bag-embedding methods such as MILES, the work by \cite{Babenko2011MILManifoldBags} laid the groundwork for learning with manifold bags and proved probably approximately correct (PAC) learnability under this (more general) framework.  While it was worth mentioning this work as it aligns with the objective of manifold learning discussed in the proposed work, the remainder of this literature review focuses solely on the standard MIL formulation.

\subsection{Tasks}
Multiple instance learning in the literature can be broadly categorized into four tasks: classification, regression, ranking and clustering \citep{Carbonneau2016MILSurvey}.  MIL classification can be performed at either the bag or instance level.  The goal is to assign a class label to either the set of instances or the individual instances themselves.  MIL regression consists of assigning a real-valued label to a bag (or instance) instead of a class label.  A few methods have been proposed to rank bags or instances instead of assigning a class label or score.  This problem differs from regression because the goal is not to obtain an exact real-valued label, but to compare the magnitude of scores to perform sorting.  The clustering task can also be performed at the bag or instance-level.  Bag-level clustering involves separating a set of unlabeled bags into distinct groupings based on a measure of similarity.  Alternatively, clustering is often performed on the instances within individual bags in attempt to quantize the data into positive and negative concepts.  Classification and ranking are the most pertinent tasks to the proposed work, and will thus be discussed in detail.

\subsection{Multiple Instance Classification}
The standard supervised learning task is to learn a classifier based on a training set of feature vectors, where each feature vector is paired with and associated class label.  In the \textit{multiple instance classification} task, the goal is to learn a classifier based on a training set of bags, where each bag is a set of feature vectors known as instances.  In this setting, each bag is paired with an associated binary class label; however, the labels of each instance in the sets are unknown. 

\subsubsection{Space Paradigms} 
The multiple instance classification problem has been formulated under three paradigms: instance-space, bag-space and embedded-space \citep{Amores2013MIClassification}.  Each paradigm is categorized according to how information presented in the MI data is exploited.  In the \textit{instance-space} paradigm, the discriminative information is considered to lie at the instance-level.  An instance-level classifier is trained to separate the true positive instances from the true negative instances.  Given an instance-level classifier, a bag-level classifier can be developed by simply aggregating the instance-level scores in a test bag.  This paradigm is based on local, instance-level information.  In the \textit{bag-space} paradigm, the discriminative information is considered to lie at the bag-level.  Under this paradigm, each bag is treated as a whole entity, and the learning process discriminates between entire bags.  This paradigm is based on global, bag-level information.  Considering that the bag space is a non-vector space, current BS methods make use of non-vectorial learning techniques which define distance metrics as a way to compare bags.  In the \textit{embedded-space} paradigm, each bag is mapped to a single feature vector which captures the relevant information about the entire bag.  Consequently, the learning problem transforms into a standard supervised problem, where each feature vector is paired with an associated (bag-level) label.  Similar to the bag-space paradigm, the embedded-space paradigm is also based on global, bag-level information.  However, the difference between the two paradigms lies in the way bag-level information is extracted.  In the bag-space paradigm, information is extracted implicitly through the definition of the distance or kernel function which maps two or more bags to a real number measuring the similarity between them.  Alternatively, information is extracted explicitly in the embedded-space paradigm through the definition of the mapping function, which aggregates the instances contained in an individual bag to form a single real-valued vector representing the bag in the embedded vector space.  Figure \ref{fig:mil_classification_space_paradigm} demonstrates the differences between standard supervised classification and the three MIL classification space paradigms. Apart from space paradigm, MIL classification methods in the literature can be organized according to the their primary approaches toward learning. A review of prominent MIL classification methods in the literature is provided, categorized according to learning approach.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=\textwidth]{"lit_review/MIL/space_examples"}
		\caption[MIL classification space paradigms.]{Illustration of MIL classification under the instance, bag and embedded-space paradigms.  The left column shows data in the input feature space and the right column shows data in the  classifier space along with learned hyperplanes.  Red denotes positive bags/instances while blue represents negative bags/instances.  Single instances in circles denote single-instance representations of bags after being embedded.}
		\label{fig:mil_classification_space_paradigm}
	\end{figure*}
\end{center}

\subsubsection{MIL Classification Approaches}
MIL classification approaches in the literature can be categorized by the underlying principle used for learning.  The categories discussed in this review are: Axis-Parallel Concepts, Maximum Likelihood, Distance-Based, Maximum Margin,  Deep Learning, Probabilistic Graphical Methods, Dictionary Learning and Ensembles of Classifiers.  Each approach is reviewed in the following.

\paragraph{Learning Axis-Parallel Concepts:}
Learning Axis-Parallel Concepts are among the first group of methods used to solve MIL problems \citep{Dietterich1996AxisParallelRectangles}.  The foundation of this category is based on the method of \textit{Axis-Parallel Rectangles} (APR), proposed by Dietterich et al. in the 1990's.  An axis-parallel hyper-rectangle is a set of thresholds (one for each feature dimension) that is used to discriminate between two classes. It can also be viewed as an overlap or aggregation region of true positive instances in the feature space. The goal of APR is to find an axis-parallel hyper-rectangle in the feature space to represent the target concept \citep{Ghaffarzadegan2018MILVAE, Bocinsky2019SPIEMIACEInitialization, Jiao2017Thesis}.  A disadvantage of these approaches is that most of the data is ignored when working with large bags.    

\paragraph{Maximum Likelihood:}
Similar to traditional \textit{Maximum Likelihood Estimation} (MLE), the objective of maximum likelihood in the MIL setting is to train a classifier which maximizes the likelihood of the data.  The most prominent of these methods is \textit{Diverse Density} (DD) \citep{Maron1998DiverseDensity}, which considers each bag as a manifold describing the instances.  The goal of learning is to discover a prototype  from the training data which maximizes the DD measure.  Diverse Density is essentially a measure of the intersection of positive bags minus the union of the negative bags.  By maximizing DD, a point of intersection can be found which is close to at least one instance  from the positive bags while being as far as possible from all points in the negative bags \citep{Ghaffarzadegan2018MILVAE}.   Zhang et al. later proposed \textit{Expectation-Maximization Diverse Density} (EM-DD) \citep{Zhang2002EMDD}. EM-DD extends DD by viewing the relationships between instances and their corresponding bag's labels as latent variables.  In other words, it asks which instance in the set corresponds to the bag's label \citep{Du2017Thesis}. In the \textit{E-step}, an the instance from each bag which is the most probable for providing the bag its label is selected.  Then in the \textit{M-step}, a new concept point is found by maximizing DD with gradient ascent.  This process is iterated until a stopping criteria is met, however, it will stop naturally as there only a finite number of instance combination that the algorithm can pick.  Multiple instance maximum likelihood approaches have been used in a variety of problems, such as: stock selection, person identification, scene classification, hyperspectral target classification and explosive hazard detection \citep{Maron1998DiverseDensity, Maron1998MILSceneClassification, Zare2016MIACE, Zare2015MILLandmineEMI, Bocinsky2019SPIEMIACEInitialization, Mccurley2019SPIEWEMIComparison}.    

\paragraph{Distance-Based:}
A simple approach for classification in the supervised learning paradigm is to compare distances of test points to samples in the training set.  \cite{Gartner2002MIKernels} defined the MI-Kernel by regarding each bag as a set of features and applying a set kernel directly to compare similarity. The \textit{Citation-$k$NN} algorithm \citep{Du2017Thesis,Carbonneau2016MILSurvey,Zhou2003MIEnsemble,Kim2010LocalDRMIL} is a nearest-neighbor style classifier which borrows the idea of scientific citers and references when considering a bag's label.  \textit{References} are simply the nearest neighbors of a bag, while \textit{citers} are the bags which have the query bag in it's $C$-nearest neighbors.  The vanilla citation-$k$NN uses the \textit{minimal Hausdorff distance} to measure bag similarity.  The Hausdorff distance between two bags $\bm{B}$ and $\bm{B}'$ is defined as:
\begin{align}
	H(\bm{B},\bm{B}') = \max \{ h(\bm{B},\bm{B}'),h(\bm{B}',\bm{B}) \}
\end{align}
\noindent
and
\begin{align}
	h(\bm{B},\bm{B}') = \max_{\bm{b}\in \bm{B}} \min_{\bm{b}' \in \bm{B}'}||\bm{b}-\bm{b}' ||
\end{align}
\noindent
where $\bm{b}$ and $\bm{b}'$ are instances in bags $\bm{B}$ and $\bm{B}'$, respectively.  Intuitively, the minimal Hausdorff distance is the smallest value $H$ such that every instance in $\bm{B}$ has a point of $\bm{B}'$ within distance $H$, and every instance in $\bm{B}'$ has an instance in $\bm{B}$ within a distance of $H$. Variants of citation-$k$NN include Bayesian citation-$k$NN and fuzzy citation-$k$NN \citep{Du2017Thesis}.  Additionally, the minimal Hausdorff distance has been substituted for the Earth Mover's Distance (EMD), Chamfer distance and specialized bag-distance kernels \citep{Amores2013MIClassification}.  

Two alternative distance-based MIL classifiers are MIGraph and miGraph \citep{Zhou2009miGraph}.  Both methods consider bags as graphs to capture interdependence between instances. The distance measures used to compare bags is what separates the two methods. MIGraph explicitly maps every bag to an undirected graph and uses a graph kernel or metric such as graph edit distance to discriminate between positive and negative bags.  Alternatively, miGraph implicitly constructs graphs by defining affinity matrices between instances and uses clique information to help distinguish positive from negative bags. 


\paragraph{Maximum Margin:}
The concept of weakly-supervised maximum margin learning has been explored under a variety of techniques, the primary being \textit{Support Vector Machines} (SVM) and \textit{metric embedding}, which will be discussed in Section \ref{sec:MetricEmedding}.  \cite{Andrews2011MISVM} proposed two SVM methods under the MIL framework, namely, mi-SVM and MI-SVM, for instance-level and bag-level classification,  respectively.  The goal of a SVM is to train a classifier to maximize the margin between a small subset of training examples (called \textit{support vectors}) and the decision hyper-plane \citep{Murphy2012}.  Both MIL SVM methods follow a similar procedure to EM-DD.  In MI-SVM, each positive bag is represented by a single instance which is considered to be the ``most positive instance" in the bag. Optimization alternates between learning a decision boundary with SVM and selecting the positive bag representatives given the new classifier. In contrast, mi-SVM considers all points in the positive bags while learning the decision boundary.  Every point in the positive bags is provided a positive label.  The instance labels are refined iteratively under the constraint of the standard MIL assumption, that at least one instance from each positive bag must lie on the positive side of the decision hyper-plane \citep{Cao2016VehicleDetectionMIL}.  MissSVM is a semi-supervised max-margin approach which considers the instances in positive bags as unlabeled, and enforces a constraint that at least one of them is positive \citep{Zhou2007MissSVM}.  DD-SVM  and \textit{Multiple-Instance Learning via Embedded Instance Selection} (MILES) are both embedding-space methods which convert MIL into a standard supervised problem \citep{Chen2006MILES}.  DD-SVM trains an SVM in a feature space constructed from a mapping defined by the local maximizers and minimizers of the DD function.  MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. A 1-norm SVM is applied to simultaneously select the important features and construct classifiers \citep{RuizMunoz2015MILBirdsongClassification}. Additionally, \cite{Xiao2017SphereMIL} developed an ensemble method in which the base classifier enforces a margin between optimal hyper-spheres while enclosing at least one instance from each positive bag inside the ball. 


\paragraph{Neural Networks and Deep Learning:} \label{sec:mil_deep_learning}
Recent developments in deep learning have also made their way into the MIL literature under the assumption that useful features can be learned by the networks using only bag-level labels.  \cite{Gao2017CountGuidedWeaklySupervisedLocalization} used \textit{convolutional neural networks} (CNN) with count-based region selection to perform weakly-supervised object localization.  \cite{Ilse2018AttentionBasedDeepMIL} modelled the MIL problem as learning the Bernoulli distribution over the bag labels, where the label probability was parameterized by a neural network.  Ilse's approach employs attention-based MIL pooling as a way to visualize which instances the network selects as being positive.  A multi-instance multi-scale CNN to detect regions of interest in medical images was proposed by \cite{Li2019CNNMedicalImageClassification}.  Li's work introduced ``top-k pooling" to aggregate feature maps of varying scales and spatial dimensions, allowing the model to be trained using weak, MIL annotations.  \cite{Wang2020WSDeepLearningRemoteSensing} explored the use of a U-Net segmentation network architecture to obtain pixel-level ground-cover classification from image-level labels.  A discriminative \textit{variational autoencoder} (VAE) was used by \cite{Ghaffarzadegan2018MILVAE} to maximize the difference between latent representations of positive and negative instances.  \cite{Tu2019MILGraphNN} developed an end-to-end \textit{graph neural network} which treats each bag as a graph.  Each bag is passed through the network to obtain a feature representation encapsulating the structural information present in the bag.  Deep learning MIL approaches have been explored in a wide variety of applications, including: retinal image classification \citep{Tu2019MILGraphNN}, histopathology classification \citep{Ilse2018AttentionBasedDeepMIL}, object localization \citep{Gao2017CountGuidedWeaklySupervisedLocalization} and region-of-interest proposal in medical images \citep{Li2019CNNMedicalImageClassification}.


\paragraph{Probabilistic Graphical Methods:}
\textit{Probabilistic graphical models} (PGMs) are powerful tools used to capture inter-relations between random variables and learn structured models. In some problems, data exhibits an underlying structure between instances or bags that is more complex than simple co-occurrence.  Capturing this structure may lead to better classification performance \citep{Carbonneau2016MILSurvey}.  \cite{Deselaers2010MICRF} proposed a multi-instance conditional random field, MI-CRF.  In this method, bags are modeled as nodes in a conditional random field (CRF), where each node can take one of the instances in the bag as its state.  Classification corresponds to selecting one instance (positive bag) or selecting no instances (negative bag).  Instance selection is formulated as inference in the CRF.  This lets all bags to be considered jointly in training and testing.  Thus, bags are jointly classified based on unary instance classifiers and pairwise dissimilarity measurements.  \cite{Hajimirsadeghi2017MIClassificationMarkovNetworks} introduced a max-margin classification scheme using Markov networks.  \cite{Yuksel2015MIHMMLandmine,Yuksel2015MIHMMLandmine2} developed a multiple instance \textit{Hidden Markov Model} (MI-HMM) for use in landmine detection.  In this scenario, each bag is associated with a label, however, the bags can be composed of time sequences of variable length.  A noisy-OR relationship is assumed between the sequences within each bag and the joint probability of the bags of sequences and the corresponding labels for the bags is maximized with a stochastic expectation maximization.  PGMs have proven to work well on MIL problems exhibiting time series data and data with structural dependence.

\paragraph{Dictionary Learning:}
\textit{Dictionary Learning} is a method of learning how to reconstruct a dataset from a much smaller set of building blocks called \textit{atoms} \citep{Cook2015Thesis}.  Given a training data set, the goal is to learn the set of atoms and sparse weights which reconstruct the data.  At test, a bag or instance can be classified based on the atoms which effectively reconstruct the sample.  Multi-Instance Dictionary Learning (MIDL) uses bag-level information with a least angle regression to alternatively learn a dictionary which represents the training data and regression weights for classification.  Max-Margin Multiple Instance Dictionary Learning (MMDL) adopts the idea of the bag of words (BoW) model and trains a set of linear SVMs as codebooks \citep{Jiao2017Thesis}. Functions of Multiple Instances (FUMI) is a supervised technique which tries to learn target and background dictionaries such that a target instance can be written as a linear combination of a single target concept and multiple background atoms .  This formulation considers target instances that may contain portions of background signature, as with sub-pixel target detection in hyperspectral imagery.  A known problem with FUMI is that it does not work well with noisy labels.  To account for this problem, extended Functions of Multiple Instances (eFUMI), uses bag-level labels to identify the data \citep{Jiao2017Thesis,Cook2015Thesis}.  A function is built in to determine whether a point labeled as target actually contains a portion of the target dictionary atoms. Task-driven extended Functions of Multiple Instances (TD-eFUMI) adopts the MI aspect of eFUMI and Task-Driven Dictionary learning to simultaneously learn target and background dictionaries in conjunction with a classifier \citep{Cook2016LandmineTaskDriveneFUMI}.  \cite{Zare2016MIACE} proposed the Multiple Instance Adaptive Cosine/Coherence Estimator and Spectral Matched Filter (MI-ACE and MI-SMF).  These methods learn discriminative prototypes under the multiple instance learning framework to classify instances.  However, these methods inherently consider only a single target concept.  In order to capture intra-class variation among target instances, Multi-Target MI-ACE/SMF were proposed by \cite{Bocinsky2019Thesis}.  Finally, \cite{Jiao2018MIHE2} presented Multiple Instance Hybrid Estimator (MI-HE) to learn multiple target and background concepts by maximizing the probability that positive bags are labeled as positive and negative bags are labeled as negative under a noisy-OR model.  Multiple instance dictionary learning problems have been applied successfully to sub-pixel hyperspectral target detection and landmine detection tasks \citep{Bocinsky2019Thesis, Zare2015MILLandmineEMI, Zare2016MIACE, Cook2016LandmineTaskDriveneFUMI, Jiao2018MIHE2}. 


\paragraph{Ensembles of Classifiers:}
\textit{Ensemble learning} paradigms train multiple versions of a base classifier and aggregate the results to achieve a stronger classifier than any of the individuals.  Ensemble methods are typically broken into two realms: \textit{bagging} and \textit{boosting}.  Bagging employs bootstrap sampling to generate several training subsets from the original training set, then trains a learner on each data subset.  The predictions from each component learner are aggregated in order to provide a final class score.  \cite{Zhou2003MIEnsemble} studied whether ensemble learning paradigms could be used to enhance MI learners by applying bagging on base MI learners, namely, Diverse Density and Citation K-NN.  Random Forest classifiers operate under the bagging paradigm and use a technique called \textit{divide-and-conquer}.  These classifiers deploy an ensemble of decision trees which iteratively divide the feature space and make simple thresholding decisions.  The predicted class labels provided by each tree in the forest are combined to give a final class label.  \cite{Leistner2010MIForests} proposed MIForest which combines the random forest learning algorithm with MIL.  Since only bag-level labels are known, MIForest treats the label of each instance as a random variable defined over a space of probability distributions.  The instance labels are disambiguated by iteratively searching for distributions which minimize the overall classification error.  

The other paradigm of ensemble learning is called boosting. The goal of boosting is, using the entire training set, to find a  weighted combination of weak learners (that may perform only slightly better than chance), such that the combination produces a strong classifier with high classification accuracy \citep{Zhang2006MIBoosting}. Several MI boosting approaches have been proposed in the literature.  Section \ref{sec:MILBoost} discusses a popular variant, MIL-Boost, in detail.


\subsection{Multiple Instance Boosting (MIL-Boost)} \label{sec:MILBoost}

\paragraph{Gradient Boosting Overview}
In the standard supervised learning setting, the goal of binary classification is to learn a classification function $h:\mathcal{X} \rightarrow \mathcal{L}$ which maps data in $\mathcal{X}$ to a binary class label $\mathcal{L} \in \{-1,+1\}$.  The objective of boosting is to train a classifier of the form
\begin{align}
	\bm{h}(\bm{x}) = \sum_{t=1}^{T}\alpha_{t}h_{t}(\bm{x})
\end{align}
\noindent
where each $h_{t}:\mathcal{X} \rightarrow \mathcal{L}$ is a \textit{weak learner} whose performance may only be slightly above chance, and the weights $\alpha_{t}$ are each weak learners' relative importance \citep{Babenko2008MIBoosting}.  \textit{Boosting} is a specific case of \textit{ensembling} which combines multiple weak learners into a single \textit{strong} classifier with low classification error.  In each phase of training, samples classified incorrectly are given more weight in order to improve classification performance in the next iteration.  The response of each weak classifier $h_{t}$ is given as the maximum response over all instances in the training set:
\begin{align}
	h_{t} = \arg \max_{h} \sum_{n=1}^{N}w_{n}h(\bm{x}_{n})
\end{align}

\paragraph{MIL-Boost}
Boosting under the MIL framework was originally proposed by \cite{Zhang2006MIBoosting} and is known as \textit{MIL-Boost}.  Under MIL, it is assumed that every instance has a true label $l_{kn} \in \{ -1,+1 \}$.  A bag is labeled positive if at least one of its instances are positive, and a bag is labeled negative if every instance is negative.  This means that the label of a bag is provided as the max label over all instances in the bag:
\begin{align}
	L_{k} = \max_{n}(l_{kn})
\end{align}
\noindent
In this setting, the goal is to learn a classifier $\bm{h}$ using only bag-level labels such that $\max_{n}(\bm{h}(\bm{x}_{kn})) = L_{k}$. The score given to a sample is $l_{kn} = \bm{h}(\bm{x}_{kn})$ and $\bm{h}(\bm{x}_{kn}) = \sum_{t=1}^{T}\alpha_{t}h_{t}(\bm{x}_{kn})$ which is a weighted sum of predicted labels from each of the weak classifiers.  Obviously, the sign of the prediction from the strong classifier provides the class label for the instance.

A natural objective is to minimize the negative log-likelihood between instances and their predicted labels.  This can also be done at the bag-level.  The probability that an instance is positive is given by the standard logistic function
\begin{align}
	p_{kn} = \frac{1}{1+\exp(-l_{kn})}
\end{align}
\noindent
and the probability that bag $k$ is positive is given by a ``noisy OR", $p_{k} = 1 - \prod_{n \in k}(1- p_{kn})$.  Therefore, the likelihood assigned to a set of training bags is given by:
\begin{align}
	\mathcal{L}(\bm{h}) = \prod_{k=1}^{K}p_{k}^{L_{k}}(1-p_{k})^{(1-L_{k})}
\end{align}
\noindent
where $L_{k} \in \{0, 1\}$ is the actual label of the bag.

Following the idea of gradient boosting, the weight on each training instance is given as the derivative of the log-likelihood with respect to a change on the score given to the instance.  Thus gradient descent can be used to update the the strong classifier.  Pseudo-code for MIL-Boost is provided in Algorithm \ref{alg:mil_boost}.

\begin{algorithm}
	\caption{MIL-Boost}
	\label{alg:mil_boost}
	\begin{algorithmic}[1]
	\Require {Dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$}
	\For{$t=1$ to $T$}                    
	\State {Compute weights $w_{kn} = -\frac{\mathcal{\partial L}}{\partial \bm{h}_{kn}}$}
	\State \begin{varwidth}[t]{\linewidth}
	{Train weak classifier $h_{t}$ using weights $|w_{kn}|$ } \par
		\hskip\algorithmicindent $h_{t} = \arg \min_{h}\sum_{kn}\bm{1}(h(\bm{x}_{kn} \neq L_{k}))|w_{kn}|$
	\end{varwidth}
	\State \begin{varwidth}[t]{\linewidth}
		{Find $\alpha_{t}$ via line search to minimize $\mathcal{L}(\bm{h})$} \par
		\hskip\algorithmicindent $\alpha_{t} = \arg \min_{\alpha} \mathcal{L}(\bm{h} + \alpha h_{t})$
	\end{varwidth}
	\State Update strong classifier $\bm{h} \gets \bm{h} + \alpha_{t} h_{t}$
	\EndFor
	\end{algorithmic}
\end{algorithm}

Each round of boosting consists of updating the weak learners according to the instances they misclassified and updating the strong classifier according to the new weights calculated over the weak learners.  The goal of MIL-Boost is to be able to correctly classify each instance in a test bag as being positive or negative such that the label of the bag is given as the maximum label over all instances in the bag.

\subsection{Multiple Instance Ranking}\label{sec:MI_Ranking}

Another primary task in MIL is \textit{ranking} \citep{Carbonneau2016MILSurvey}.  The ranking problem is different from classification because, instead of providing a binary class label, the objective is to order a set of bags \citep{Bergeron2008MIRanking,Bergeron2012FastBundleMILRanking} or instances \citep{Hu2008MIRanking} according to preference for a particular task.  Ranking is a key task under \textit{Preference Learning}, or learning to predict preferences on a set of alternatives, which are often represented in the form of an order relation \citep{Furnkranz2003RankingSummary}.  The statement that $\bm{x}$ is preferred to $\bm{x}'$ can be simply expressed as an inequality relation $f(\bm{x}) > f(\bm{x}')$, where $\bm{x}$ and $\bm{x}'$ are instances, and $f$ defines a preference function.  For example, given a news story about the Olympics, one might prefer to give it the label ``sports" rather than ``politics" or ``weather".  Alternatively, one might prefer to label one bag or instance as ``positive" over another.  In machine learning, the preference learning problem is often analyzed in two cases: \textit{learning instance preference} and \textit{learning label preference} \citep{Chu2005LearningPreferencesWithGaussianProcesses}.  Under the scenario of learning instance preferences, the training set consists of a set of pairwise preferences between instances.  The objective is to learn the underlying ordering from the set of pairwise distances (such as ordering bags from the ``most positive" to ``least positive").  Alternatively, the goal of label preference learning is to order a pre-defined set of labels for each individual instance (such as ordering the preference of ``positive" or ``negative" on each individual bag or instance) \citep{Dekel2004Ranking,Aiolli2004LearningPreferences}.   

Ranking under the multiple instance framework was proposed by \cite{Bergeron2008MIRanking} for predicting hydrogen atom grouping in computational chemistry. The method is called \textit{MIRank} \citep{Bergeron2012FastBundleMILRanking}.  MI ranking differs from MIC in that the label for each bag is not known.  Instead, the MI ranking algorithms are provided preference information between pairs of bags. MIRank considers the partial ranking problem, which inherently exhibits three levels of structure: items (instances) belong to bags and bags belong to boxes.  The objective is to learn a ranking function that can identify the preferred bag in each box.  A concrete example where this framework can be applied is in learning positive target concepts.  For a set of video frames (boxes), one may want to predict the smaller image chips (bags) that have the highest probability of containing a target object (positive instances).  MIRank uses a linear prediction function to rank instances in individual bags.  The ranking of instances $\bm{x}_{i}$ and $\bm{x}_{j}$ in bags $I$ and $J$ is guided by the preference information between $I$ and $J$. Work by \cite{Hu2008MIRanking} introduced \textit{multiple-instance ranking} based on a max-margin framework.  In this setting, images were represented by sets of regions and the goal was to rank images according to relevance to a keyword.  Assuming the preference relationship that $\bm{x}_m$ is preferable to $\bm{x}_n$ is denoted by $\bm{x}_m \succ \bm{x}_n$, the goal is to induce a ranking function $f:\mathbb{R}^{D} \to \mathbb{R}$ that fulfills the set of constraints
\begin{align}
	\forall \bm{x}_m \succ \bm{x}_n: f(\bm{x}_{m}) > f(\bm{x}_{n})
\end{align}
\noindent
The value of $f(\bm{x}_n)$ is referred to as the ranking score of $\bm{x}_n$ and is typically a linear function $f(\bm{x}_n) = \langle \bm{w},\bm{x}_{n} \rangle = \bm{w}^{T}\bm{x}_{n}$.  Adding slack variable $\zeta_{mn}$, the optimization problem can be solved with with the following objective:
\begin{align}
	\begin{split}
	\min_{\bm{w},\bm{\zeta}} \quad &\frac{1}{2} ||\bm{w} ||^{2} + \gamma \sum_{m,n} \zeta_{mn} \\
	\text{s.t.} \quad &\forall \bm{x}_m \succ \bm{x}_n: \langle \bm{w},\bm{x}_{m} \rangle \geq \langle\bm{w},\bm{x}_{n} \rangle + 1 -\zeta_{mn}\\
	&\forall m,n: \zeta_{mn} \geq 0\\
	\end{split}
\end{align}
\noindent
which is referred to as a \textit{ranking SVM}.  Assuming the optimal solution is $\bm{w}^{*}$, the ranking score of a test point $\bm{x}'$ is given as $f(\bm{x}') = \langle \bm{w}^{*}, \bm{x}' \rangle$.  This framework assumes each sample $\bm{x}_{n}$ is a single instance.  In multiple-instance ranking, we are given a set of preference relations between bag pairs and it is assumed that the score of a bag $\bm{B}_{k}$ is determined by the scores of the instances it contains
\begin{align}
	h(\bm{B}_{k}) = h \left( \{ f(\bm{x}_{n})\}_{n=1}^{N_{k}} \right)
\end{align}
\noindent
Under this formulation, the objective can be re-written to consider bag scores as
\begin{align}
\begin{split}
\min_{f\in\mathcal{H},\bm{\zeta}} \quad &\frac{1}{2} ||f ||^{2}_{\mathcal{H}} + \gamma \sum_{m,n} \zeta_{mn} \\
\text{s.t.} \quad &\forall \bm{B}_m \succ \bm{B}_n: h(\bm{B}_{m}) \geq h(\bm{B}_{n}) + 1 -\zeta_{mn}\\
&\forall m,n: \zeta_{mn} \geq 0\\
\end{split}
\end{align}
\noindent
A bag's score is determined by the scores of its instances.  Different functions for providing a bag score have been investigated, including using the max of instance scores, the mean of  instance scores and the softmax of instance scores.

Besides MIRank and multiple-instance ranking, \cite{Asif2017LargeMarginMIRanking} recently proposed \textit{pyLEMMINGS}, which implements locally linear MI ranking by learning a large margin discriminant function from bags with corresponding integer rankings.  While ranking has been successfully applied to text information retrieval, image retrieval \citep{Hu2008MIRanking} and bioinformatics \citep{Asif2017LargeMarginMIRanking}, ranking under the MIL framework is still a relatively unexplored area of research.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Learning} \label{sec:Manifold_Learning}

Real-world remote sensing data such as hyperspectral imagery, ground-penetrating radar scans and sonar signals are naturally represented by high-dimensional feature vectors.  However, in order to handle such real-world data adequately, its dimensionality usually needs to be reduced \citep{VanDerMaaten2009DRReview,Belkin2004SemiSupLearningRiemannianManifolds}. The problem considered in this work is discovering feature representations that promote class discriminability for target or anomaly detection.  This is typically achieved in a few different ways.  First, features can be ``engineered" by leveraging known qualities about the data statistics, sensors or collection environments.  Alternatively, poor features can be projected into a high-dimensional space (such as a Kernel Hilbert Space) using a kernel function.  A popular approach is to extract discriminative features from systems which learn data representations in an end-to-end fashion, such as \textit{artificial neural networks} \citep{Tschannen2018RecentAdvancesAutoencoder,Chen2019DeepAutoencoders,Kosiorek2019StackedCapsuleAutoencoders}.  Finally, this work focuses on methods which transform the data into a new (often lower-dimensional) coordinate system which optimizes feature representations for discrimination \citep{Vural2018StudySupervisedManifoldLearning}.


The application of \textit{dimensionality reduction} (DR) has proven useful in myriad applications in the literature, such as: visualization of high-dimensional data, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and reducing the effects of the Curse of Dimensionality \citep{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.  In classification of object entities, it is often assumed that classes can be described by an \textit{intrinsic} subset of representative features which describe the factors of variation in a set of data \citep{Belkin2006ManReg}. These structures are called intrinsic \textit{manifolds}, and they represent the generating distributions of class objects exactly by the number of degrees of freedom in a dataset \citep{Thorstensen2009ManifoldThesis, Belkin2004SemiSupLearningRiemannianManifolds}.     Consider the example shown in Figure \ref{fig:manifold_eg}.  This figure shows samples from the LFW Faces in the Wild dataset \citep{LFW}.  While each individual image is represented by a vector of features (pixel intensities in this case) in $\mathbb{R}^{1850}$, the dataset only exhibits a few degrees of freedom, such as: illumination, face direction, facial expression and whether or not the subject is wearing glasses.  Thus, it is intuitive that the dataset lies on a smooth, intrinsic submanifold spanning approximately four dimensions which inherently capture the degrees of freedom in the data.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/face_manifold"}
		\caption[Factors of variation example]{Manifold learning example on a single class of the LFW Faces in the Wild dataset.  The image shows the embedding of 1850-dimensional features into a 2-dimensional space.  The samples are distributed according to the two greatest factors of variation in the dataset, seemingly, illumination and the direction the subject is facing.}
		\label{fig:manifold_eg}
	\end{figure*}
\end{center}

The goal of manifold learning is then to discover embedding functions which take data from the input feature space and transform it into a lower-dimensional coordinate system (also called a \textit{latent space} in the literature) which captures the ``useful" properties of the data, while enforcing constraints such as smoothness (the transformation function should not produce sporadic images), continuity (no discontinuous points on the hyper-surface), topological ordering (neighbors in the input space should also be neighbors in the embedded space ) or class separability (samples from the same class should fall metrically close to each other in the embedded space and disparate classes should be distinctly far) \citep{Vural2018StudySupervisedManifoldLearning}.

This dissertation focuses on investigating the use of manifold learning to increase instance discriminability in the latent space, where labels are solely provided at the bag-level.   While there is an expansive literature in unsupervised manifold learning methods, this document will pay special attention to both strictly- and semi-supervised methods, since they are typically adaptations of unsupervised approaches, as well as manifold learning under the MIL framework.

\subsection{Definition and General Notation}
Most studies perform classification or regression after applying unsupervised dimensionality reduction.  However, it has been shown that there are advantages of learning the low-dimensional representations and classification/regression models simultaneously \citep{Chao2019RecentAdvancesSupervisedDimRed,Rish2008SupDimRedGLM}.  Considering classification as the main goal of dimensionality reduction, this section provides a summary of the current literature in the area. \newline

Given a data matrix $\bm{X} = [\bm{x}^{T}_{1}, \bm{x}^{T}_{2}, \dots, \bm{x}^{T}_{N}] \in \mathbb{R}^{N \times D}$ where $N$ is the total number of samples and $D$ is the dimensionality of the input feature space, general dimensionality reduction seeks to find a representation $\bm{Z} \in \mathbb{R}^{N \times d}$ with $d \ll D$ that enhances the between-class separation while preserving the intrinsic geometric structure of the data\citep{Vural2018StudySupervisedManifoldLearning}.  In other words, it is assumed that the data lie on a smooth manifold $\mathcal{X}$, which is the image of some parameter domain $\mathcal{Z} \subset \mathbb{R}^{d}$ under a smooth mapping $\Psi : \mathcal{Z} \rightarrow \mathbb{R}^{D}$.  The goal of manifold learning is to discover an inverse mapping to the low-dimensional pre-image coordinates $\bm{z}_n \in \bm{z}$ corresponding to points $\bm{x}_n \in \bm{X}$.  The data matrices $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}]$ and $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ are of size $N \times D$ and $N \times d$, respectively.  Since these low-dimensional data representations are unknown, they are often referred to as \textit{latent} vectors and the span in $\mathbb{R}^d$ is sometimes called the \textit{latent feature space} or \textit{latent space} for brevity \citep{Murphy2012}. The primary difference between traditional, unsupervised manifold learning and supervised approaches is that, in supervised manifold learning, data matrix $\bm{X} $ is accompanied with a corresponding label vector $\bm{l} = [l_1, \dots, l_N]$ indicating the corresponding class labels of each sample in $\bm{X}$. \newline

Manifold learning methods can be subdivided into a wide taxonomy of approaches, with \textit{linear} and \textit{nonlinear} at the root. Nonlinear approaches can be further divided into purely global methods and approaches that capture global structure solely from local information.  We begin with a review of popular linear manifold learning techniques before moving into the realm of nonlinear approaches. Base, unsupervised methods are reviewed along with corresponding supervised and semi-supervised adaptations. 

\subsection{Linear Manifold Learning}
A review of linear manifold learning approaches is provided.  Linear approaches are advantageous over many nonlinear techniques because they inherently allow for out-of-sample extensions.  In other words, linear transformation matrices are learned which can be easily applied on data not included in the training set. (Although there are nonlinear approaches for which this is true in locally linear neighborhoods \citep{Roweis2000LLE}) However, linear approaches are limited in their abilities to capture irregular data surfaces \citep{Kegl2008PrincipalManifoldsTextbook}.  Principal Component Analysis (PCA), Multi-dimensional Scaling (MDS), Nonnegative Matrix Factorization (NMF) and Fisher's Linear Discriminant Analysis (LDA) are reviewed.  General approaches are discussed and supervised as well as nonlinear extensions are elaborated. Special focus is given to (LDA), as it is the only inherently supervised technique out of the included approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  PCA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Principal Component Analysis (PCA)} \label{sec:PCA}

\paragraph{Unsupervised PCA}
Principal Component Analysis (PCA) is arguably the most popular (and best-studied) technique for dimensionality reduction and manifold learning.  It attempts to learn an orthogonal projection of the input data into a lower-dimensional space, known as the principal subspace, such that the variance of the projected data is maximized \citep{Chao2019RecentAdvancesSupervisedDimRed}.  In other words, each \textit{principal axis}, or \textit{principal component}, of the learned coordinate system is orthogonal to the other principal components.  In summary, the problem of PCA is to discover basis vectors which linearly combine to reconstruct the data.  In practice, data in the input feature space are projected into a new coordinate system of $d$ dimensions, such that the variance along each principal axis is maximized and the reconstruction errors of the data are minimized in the mean-square sense \citep{Thorstensen2009ManifoldThesis}.  Let $V$ be a $d$-dimensional subspace of $\mathbb{R}^{D}$ and let  $\bm{w}_1, \dots, \bm{w}_D$ be an orthonormal basis of $\mathbb{R}^{D}$ such that $\bm{w}_1, \dots, \bm{w}_d$ is a basis of $V$.  The goal of PCA is to find an orthogonal set of basis vectors $\bm{w}_n \in \mathbb{R}^{D}$ and corresponding latent coordinates $\bm{z}_n \in \mathbb{R}^{d}$ such that the average reconstruction error is minimized \citep{Murphy2012}
\begin{align}
	J(\bm{W}, \bm{Z}) = \frac{1}{N}\sum_{n=1}^{N} ||\bm{x}_n - \hat{\bm{x}}_n ||^{2}
\end{align}
\noindent
where $\hat{\bm{x}}_n = \bm{W}\bm{z}_{n}$, subject to the constraint that $\bm{W}$ is \textit{orthonormal}, or that $\bm{w}_{i}^{T}\bm{w}_{j}=0,\forall i \neq j$ and $\bm{w}_{i}^{T}\bm{w}_{i}=1 $.  This is equivalently written as 
\begin{align}
		J(\bm{W}, \bm{Z}) = ||\bm{X} - \bm{W}\bm{Z} ||^{2}_{F}
\end{align}
\noindent
where $\bm{Z}$ is a $N \times d$ matrix with the $\bm{z}_{n}$ in its rows and $||\bm{A}||_{F}$ is the \textit{Frobenius norm} of matrix $\bm{A}$, defined by 
\begin{align}
	||\bm{A}||_{F} &= \sqrt{\sum_{m=1}^{M}\sum_{n=1}^{N}a^{2}_{mn}} = \sqrt{tr(\bm{A}^{T}\bm{A})} = ||\bm{A}(:)||_{2}
\end{align} 
\noindent
The optimal solution is obtained by setting $\hat{\bm{W}} = \bm{U}_{d}$, where $\bm{U}_{d}$ contains the eigenvectors corresponding to the $d$ largest eigenvalues of the mean-subtracted, empirical data covariance matrix, $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}(\bm{x}_{n}-\hat{\bm{\mu}})(\bm{x}_{n}-\hat{\bm{\mu}})^{T}$, where $\hat{\bm{\mu}}$ is the empirical data mean.  Therefore, the low-dimensional encoding of the data is given by $\bm{z}_n = \hat{\bm{W}}^{T}\bm{x}_n$, which is the orthogonal, linear projection of the data onto the column space spanned by the eigenvectors of the $d$ largest eigenvalues of the empirical data covariance.  

The example shown in Figure \ref{fig:pca_eg} demonstrates the projection of 2-dimensional data onto the first principal axis.  As can be seen from the figure, the first principal axis corresponds to the direction of maximal variance of the data.  PCA from the viewpoint of variance maximization is often called the \textit{analysis view} of PCA \citep{Murphy2012}.
\begin{center}
	\begin{figure*}[t]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/pca"}
		\caption[PCA example.]{Example of dimensionality reduction on a synthetic dataset using principal component analysis.  Left) Two-dimensional data, where $\bm{w}_{1}$ and $\bm{w}_{2}$ correspond to the first and second principal axes of the dataset, respectively.  Right) Orthogonal projection of the data onto the first principal axes.  It can be observed that $\bm{w}_{1}$ corresponds to the direction of maximum variance for the data. }
		\label{fig:pca_eg}
	\end{figure*}
\end{center}
PCA has been successfully applied to a large  number of domains such as face recognition, coin classification and seismic series analysis \citep{VanDerMaaten2009DRReview}. It's success is partially because of its convenience of use.  Embedding out-of-sample points with PCA is simple since the transformation is just a rotation and scaling. However PCA suffers from a few drawbacks.  First, the dimensionality of the covariance matrix is proportional to the dimensionality of the data points.  As a result, the computation of the eigenvectors may be infeasible or untrustworthy (singular) for high-dimensional data.  Additionally, PCA focuses mainly on preserving large pairwise distances between data samples instead of retaining local relationships, which may be important in certain applications.  PCA also assumes Gaussian distributed data, which is unlikely in real-world applications. Finally, PCA is sensitive to feature magnitude. It is typical to standardized data before applying PCA as it can be misled by directions in which the variance is high simply because of the measurement scale.

Many extensions have been made to PCA, including the development of nonlinear versions (Sections \ref{sec:KPCA}and \ref{sec:Autoencoders}) \citep{Scholkopf1999KPCA, Scholz2008NonlinearPCA} and formulating it as a factor analysis problem (Section \ref{sec:FA_and_PPCA}) \citep{Tipping1999PPCA}. 

\paragraph{Independent and Canoncial Component Analyses}
It is also worth mentioning two popular approaches closely related to PCA, namely, \textit{Independent Component Analysis} (ICA) and \textit{Canonical Component Analysis} (CCA).  ICA attempts to solve the \textit{blind-source separation problem}, in which the goal is to deconvolve mixed signals into their constituent parts \citep{Murphy2012,Hyvarinen2000ICA,Tharwat2018ICATutorial}. Instead of discovering the directions of maximum variance as done with PCA, ICA attempts to uncover the directions such that the data projected onto these directions have maximum statistical independence.  On the other hand, CCA jointly considers multiple variables (multiple feature spaces) and tries to discover the correlations between them.  CCA looks for directions in each feature space such that the data projected onto the direction found in each space has the maximum possible correlation \citep{Bach2005CCA}.  Thus, CCA can be used to simultaneously reduce the dimensionality of data in multiple feature spaces \citep{Murphy2012}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  MDS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Multi-Dimensional Scaling (MDS)} \label{sec:MDS}

\paragraph{Unsupervised MDS} 
Most modern manifold learners have theoretical and algorithmic roots in one of three basic dimensionality reduction techniques: PCA, K-means and \textit{Multidimensional Scaling} (MDS) \citep{Torgerson1952MDS}. Whereas PCA looks for linear projection bases which are constructed from the eigenvectors of a data covariance or scatter matrix, MDS tries to find a linear projection that preserves pairwise distances as well as possible. This idea is demonstrated by Figure \ref{fig:mds_example}, where the pairwise distances between samples in the 3-dimensional space are preserved in the 2-dimensional embedding space. While MDS does not construct an embedded manifold explicitly, it holds the status of being one of the first ``one-shot" (non-iterative) manifold learners, such as Isomap and Locally Linear Embedding (LLE), which are discussed later in this literature review \citep{Kegl2008PrincipalManifoldsTextbook}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/mds"}
		\caption[Example of MDS distance preservation.]{Example of multidimensional scaling on a synthetic dataset.  The coordinates of the two-dimensional data (right) approximately preserve the pairwise distances between samples in the three-dimensional space (left).}
		\label{fig:mds_example}
	\end{figure*}
\end{center}
The steps of MDS correspond exactly to those of PCA except that, instead of a scatter matrix $\bm{S}=\frac{1}{N}\bm{X}\bm{X}^{T}$, MDS operates with a positive semi-definite, dissimilarity matrix $\bm{D} \in \mathbb{R}^{N \times N}$, where $N$ is the number of data samples and a real, symmetric Gram matrix $\bm{K} = \bm{X}^{T}\bm{X}$ (inner-product matrix) where $\bm{K}_{mn}$ is the inner product between $\bm{x}_{m}$ and $\bm{x}_{n}$.  The problem of MDS is posed as finding $d$-dimensional Euclidean coordinates for each sample $\bm{x}_{n}$ in dataset $\bm{X}$ such that the Euclidean distances in the low-dimensional embedding space are proportional to the pairwise distances in the input space \citep{Thorstensen2009ManifoldThesis,Sorzano2014DRReview}. While the literature poses several cost functions for this task, this review focuses on classical MDS, which is described as follows:

First, the pairwise distance matrix $\bm{D}$ is computed such that 
\begin{align}
	\bm{D}_{mn} = \mathcal{D}_{\mathcal{X}}(\bm{x}_{m},\bm{x}_{n}) = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} = (\bm{x}_{m} - \bm{x}_{n})^{T}(\bm{x}_{m} - \bm{x}_{n}) 
\end{align}
\noindent
where $\mathcal{D}_{\mathcal{X}}(\cdot,\cdot)$ is a chosen dissimilarity metric (Euclidean distance for classical MDS).  Then, the double-centered Gram matrix $\bm{K}$ is computed by
\begin{align}
	\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}
\end{align}
\noindent
where 
\begin{align}
	\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}
\end{align}
\noindent
with $\mathbb{I}_{N}$ denoting the $N \times N$ identity matrix and $\bm{e}= (1, \dots, 1)^{T}$ the $N \times 1$ column vector of all ones.  Multiplying $\bm{D}$ on both sides by $\bm{H}$ performs \textit{double centering}, which subtracts the row and column means from $\bm{D}$ (and adds back the global mean which gets subtracted twice), so that both the row and column means of $\bm{K}$ are equal to zero.  Double centering is necessary to remove the translational freedom from the low-dimensional embedding coordinate representations.  Without it, the embedding points would be arbitrary to a translational degree of freedom.  Moreover, the mean removal ensures that the solution has a meaningful interpretation.  With the pairwise distances centered around zero, the largest eigenvectors of the kernel matrix (Equation \ref{eq:MDS_eig}) correspond to the directions of maximum variance between the pairwise similarities.  This ensures that global distance relations are preserved through the projection to the low-dimensional space. 

The goal of MDS is to find $\bm{Z}= \{\bm{z}_{1}, \dots, \bm{z}_{N}\} \in \mathbb{R}^{d}$ which minimizes the objective 
\begin{align}
	J(\bm{D}_{\bm{X}},\bm{D}_{\bm{Z}}) = ||\bm{K}_{\bm{X}} - \bm{D}_{\bm{Z}} ||^{2} = \left |\left|-\frac{1}{2} \bm{H} (\bm{D}_{\bm{X}} - \bm{D}_{\bm{Z}}) \bm{H} \right |\right|^{2}
\end{align}
Similarly to PCA, this can be solved by a generalized eigenvalue problem 
\begin{align} \label{eq:MDS_eig}
	\bm{K}v = \lambda v
\end{align}
\noindent
such that 
\begin{align}
	\bm{Z} = \bm{V}\bm{\Lambda}^{\frac{1}{2}}
\end{align}
\noindent
with $\bm{\Lambda}^{\frac{1}{2}} = diag(\sqrt{\lambda_{1}},\dots,\sqrt{\lambda_N})$ being a diagonal matrix with entries equal to the square roots of the eigenvalues of $\bm{K}$ sorted from largest to smallest ($\lambda_{1} \geq  \lambda_{2} \geq \dots \geq 0$), and $\bm{V} = \{ \bm{v}_{1}, \dots, \bm{v}_{N} \}$ the corresponding eigenvectors.  The low-dimensional embedding coordinates $\bm{Z} \in \mathbb{R}^{N \times d}$ are obtained by $\bm{Z} = \{\sqrt{\lambda_{1}}\bm{v_{1}}, \dots, \sqrt{\lambda_{d}}\bm{v_{d}} \}$ \citep{Chao2019RecentAdvancesSupervisedDimRed}.  Pseudo-code for MDS is provided in Algorithm \ref{alg:MDS}.

\begin{algorithm}[H]
	\caption{MDS}
	\label{alg:MDS}
	\begin{algorithmic}[1]
		\Require {Distance matrix $\bm{D} \in \mathbb{R}^{N \times N}$, embedding space dimensionality $d$}
		\Ensure {Low-dimensional data representations $\bm{Z} \in \mathbb{R}^{N \times d}$}
		\State {Calculate  $\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}$, where $\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}$ is the centering matrix} 
		\State {Compute eigenvectors and eigenvalues $\bm{K}v = \lambda v$} 
		\State {$\bm{V}',\bm{\Lambda}' \gets $ $\textsc{SortDecreasing}(\bm{V},\bm{\Lambda})$}
		\State {$\bm{Z} \gets \bm{V}'\bm{\Lambda}^{'\frac{1}{2}}$}     
	\end{algorithmic}
\end{algorithm}

It has been proven that the eigenvalues of Gram matrix $\bm{K}$ and covariance $\bm{S}$ are the same, and that the space spanned by MDS and PCA are the identical for any $d \leq rank(\bm{K}) = rank(\bm{S})$.  This implies that a rotation matrix $\bm{A}$ could be found such that $\bm{A}^{T}\bm{Z}_{MDS} = \bm{Z}_{PCA}$ \citep{Sorzano2014DRReview}.  Additionally, MDS can be computed even if the data observation matrix $\bm{X}$ is unknown.  All that is needed is the Gram matrix or a dissimilarity matrix.  This feature potentially allows MDS to be applied in a variety of data-sensitive and privacy-concerned scenarios. 

A pitfall of MDS is that it focuses on retaining global pairwise distances as opposed to local distances, which are typically much more important for capturing the geometry of the data \citep{VanDerMaaten2009DRReview}.  Several MDS variants have been proposed to address this weakness.  A popular variant is known as \textit{Sammon Mapping} and is discussed in Section \ref{sec:sammon_mapping}. 

\paragraph{Supervised MDS}
As with most manifold learning methods in the literature, MDS does not inherently consider class information when learning the embedding function.  In attempt to promote class separability in the  low-dimensional embedding space, \cite{Witten2011SuperMDS} proposed a \textit{Supervised Multidimensional Scaling} (SMDS).  this method follows the idea of traditional MDS where the goal is to find low-dimensional coordinate or \textit{configuration points} $\bm{z}_{n} \in \mathbb{R}^{d}$, such that pairwise distances in the input feature space are preserved in the  embedding space.  Incorporating class label information, the goal of SMDS is to not only preserve distances, but ensure the coordinate values $z_{mk} > z_{nk}$ when $l_{m} > l_{n}, \quad \forall k = 1, \dots, d$, where $l$ are the instance-level labels and $d$ is the dimensionality of the embedding space.  Considering the binary target classification case, SMDS can be formulated as
\begin{align}
	\min_{\bm{Z}} \quad \frac{1}{2}(1-\alpha)\sum_{m=1}^{N}\sum_{n=1}^{N}(\bm{D}_{mn} - ||\bm{z}_{m} - \bm{z}_{n} ||^{2}) + \alpha \sum_{m:l_{m}=1} \sum_{n:l_n=2} \sum_{k=1}^{d}\left(\frac{\bm{D_{mn}}}{\sqrt{d}} - (z_{nk} - z_{mk})^{2} \right)
\end{align}
This objective has two terms.  The first is the traditional metric MDS \textit{stress}.  This term attempts to ensure that the Euclidean distances of two points in the embedding space is the same as the dissimilarity between the points in the input feature space.  The second term is the supervised term which enforces that each dimension of the embedded configuration points be larger if belonging to the class with the larger label, and smaller if belonging to the class with a smaller-valued label.  The term $\alpha \in [0,1]$ is a tuning parameter.  When $\alpha = 0$, the objective reduces to the MDS stress function.  As $\alpha$ increases, however, the objective becomes increasingly more supervised, focused on ensuring class separation of the training data.

A least square regression was applied to estimate the embedding function for out-of-sample  test points.  SMDS was successfully applied to tasks in data visualization, bipartite ranking and classification of prostate data and USPS handwritten digits.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  NMF  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Non-negative Matrix Factorization (NMF)} \label{sec:NMF}
\textit{Non-negative Matrix Factorization} (NMF) is a tool for linear dimensionality reduction that, given a set of data $\bm{X} \in \mathbb{R}^{N \times D}$, aims to decompose the matrix into a coefficient matrix $\bm{Z} \in \mathbb{R}^{N \times d}$ and basis matrix $\bm{W} \in \mathbb{R}^{d \times D}$ \citep{Gillis2014NMF}.  In this case, the columns of matrix $\bm{W}$ are basis elements and  the columns of matrix $\bm{Z}$  give the coordinates of data samples in the basis $\bm{W}$. Similar to PCA, the goal of NMF is to find a $\bm{Z}$ and $\bm{W}$ such that $\hat{\bm{X}} = \bm{Z}\bm{W}$ approximates the data as close as possible, given that every element must be non-negative.  This notion is formalized by
\begin{align}
	\min_{\bm{Z},\bm{W}} || \bm{X} - \bm{Z}\bm{W} ||^{2}_{F} \quad s.t. \quad \bm{Z} \geq 0, \bm{W} \geq 0
\end{align}
\noindent
Another popular variant of NMF is to substitute the Frobenius norm for the Kullback-Leibler (KL) divergence, where $D(\bm{X}||\hat{\bm{X}}) = \sum_{m,n}(\bm{X}_{mn} \log \frac{\bm{X}_{mn}}{\hat{\bm{X}}_{mn}} + \bm{X}_{mn} - \hat{\bm{X}}_{mn})$.  Given that $d \ll D$, it is intuitive that $\bm{Z} \in \mathbb{R}^{N \times d}$ provides the desired low-dimensional representations of high-dimensional data $\bm{X}.$  Many approaches have been used to solve for the non-negative matrices, including alternating least squares, projected gradient descent, coordinate descent and the Alternating Direction Method of Multipliers (ADMM) \citep{Chao2019RecentAdvancesSupervisedDimRed}.  NMF has been successfully applied to applications in image processing, text mining, hyperspectral imaging, air emission control, computational biology, blind source separation, single-channel source separation, clustering, music analysis and collaborative filtering \citep{Gillis2014NMF}.  

As described by \cite{Chao2019RecentAdvancesSupervisedDimRed}, two groups of supervised NMF have been proposed in the literature according to the way label information is utilized.  In \textit{direct supervised NMF} approaches, label information is incorporated directly into the loss function to promote learning of well-separated coordinate representations for samples in different classes.  Approaches taken under this framework include incorporating simple indicator variables to denote the class of a sample, integrating NMF with a SVM and formulating the problem under task-driven dictionary learning.  An alternative approach to the direct supervised NMF is \textit{discriminative NMF}.  Discriminative NMF approaches are based on \textit{Linear Discriminant Analysis} (LDA).  Essentially, the objective for this class of algorithm is to decompose the data matrix such that the between-class distances of the low-dimensional coordinates $\bm{Z}$ are maximized, while the within-class distances are minimized.  Supervised NMF approaches have been applied successfully to problems in acoustic separation, brain tumor detection and emotion classification.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LDA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fisher's Linear Discriminant Analysis (LDA)} \label{sec:LDA}

\paragraph{Classical LDA}
\textit{Linear Discriminant Analysis} (LDA) is a popular method for supervised, linear dimensionality reduction.  LDA currently forms the basis for Multiple Instance Learning dimensionality reduction methods exhibited in the literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity,Xu2011MI_Metric_Learning}. Whereas PCA tries to project data into a space which maximizes variance, LDA considers class label information and tries to find a transformation which both maximizes between-class (inter-class) dissimilarity and minimizes within-class (intra-class) scatter \citep{Yan2007GeneralGraphEmbeddingFramework,Chao2019RecentAdvancesSupervisedDimRed, Sun2010MIDR, Murphy2012}.   This is done by maximizing the ratio between the inter-class $\bm{S}_{b}$ and intra-class $\bm{S}_{w}$ scatter matrices, defined as:
\begin{align}
	\bm{S}_{w} = \sum_{k=1}^{K}\bm{S}_{k}
\end{align}
\begin{align}
	\bm{S}_{k} = \sum_{n \in C_{k}}(\bm{x}_{n} - \hat{\bm{\mu}_{k}})(\bm{x}_{n} - \hat{\bm{\mu}_{k}})^{T}
\end{align}
\begin{align}
	\bm{S}_{b} = \sum_{k=1}^{K}N_{k}(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})^{T}
\end{align}
\noindent
Here, $\bm{S}_{w}$ is the global within-class scatter matrix which is defined as the sum over each individual class' scatter matrix $\bm{S}_{k}$, and $\bm{S}_{k}$ is essentially an outer product between all samples belonging to class $C_{k}$ after subtracting the respective empirical class mean $\hat{\bm{\mu}_{k}}$.  This scatter matrix would be the class covariance if it was normalized by the number of samples $N_{k}$ in class $C_{k}$.  However, this normalization constant does not affect the final solution and can thus be ignored.  The between-class scatter $\bm{S}_{b}$ is defined by the sum of outer products of the differences between the empirical class means $\hat{\bm{\mu}_{k}}$ and the global data mean $\hat{\bm{\mu}}$, weighted by the number of samples in each class.  The objective of LDA is then to solve for $\bm{W}^{*}$ which maximizes the ratio $J(\bm{W})$:
\begin{align}
	\bm{W}^{*} = \arg\min_{\bm{W}} J(\bm{W}) =  \arg\max_{\bm{W}} \frac{|\bm{W}^{T}\bm{S}_{b}\bm{W}|}{|\bm{W}^{T}\bm{S}_{w}\bm{W}|}
\end{align}
It has been shown that the optimal projection matrix $\bm{W}^{*}$ is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of the generalized eigenvalue problem 
\begin{align}
	\bm{S}_{b}\bm{w}=\lambda \bm{S}_{w}\bm{w} \Rightarrow \bm{S}_{w}^{-1}\bm{S}_{b}\bm{w} = \lambda\bm{w}
\end{align}
Since $\bm{S}_{b}$ is the sum of $K$ matrices of rank $\leq 1$, this implies that $\bm{S}_{b}$ will be of rank $(K-1)$ or less and only $(K-1)$ of the eigenvalues $\lambda$ will be non-zero.  
A low-dimensional coordinate representation $\bm{z}_{n} \in \mathbb{R}^{(K-1)}$ of sample $\bm{x}_{n} \in \mathbb{R}^{D}$ is given by the linear projection of $\bm{x}_{n}$ onto the hyper-plane parameterized by $\bm{W}^{*}$, $\bm{z}_{n} = \bm{W}^{*T}\bm{x}_{n}$  It should be noted that for LDA, the dimensionality of the latent space is not a free-parameter, but is always fixed at $d=(K-1)$, or one less than the number of classes present in the dataset.  Equivalently, LDA can be derived by maximum likelihood for normal class-conditional densities where the covariances for each class are assumed to be equivalent \citep{Murphy2012}.  For the special case of binary target classification, the LDA transformation will place every sample onto a single line in 1-dimension, and thus the LDA solution can be simplified:
\begin{align}
	\bm{w}^{*} = \arg\max_{\bm{w}} \frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}} = \bm{S}_{w}^{-1}(\hat{\bm{\mu}_{2}} - \hat{\bm{\mu}_{1}})
\end{align}  
where $\hat{\bm{\mu}_{1}}$ and $\hat{\bm{\mu}_{2}}$ are the empirical means for classes $1$ and $2$, respectively.  Figure \ref{fig:lda_example} demonstrates the differences between PCA and LDA.  While PCA projects data onto the axes exhibiting the maximal variation, LDA projects the data into a space which attempts to simultaneously enforce between-class separation and within-class compactness. 

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/lda"}
		\caption[PCA versus LDA]{Example of dimensionality reduction on a synthetic dataset using PCA (left) and LDA (right).  The dataset shown is comprised of two classes, red and blue, respectively.  As shown on the left, PCA projects the data onto the principal axis corresponding to the direction of maximum variance for the dataset.  The classes would not be separable in the one-dimensional projection space found by PCA.  As shown on the right, LDA projects the data onto the hyperplane which both maximizes between-class dissimilarity while minimizing within-class dissimilarity.  For this dataset, the data in the embedding space found by LDA would be linearly separable. }
		\label{fig:lda_example}
	\end{figure*}
\end{center}

Although LDA is the basis for large number of discriminative dimensionality reduction approaches, it does not guarantee class separation in the embedding space.  For example, LDA projects data into a space of at most $(K-1)$ dimensions, however, more features may be necessary for adequate class discrimination.  Additionally, LDA is a parametric method which assumes unimodal Gaussian likelihoods.  This implies that it may not be able to preserve complex data structure.  Finally, LDA will fail if the discriminatory information is contained in the variance of the data instead of the mean.  Despite these pitfalls, LDA  has been successfully applied to object detection and recognition tasks \citep{Wang2016OrthogonalLDA}  Many variations of LDA have been developed, such as Non-parametric LDA \citep{Fukunaga1983NonparametricLDA}, Orthonormal LDA \citep{Wang2016OrthogonalLDA}, Generalized LDA \citep{Baudat2000GeneralizedDiscriminantAnalysis} and Multilayer Perceptrons \citep{Webb1990MLPLDA}.  Additionally, LDA serves as the foundation for many of the Multiple Instance Learning dimensionality reduction approaches in the current literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity}.

\subsection{Nonlinear Manifold Learning}

Linear methods such as PCA and MDS are convenient for projecting out-of-sample test points into the embedding space.  However, they are unable to capture the structure of data that are sampled from nonlinear manifolds \citep{Kegl2008PrincipalManifoldsTextbook}. This section will discuss a variety of nonlinear dimensionality reduction and manifold learning approaches.  All methods reviewed assume the data is distributed along a $d$-dimensional sub-manifold $\mathcal{X}$ embedded in $\mathbb{R}^{D}$.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{"lit_review/manifold/nonlinear_manifold"}
		\caption[Example of a nonlinear manifold.]{Example of a nonlinear manifold.  This particular dataset is known as the "Swiss Roll", and it represents a two-dimensional manifold embedded in three dimensions.  Global, linear manifold learning approaches cannot appropriately unroll the manifold.}
		\label{fig:nonlinear_manifold}
	\end{figure*}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Kernels  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Kernelization} \label{sec:Kernels}
Although each of the manifold learning techniques previously discussed are inherently linear, nonlinear adaptations have been made.  One approach is to utilize kernel functions as means to provide nonlinearity in the embeddings. 

\paragraph{Kernels} A \textit{kernel function}, $\kappa(\bm{x},\bm{x}') \in \mathbb{R}$, is a real-valued function of two arguments, $\bm{x},\bm{x}' \in \mathcal{X}$, which maps vectors from the input feature space to a single value in $\mathbb{R}$.  The function is typically symmetric (i.e.  $\kappa(\bm{x},\bm{x}') =  \kappa(\bm{x}',\bm{x})$) and non-negative (i.e.  $\kappa(\bm{x},\bm{x}') \geq 0$), which implies that it can be interpreted as a measure of similarity \citep{Murphy2012}.  The notion of kernels is very useful in certain applications where data representation is not straightforward, such as representing text documents or molecular structures which can have variable length.  

A popular choice of kernel in manifold learning is the \textit{radial basis function} (RBF) kernel, defined as:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \exp \left( - \frac{|| \bm{x} - \bm{x}' ||^{2}}{\beta} \right)
\end{align}
\noindent
where $\beta$ is the bandwidth of the isotropic function.  Another popular kernel for text classification is \textit{cosine similarity}, defined by:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \frac{\bm{x}^{T}\bm{x}'}{||\bm{x}||_{2}||\bm{x}'||_{2}} 
\end{align}
\noindent
This kernel measures the cosine of the angle between vectors $\bm{x}$ and  $\bm{x}'$ after scaling them onto the unit hyper-sphere.  If  $\bm{x}$ and  $\bm{x}'$ are strictly positive vectors (counts in the bag-of-words model, for example), then the kernel provides values in $[0,1]$, where a value of $0$ means the feature vectors are orthogonal and, therefore, have no features in common, and a value of $1$ means the vectors are the same. 

Some of the nonlinear manifold learning methods in the literature require the kernel function to satisfy the requirement that the \textit{Gram matrix}
\begin{align}
	\bm{K} = 	
	\begin{bmatrix}
		\kappa(\bm{x}_{1},\bm{x}_{1}) &  \dots & \kappa(\bm{x}_{1},\bm{x}_{N}) \\
		 & \vdots  & \\
		\kappa(\bm{x}_{N},\bm{x}_{1}) & \dots & \kappa(\bm{x}_{N},\bm{x}_{N})
	\end{bmatrix}
\end{align}
\noindent
be positive definite for any set of inputs $\{\bm{x}_{n}\}_{n=1}^{N}$.  This type of kernel is called a \textit{Mercer kernel} or \textit{positive definite kernel}, and is required to induce a Reproducing Kernel Hilbert Space (RKHS).  The importance of the Mercer Kernel is the following result, known as \textit{Mercer's theorem}.  This theorem states that if the Gram matrix is positive definite, its eigenvector decomposition can be written as
\begin{align}
	\bm{K} = \bm{U}^{T}\bm{\Lambda}\bm{U}
\end{align}
As derived by \cite{Murphy2012} and \cite{Liu2010KernelAdaptiveFiltering}, it then follows that each entry of $\bm{K}$ can be computed as 
\begin{align}
	\kappa(\bm{x},\bm{x}') = \phi(\bm{x})^{T}\phi(\bm{x}') = <\phi(\bm{x}),\phi(\bm{x}')>
	\label{eq:kernel_inner_product}
\end{align}
meaning that the entries of the kernel matrix can be defined by the inner product of some feature vectors that are implicitly defined by the eigenvectors $\bm{U}$.  If the kernel is Mercer, then there exists a function $\phi$ which maps $\bm{x} \in \mathcal{X}$ to $\mathbb{R}^{D}$ such that Equation \ref{eq:kernel_inner_product} holds.  Additionally, $\phi$ depends on the eigenfunctions of $\kappa$, meaning that $D$ is a potentially infinite dimensional space.
Additionally, instead of representing feature vectors in terms of kernels $\phi(\bm{x}) = [\kappa(\bm{x},\bm{x}_{1}), \dots, \kappa(\bm{x},\bm{x}_{N})]$, algorithms can instead work with the input feature vectors $\bm{x}$ by replacing all inner products $<\bm{x},\bm{x}'>$ with a call to the kernel function $\kappa(\bm{x},\bm{x}')$.  This is called the \textit{kernel trick}, and it turns out that many algorithms can be kernelized in this way.

Kernel functions play an important role in the dimensionality reduction literature for both applying nonlinearity to inherently linear problems and in defining similarity measures for graph-based manifold learning methods.  Specifically, nonlinear adaptations of the inherently-linear PCA \citep{Scholkopf1999KPCA}, MDS \citep{Webb2002KMDS} and LDA \citep{Ghojogh2019KDATutorial} algorithms have been formulated. The kernelization of PCA is briefly described in the following. 

\subsubsection{Kernel PCA (KPCA)} \label{sec:KPCA}
Section \ref{sec:PCA} showed how PCA could be used to compute linear low-dimensional embeddings of data.  This process involved finding the eigenvectors of the empirical data covariance matrix $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}\hat{\bm{x}}_{n}\hat{\bm{x}}_{n}^{T} = \frac{1}{N}\hat{\bm{X}}^{T}\hat{\bm{X}}$, where $\hat{\bm{x}}_{n} = \bm{x}_{n} - \hat{\bm{\mu}}$ is the mean subtracted feature vector.  However, PCA can also be computed by finding the eigenvectors of the inner product matrix $\bm{X}\bm{X}^{T}$ \citep{Murphy2012,Wang2014KPCAReview}.  This interpretation allows the production of nonlinear embeddings by taking advantage of the kernel trick.  This approach is known as \textit{Kernel PCA} (KPCA) \citep{Scholkopf1999KPCA}.  Assuming the data is mapped to a new feature space in $\mathbb{R}^{M}$ by a nonlinear transformation $\phi(\bm{x})$, PCA could be performed in the new feature space.  However, this computation can be extremely costly and inefficient.  Instead, kernel methods can be used to simply the computation.  Following the derivation defined by \cite{Wang2014KPCAReview}, first assume that the data in the new feature space has zero mean:
\begin{align}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n}) = 0	
\end{align}

\noindent
The covariance matrix of the projected features is a $M \times M$ matrix 
\begin{align} 
	\label{eq:kernel_covariance}
	\bm{S}_{\phi} =  \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}
\end{align}
\noindent
The eigenvectors $\bm{V}$ and eigenvalues $\lambda$ for $\bm{S}_{\phi}$ satisfy
\begin{align} 
	\label{eq:kernel_eigenvectors}
	\bm{S}_{\phi}\bm{v}_{k} = \lambda_{k} \bm{v}_{k}, \quad \forall k = 1, \dots, M
\end{align}
\noindent
From Equations \ref{eq:kernel_covariance} and \ref{eq:kernel_eigenvectors}, we have 
\begin{align}
	\label{eq:kernel_equivalent_system}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}\bm{v}_{k} = \lambda_{k} \bm{v}_{k}
\end{align}
\noindent
which can be re-written as
\begin{align}
	\label{eq:kernel_eigenvectors_and_weights}
	\bm{v}_{k} = \sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n})
\end{align}
\noindent
Substituting Equation \ref{eq:kernel_eigenvectors_and_weights} into Equation \ref{eq:kernel_equivalent_system}, gives 
\begin{align}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}\sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n}) = \lambda_{k} \sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n})
\end{align}
\noindent
A $N \times N$ matrix $\bm{K}$ can be defined by 
\begin{align}
	\bm{K}_{mn} = \kappa(\bm{x}_{m},\bm{x}_{n}) = \phi(\bm{x}_{m})^{T}\phi(\bm{x}_{n})
\end{align}
\noindent
which simplifies the problem to 
\begin{align}
	\bm{K}^{2}\bm{\alpha}_{k} = N \lambda_{k}\bm{K} \bm{\alpha}_{k}
\end{align}
\noindent
where $\bm{\alpha}_{k}$ is a column vector with entries $\alpha_{k1}, \dots, \alpha_{kN}$.  Each $\bm{\alpha}_{k}$ can be found by solving the eigenvalue problem 
\begin{align}
	\bm{K}\bm{\alpha}_{k} = N \lambda_{k}\bm{\alpha}_{k}
\end{align}
\noindent
Then the projection of a test point $\bm{x}$ onto the $k^{th}$ principal component can be found by
\begin{align}
	z_{k}(\bm{x}) = \phi(\bm{x})^{T}\bm{v}_{k} = \sum_{n=1}^{N} \alpha_{kn}\kappa(\bm{x}_{n},\bm{x})
\end{align}
\noindent
This formulation assumes that the projected data has zero mean.  However, this is not generally the case and the mean cannot simply be subtracted in the projected space \citep{Murphy2012}.  Therefore, KPCA can be performed on the centered Gram matrix, defined by
\begin{align}
	\tilde{\bm{K}} = \bm{H}\bm{K}\bm{H}
\end{align}
\noindent
where
\begin{align}
	\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}
\end{align}
\noindent
is the $N \times N$ centering matrix.  Pseudo-code for KPCA is given in Algorithm \ref{alg:KPCA}.


\begin{algorithm}[h!]
	\caption{KPCA}
	\label{alg:KPCA}
	\begin{algorithmic}[1]
		\Require {Gram matrix of training data $\bm{K} \in \mathbb{R}^{N \times N}$, Gram matrix augmented with test data $\bm{K}_{*} \in \mathbb{R}^{N_{*} \times N}$, dimensionality of latent space $d$}
		\Ensure {Embedded data coordinates $\bm{Z} \in \mathbb{R}^{N \times d}$}
		\State {$\bm{H} \gets \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}$}   
		\State {$\tilde{\bm{K}} \gets \bm{H}\bm{K}\bm{H}$}   
		\State {$[\bm{U},\bm{\Lambda}] \gets \textsc{eig}(\tilde{\bm{K}})$}     
		\For{$n \in N$}
		\State{$\bm{v}_{n} \gets \bm{u}_{n}/\sqrt{\lambda_{n}}$}
		\EndFor
		\State {$\bm{H}_{*} \gets \frac{1}{N_{*}}\bm{e}_{*}\bm{e}^{T}$}
		\State {$\tilde{\bm{K}}_{*} \gets \bm{K}_{*} - \bm{H}_{*}\bm{K}_{*} - \bm{K}_{*}\bm{H}_{*} + \bm{H}_{*}\bm{K}_{*}\bm{H}_{*}$}  
		\State {$\bm{Z} \gets \tilde{\bm{K}}_{*}\bm{V}_{1:d}$}
	\end{algorithmic}
\end{algorithm}
\noindent

Whereas linear PCA is limited to $d<D$ components, KPCA can use up to $N$ components.  Using a nonlinear feature embedding function along with the kernel trick provides for an elegant solution for capturing global nonlinear data structure.  As can be seen in Algorithm \ref{alg:KPCA}, embedding out-of-sample test points is done by appending rows to the Gram matrix, where new entries are defined between the test points and training data points, but not between test points and other test points.  Embedded data coordinates are computed by multiplying the augmented Gram matrix with the top $d$ scaled eigenvectors of the training Gram matrix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Graphs  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Graph-based Methods}
Nonlinear manifold learning methods typically rely on the use of computational graphs.  These graphs represent data structure pooled from local neighborhoods of samples.  \textit{Spectral graph theory} focuses on constructing, analyzing and manipulating graphs.  It has proved useful for object representation, graph visualization, spectral clustering, dimensionality reduction and numerous other applications in chemistry, physics, signal processing and computer science \citep{Shuman2013SignalProcessingGraphs, Bengoetxea2002ThesisGraphMatching}.   An overview of computational graphs as well as prominent methods for graph construction in manifold learning are presented. Additionally, geodesic distance approximation from pairwise distances is reviewed.  It should be noted that the work by \cite{Yan2007GeneralGraphEmbeddingFramework} shows how each of the undermentioned graph-based manifold learning approaches can be succinctly described under a general graph-based framework for dimensionality reduction.  It is encouraged that readers turn to that work for additional information on the mathematical relationships between the algorithms, as well as how linearization, kernelization and tensorization are applied in the graph-based setting.

\paragraph{Terminology}
Many dimensionality reduction methods in the literature are interested in analyzing relationships between samples defined on an undirected, weighted graph $G = \{ \mathcal{V}, \mathcal{E}, \bm{W} \}$, which consists of a finite set of \textit{vertices}  $\mathcal{V}$ (also called \textit{nodes} or \textit{points}) with cardinality $\mathcal{V}=N$, a set of \textit{edges} $\mathcal{E} \subset \mathcal{V} \times \mathcal{V} = [\mathcal{V}]^{2}$ (also known as \textit{arcs} or \textit{lines}) and a weighted \textit{adjacency} or \textit{affinity} matrix $\bm{W}$ \citep{Shuman2013SignalProcessingGraphs, Livi2013GraphMatchingProblem, Bengoetxea2002ThesisGraphMatching}.  The the size or \textit{order} of a graph is defined by the number of nodes $|\mathcal{V}|$ and edges $|\mathcal{E}|$.  If two vertices in $G$, say $\bm{u},\bm{v} \in \mathcal{V}$, are connected by an edge $e \in \mathcal{E}$, this is denoted by $e=(\bm{u},\bm{v})$ and the two vertices are said to be \textit{adjacent} or \textit{neighbors}.  When edges do not have a direction, they are coined as undirected.  A graph solely containing this type of connection is termed as an \textit{undirected graph}.  When all edges have directions, meaning $(\bm{u},\bm{v})$ and $(\bm{v},\bm{u})$ are distinguishable, the graph is said to be \textit{directed}.  In the literature, the term \textit{arc} is typically used to denote connections between nodes in directed graphs, while \textit{edge} is used when they are undirected.  The graph-based methods included in this literature review focus on analyzing affinities between data samples in undirected graphs.  Moreover, a \textit{path} between any two nodes in $\bm{u},\bm{u'} \in \mathcal{V}$ is a non-empty sequence of $k$ different vertices $<\bm{v}_{0}, \bm{v}_{1}, \dots, \bm{v}_{k}>$ where $\bm{u}=\bm{v}_{0},\bm{u}'=\bm{v}_{k}$ and $(\bm{v}_{i-1},\bm{v}_{i})\in \mathcal{E}$, $i=1,2,\dots,k$.  Additionally, a graph is said to be \textit{acyclic} if there are no cycles between its edges, regardless of whether it is directed or undirected.  

When using graphs for dimensionality reduction, vertices usually represent features of individual samples, and edges express relationships between them.  The most straight-forward way to construct a graph is to instantiate edges between every vertex in the graph, where each edge is weighted by the distance between the vertices it connects according to a pre-defined metric.  This type of graph is called \textit{full mesh}. Weights on edges are captured in the graph adjacency matrix $\bm{W}$.  When weights are not naturally defined by an application, a common way to the define the weight of an edge connecting vertices $\bm{u} \sim \bm{u}'$ is by a symmetric affinity function $W_{\bm{u},\bm{u}'} = K(\bm{u};\bm{u}')$; typically a \textit{radial basis function (RBF)} or \textit{heat kernel}, defined as:
\begin{align}
\bm{W}_{\bm{u},\bm{u}'}= w_{\bm{u},\bm{u}'} = \exp \left ( - \frac{|| \bm{u} - \bm{u}' ||^{2}}{\beta}  \right )
\end{align}
\noindent
where $\beta$ is the non-negative \textit{bandwidth} of the kernel.  Vertices will have a nonzero weight only if they fall within the nonzero mapping domain of the kernel. Additionally, a threshold could be set to truncate the weights of neighbors far from individual samples.


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{"lit_review/manifold/graphs"}
		\caption[Examples of graphs.]{Examples of $k$-nearest neighbor and $\epsilon$ graphs.  The leftmost picture shows two-dimensional, synthetic data.  The center image demonstrates an example of a $k$-nearest neighbor graph with $k=3$.  As can be observed, the instances colored green, orange, and blue, respectively, are each connected only to their three closest neighbors (as denoted by corresponding colored edges) in the data space.  The rightmost image shows an example of an $\epsilon$-ball or $\epsilon$-neighborhood graph.  Edges only exist between samples if they fall within an $\epsilon$ radius (denoted by the red circle) from the query instance.}
		\label{fig:examples_of_graphs}
	\end{figure*}
\end{center}


\paragraph{$\bm{K}$-Nearest Neighbor Graph}
In a $K$-nearest neighbor graph, every data point (vertex) $\bm{x}_n \in \bm{X}$ is connected by edges to its $K$-nearest neighbors, where $K \in \mathbb{Z}^{+}$ is fixed. An example of a $K$-nearest neighbor graph is depicted in a of Figure \ref{fig:examples_of_graphs}. The downside of this graph is that it might impose edges between neighbors that should not actually be connected, as in the case where a sample is metrically distant from all of its nearest neighbors. Although, this feature may actually be useful in domains such as outlier detection, where low adjacency weights indicate that the sample is far form the sampling distribution.  Two alternative $K$-nearest neighbor graphs, a  symmetric and mutual neighbors, might instead by utilized.  In the symmetric $K$-nearest neighbors graph, two vertices $\bm{u}$ and $\bm{u}'$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{or} $\bm{u}'$ is among the neighbors of $\bm{u}$.  The mutual $K$-nearest neighbors graph, however, only connects vertices $(\bm{u},\bm{u}')$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{and} $\bm{u}'$ is among the $K$-nearest neighbors of $\bm{u}$.  The weights on each edge are provided as the similarity of the adjacent nodes.

\paragraph{$\bm{\epsilon}$-Neighborhood Graph}
Another method for graph construction is to use $\epsilon$-neighborhoods (or $\epsilon$-balls).  In this graph, two vertices $(\bm{u},\bm{u}')$ are connected by an edge if and only if the distance between them is equal to or smaller than some value $\epsilon$, $\mathcal{D}_{\mathcal{U}}(\bm{u},\bm{u}') \leq \epsilon$.  This idea is represented in b of Figure \ref{fig:examples_of_graphs}.  In both the $K$-nearest and $\epsilon$-neighborhood graphs, a parameter controlling the number of edges in the graph, $K$ or $\epsilon$, must be chosen.   These parameters are highly influential for graph construction and can thus greatly affect dimensionality reduction quality.  Contrary to the $K$-nearest neighbor graph, an $\epsilon$-neighborhood will not create connections between distant vertices.  However, when the data is sampled sparsely from a highly-curved manifold, the $\epsilon$-neighbor graph will not be able to appropriately capture the geometry \citep{Thorstensen2009ManifoldThesis}.

\paragraph{Geodesic Distance Approximation} \label{sec:geodesic_distance}
The ultimate goal of manifold learning is to uncover an underlying low-dimensional sub-manifold which is embedded in $\mathbb{R}^{D}$.  Many dimensionality reduction methods in the literature discover projections of data into a low-dimensional space which preserve topological ordering of the data \citep{Kegl2008PrincipalManifoldsTextbook}.  These processes require a notion of distance between samples.  \textit{Euclidean distance} is a popular metric which captures the straight-line disparity between two points. As shown in Figure \ref{fig:geodesic_distance}, however, samples that are actually distant on the manifold may appear deceptively close in the high-dimensional input feature space, as measured by Euclidean distance \citep{Tenenbaum2000Isomap}. \textit{Geodesic distance}, also called \textit{curvilinear} or \textit{shortest-path distance}, Figure \ref{fig:geodesic_distance}, on the other hand, follows the curvature of a manifold and may provide a better measure of dissimilarity between data samples.  Geodesic distance can be estimated by the shortest path through a graph constructed by assuming the distances between neighbors is locally Euclidean \citep{Sorzano2014DRReview}.  This can be conceptualized by a simple example.  The Earth is a sphere and naturally has curvature.  Two people standing in a room, however, would estimate the distance between themselves by a straight line.  Thus, in a very local region on the Earth, the measure of curvature would be negligible and the true distances between objects could be estimated with Euclidean distance. The same concept is true for manifolds where, if data is sampled densely enough, geodesic distance can be approximated by the shortest-path through a neighborhood graph where the dissimilarities between neighbors is assumed to be locally Euclidean.  Geodesic distance can be estimated efficiently by methods such as Dijskstra's or Floyd's shortest-path algorithms \citep{Tenenbaum2000Isomap}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/geodesic_03"}
		\caption[Demonstration of geodesic distance]{The left image shows the Euclidean distance between two points, A and B, from the blue and red classes, respectively.  A classifier using Euclidean distance would assume the two points should be assigned the same class label because they are (Euclidean-wise) close in the feature space.  However, the two points actually belong to two disparate classes, each at an opposite end of the manifold.  Instead, the image on the right shows an approximation of the geodesic distance between the samples.  The geodesic distance, which measures the distance through the manifold, would be a more accurate representation of dissimilarity for the two instances.}
		\label{fig:geodesic_distance}
	\end{figure*}
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ISOMAP  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Isomap} \label{sec:Isomap}
\paragraph{Traditional Isomap}
While MDS has proven to be successful in a variety of applications, it suffers from the fact that is solely aims to retain pairwise Euclidean distances and does not consider the distributions of neighboring samples.  This implies that MDS is not able to capture the geometry of high-dimensional data which lies on or near to a curved manifold, such as the Swiss roll dataset \citep{VanDerMaaten2009DRReview,Chao2019RecentAdvancesSupervisedDimRed}. Isometric Feature Mapping (Isomap) \citep{Tenenbaum2000Isomap} is a technique which resolves this problem by attempting to preserve pairwise geodesic distances between datapoints.  Isomap can be considered as a generalization of classical MDS in which the pairwise distance matrix is replaced by a matrix of pairwise geodesic distances approximated by distances in the graph \citep{Thorstensen2009ManifoldThesis}.  The classic, unsupervised algorithm consists of a few steps:

\begin{enumerate}
\item Given a set of input data $\bm{X} = \{\bm{x}_{n}\}^{N}_{n=1} \subset \mathbb{R}^{D}$, construct a sparse neighborhood graph (such as the $K$-nearest  or $\epsilon$-ball graphs discussed previously) where each edge is weighted by the Euclidean distance between the neighbors it connects:
\begin{align}
	\bm{W}_{mn} = w_{mn} = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} 
\end{align}
where $\bm{W}$ is the graph adjacency matrix.  

\item Next, the geodesic distances between all pairs of samples is computed by finding the shortest paths between the points through the graph.  This is commonly done with Dijkstra's or Floyd's shortest-path algorithms \citep{Tenenbaum2000Isomap}.  

\item These geodesic distances form a pairwise distance matrix which is substituted into classical MDS as described in Section \ref{sec:MDS}.  This provides the low-dimensional embedding coordinates $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}] \in \mathbb{R}^{N \times d}$ of high-dimensional input data  $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}] \in \mathbb{R}^{N \times D}$, where $d \ll D$.

\end{enumerate}

While Isomap has been successfully applied in the areas of financial analysis \citep{Ribeiro2008SupervisedIsomap}, facial and object recognition \citep{Zhang2018IsomapMultiManifold}, visualization and classification tasks \citep{Vlachos2002NonlinearDRClassification}, a few important weaknesses are prevalent.  First, Isomap may be topologically unstable.  That is, it may construct erroneous connections in the neighborhood graph.  This is known as short-circuiting, and it can severely impair the performance  of Isomap.  Several approaches have been proposed to nullify the short-circuiting problem, such as removing datapoints with large total flows or by removing nearest neighbors that violate local linearity of the neighborhood graph \citep{VanDerMaaten2009DRReview}.  Another weakness of Isomap is that it may not perform correctly if there are holes  in the manifold, as this causes the geodesic distances of some samples to appear further on the manifold than  they truly are.  A third weakness is that Isomap can fail if the manifold is non-convex.  Therefore, we see that Isomap can perform very well due to theoretical guarantees on qualities such as convergence, as long as the manifold  is isometric to a convex open set of $\mathbb{R}^{d}$, $\mathcal{D}_{\mathcal{X}}(\bm{u},\bm{u}') = \mathcal{D}_{\mathcal{Y}}(f(\bm{u}),f(\bm{u}')) $, meaning that the geodesic distances in the graph are almost equal to the Euclidean distances in the embedding space $\mathbb{R}^{d}$.  Continuing, an additional drawback of Isomap is the fact that it requires the decomposition of a large, dense Gram matrix which scales with the number of training data points.  If the dataset grows too large, a solution will no longer be tractable.  Furthermore, the constraint on $\mathcal{X}$ to be isometric to a convex open set of $\mathbb{R}^{d}$ is rarely met.  As mentioned by \cite{Thorstensen2009ManifoldThesis}, these problems may be circumvented by sparsifying  large datasets using landmarks, as with Landmark Isomap \citep{deSilva2002IsomapReview} and looking at conformal maps, as is done in Conformal Isomap \citep{deSilva2002ConformalIsomap}. Finally, as with most nonlinear manifold learning techniques, it is nontrivial to embed out-of-sample data points into the lower dimensional feature space.  

\paragraph{Supervised Isomap Approaches}
As with most traditional manifold learning methods in the literature, Isomap is not inherently well-suited for classification tasks.  However, supervised approaches which consider class label information have been adopted to increase class separability in the latent embedding space. The work by \cite{Vlachos2002NonlinearDRClassification} was the first to investigate a supervised adaptation of Isomap.  Two supervised Isomap procedures were proposed which combine Isomap with a nearest neighbor classifier. These methods, Iso+Ada and WeightedIso take label information into consideration to scale the computed Euclidean distances utilized by Isomap by a constant factor according to class label. The idea is to make points closer in the embedding space if they have the same class label and farther if they have opposing class labels.  \cite{Ribeiro2008SupervisedIsomap} proposed an enhanced supervised Isomap (ES-Isomap) in which the dissimilarity matrix is weighted according to rules which consider class label information.  The dissimilarity matrix (considered as the adjacency matrix), $\bm{W}$, which was the same used in the Supervised Isomap method \citep{Geng2005SupNonlinearDimRed}, is defined as:
\begin{align}
	\bm{W}(\bm{x}_{m},\bm{x}_{n}) = \begin{cases} \sqrt{1 - \exp{\frac{-\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}}, \quad l_m = l_n \\  \sqrt{ \exp{\frac{\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}} - \alpha, \quad l_m \neq l_n\end{cases}
	\label{eq:supervised_isomap_dissimilarity}
\end{align}
\noindent
where $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ denotes the distance measure between samples $\bm{x}_{m}$ and $\bm{x}_{n}$, $\beta$ is used to prevent $\bm{W}(\bm{x}_{m},\bm{x}_{n})$ from increasing too quickly when $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ is large and is typically set according to the density of the data, $\alpha$ is a constant in $[0,1]$ which controls the dissimilarity between points in different classes and keeps the graph from becoming disconnected, and $l_{m}$ and $l_{n}$ are the corresponding class labels of samples  $\bm{x}_{m}$ and $\bm{x}_{n}$, respectively.  In Equation \ref{eq:supervised_isomap_dissimilarity}, the dissimilarity between two points is greater than or equal to one if their class labels are different and less than one if the points have the same class label.  Therefore, the between-class dissimilarity will always be larger than the within-class, which is an important property for classification tasks.  Pseudo-code for SE-Isomap is given in Algorithm \ref{alg:SE_Isomap}.

\begin{algorithm}[h!]
	\caption{SE-Isomap}
	\label{alg:SE_Isomap}
	\begin{algorithmic}[1]
		\Require {Dataset $\bm{X} = \{\bm{x}_{1}, \dots, \bm{x}_{N}\} \in \mathbb{R}^{N \times D}$, $\{l_{1}, \dots, l_{N} \} \in \{-1, +1\}^{N}$, parameters $k, \beta,\alpha, d$}
		\Ensure {Low-dimensional data representations $\bm{Z} \in \mathbb{R}^{N \times d}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{1 - \exp{\frac{-||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}}$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{ \exp{\frac{||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}} - \alpha$}
		\Else
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets 0$}
		\EndIf
		\EndFor
		\State {$\bm{D} \gets $ squares of the shortest distances between all points using Dijkstra's or Floyd's algorithm on $\bm{W}$}
		\State {$\bm{Z} \gets $ $\textsc{MDS}(\bm{D})$}         
	\end{algorithmic}
\end{algorithm}

\cite{Li2006SupervisedIsomap} proposed Supervised Isomap with Explicit mapping (SE-Isomap).  SE-Isomap enforces discriminability on the matrix of geodesic distances, as compared to the Euclidean distance matrix used in the aforementioned approaches, to learn an explicit mapping to the low-dimensional embedding space.  Finally, \cite{Zhang2018IsomapMultiManifold} developed a semi-supervised Isomap to utilize both labeled and unlabeled data points in training.  This method aims at minimizing pairwise distances of within-class samples in the same manifold while maximizing the distances over different manifolds.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Sammon Mapping  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Sammon Mapping} \label{sec:sammon_mapping}
Classical scaling, a convex technique for multidimensional scaling, was introduced in Section \ref{sec:MDS}.  As discussed, a pitfall of MDS is that it focuses on retaining global pairwise distances as opposed to local distances, which are typically much more important for capturing the geometry of the data \citep{VanDerMaaten2009DRReview}.  Several MDS variants have been proposed to address this weakness.  A popular variant is \textit{Sammon Mapping} \citep{Sammon1966SammonMapping}.

The classical scaling cost function looks to minimize the difference between the pairwise distance matrices of input data and their corresponding low-dimensional representations.  This cost puts emphasis on retaining the global data structure.  Sammon mapping adapts the classical scaling cost function by weighting the contribution of each sample pair $(\bm{x}_{m},\bm{x}_{n})$ by the inverse of their pairwise distances in the high-dimensional input space $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$.  In this way, the objective becomes
\begin{align}
	J(\bm{X},\bm{Z}) = \frac{1}{\sum_{m,n}\mathcal{D}(\bm{x}_{m},\bm{x}_{n})}\sum_{m \neq n} \frac{(\mathcal{D}(\bm{x}_{m},\bm{x}_{n}) - \mathcal{D}(\bm{z}_{m},\bm{z}_{n}))^{2}}{\mathcal{D}(\bm{x}_{m},\bm{x}_{n})}
\end{align}
\noindent
where $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ denotes the Euclidean distance between high-dimensional input samples $\bm{x}_{m}$ and $\bm{x}_{n}$, $\mathcal{D}(\bm{z}_{m},\bm{z}_{n})$ is the Euclidean distance between low-dimensional data coordinates $\bm{z}_{m}$ and $\bm{z}_{n}$ and the constant in the front is added to simplify the gradient of the objective.  This cost function gives more weight to preserving distances between samples that are close in the input space.   Minimization of the Sammon objective function is typically performed with gradient descent or a pseudo-Newton method \citep{Sorzano2014DRReview}.

A weakness (and strength) of Sammon mapping is that it will give much more importance to retaining a very small distance, say $10^{-5}$, as compared to  $10^{-4}$.  Despite this, Sammon mapping has been successfully applied to visualization tasks and has reported on applications using gene data and geospatial information \citep{VanDerMaaten2009DRReview}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  MVU  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Maximum Variance Unfolding (MVU)} \label{sec:MVU}
Section \ref{sec:KPCA} described how Kernel PCA \citep{Scholkopf1999KPCA,Wang2014KPCAReview} allows PCA to be performed in a feature space defined by a kernel function $\kappa$.  However, the choice of kernel function is arbitrary and may not be optimal for learning the intrinsic structure of a set of data.  To resolve this issue, \textit{Maximum Variance Unfolding} (MVU, formerly known as \textit{Semidefinite Embedding})\citep{Weinberger2004MVU} attempts to learn an appropriate kernel matrix to be used in conjunction with KPCA.  This is done by defining a neighborhood graph over the data and retaining the pairwise distances through the embedding to a low-dimensional space, as done in Isomap (Section \ref{sec:Isomap}).  Unlike Isomap, however, MVU attempts to ``unfold" the data data manifold by maximizing the Euclidean distances between data points under the constraint that the local geometry of the data manifold is unperturbed \citep{VanDerMaaten2009DRReview}.  The optimization can be solved using semidefinite programming.  MVU begins by forming a $k$-nearest neighbor graph $G$.  It then tries to maximize the sum of Euclidean distances between all samples such that the distances inside $G$ are unchanged.  This equates to the to following maximization problem:
\begin{align}
	\max_{\bm{Z}} \sum_{m,n} ||\bm{z}_{m} - \bm{z}_{n} ||^{2} \quad s.t. \quad ||\bm{z}_{m} - \bm{z}_{n} ||^{2} = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} \quad \forall (m,n) \in G
\end{align}
\noindent
This objective can be solved through the optimization of a semidefinite programming problem (SDP) for the kernel matrix $\bm{K}$:

\begin{align}
	\begin{split}
		&\max_{\bm{K}} \quad Tr(\bm{K}) \quad s.t.\\
		&1. \quad \bm{K} \succ 0\\
		&2. \quad \sum_{m,n} \bm{K}_{m,n}=0\\
		&3. \quad \bm{K}_{mm} + \bm{K}_{nn} - 2\bm{K}_{mn} = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} \quad \forall (m,n) \in G\\
	\end{split}
\end{align}
\noindent
with $\bm{K}$ defined as the outer product matrix of the low-dimensional data coordinates $\bm{Z}$.  The solution of the SDP provides a kernel matrix which is used in Kernel PCA.  The low-dimensional embedding coordinates $\bm{Z}$ are found by eigendecomposition of the kernel, as described in Section \ref{sec:KPCA}. 


Similarly to Isomap, MVU may suffer from short-circuiting due to optimization constraints which impair successful manifold unfolding.  Despite this weakness, MVU has been applied successfully to applications on microarray data and sensor localization \citep{VanDerMaaten2009DRReview}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LLE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Locally Linear Embedding (LLE)} \label{sec:LLE}
\textit{Locally Linear Embedding} (LLE) was first introduced by \cite{Roweis2000LLE} and it is, along with Isomap, a foundational graph-based approach for nonlinear manifold learning.  Whereas Isomap attempts to preserve global pairwise distances, LLE attempts to preserve solely local properties of the data \citep{VanDerMaaten2009DRReview}.  Since LLE looks solely at local neighborhoods around samples, it is much less sensitive to short-circuiting than Isomap.  Furthermore, the preservation of local properties allows the algorithm to effectively embed non-convex manifolds.  Essentially, LLE looks to represent each data sample as a linear combination of its $k$-nearest neighbors.  This fits a hyperplane through every datapoint and it's nearest neighbors, thus assuming the intrinsic manifold is locally linear.  The local linearity assumption implies that the reconstruction weights $\bm{w}_{n}$ of high-dimensional sample $\bm{x}_{n}$ are invariant to transformations such as translation, rotation and scaling.  Therefore, it is assumed that the same reconstruction weights $\bm{w}_{n}$ that can be used to represent $\bm{x}_{n}$ in the high-dimensional space will also reconstruct it's low-dimensional representation $\bm{z}_{n}$ from its corresponding neighbors in the low-dimensional space \citep{Roweis2000LLE, Sorzano2014DRReview, Chao2019RecentAdvancesSupervisedDimRed}.  Therefore, low-dimensional data representations $\bm{Z}$ are found by minimizing the objective
\begin{align} \label{eq:LLE_objective}
	J(\bm{W},\bm{Z}) = \sum_{n=1}^{N} \left | \left | \bm{z}_{n} - \sum_{k=1}^{K}w_{nk}\bm{z}_{k} \right | \right |^{2} \quad s.t. \quad \frac{1}{N}\bm{Z}^{T}\bm{Z}=\bm{\mathbb{I}}_{d}
\end{align} 
\noindent
The constraint on the covariance of the embedded data representations is included to avoid the trivial solution $\bm{Z} = \bm{0}$.  Solving for the low-dimensional data representations is done in three steps \citep{Thorstensen2009ManifoldThesis}.  First, a $k$-nearest neighborhood graph $G$ is constructed from the high-dimensional data $\bm{X}$.  Next, the reconstruction weight vector $\bm{w}_{n}$ that best represents each point  $\bm{x}_{n}$ as a linear combination of its $k$-nearest neighbors is found by optimizing the problem
\begin{align} \label{eq:LLE_Weight_Equation}
	\min_{\bm{W}}\sum_{n=1}^{N} \left | \left | \bm{x}_{n} - \sum_{k=1}^{K}w_{nk}\bm{x}_{k} \right | \right |^{2} \quad s.t. \quad \sum_{k=1}^{K}w_{nk} =1
\end{align}
\noindent
Optimization of Equation \ref{eq:LLE_Weight_Equation} can be solved directly by using the method of Lagrange Multipliers. Finally, the low-dimensional embedding coordinates are found by minimizing the quadratic error function for $\bm{Z}$ according to Equation \ref{eq:LLE_objective}.  \cite{Roweis2000LLE} showed that the coordinates that minimize the objective can be found as the eigenvectors corresponding to the $d$ smallest nonzero eigenvalues of the inner product $(\bm{\mathbb{I}}_{N}-\bm{W})^{T}(\bm{\mathbb{I}}_{N}-\bm{W})$ where $\bm{W}$ is a $N \times N$ sparse matrix whose entries are equal to the corresponding reconstruction weight if $\bm{x}_{m}$ and $\bm{x}_{n}$ are connected in the neighborhood graph and $0$ otherwise.

The popularity of LLE has led to the development of linear variants, namely \textit{Neighborhood Preserving Projections} (NPP) \citep{Pang2005NPP} and \textit{Orthogonal Neighborhood Preserving Projections} \citep{Kokiopoulou2007OrthoNPP}, and has been applied successfully to applications in super-resolution and sound localization \citep{VanDerMaaten2009DRReview}.  However, LLE tends to collapse large portions of the data close to each other in the embedding space due to the constraint on the covariance matrix.  This may also result in undesired scalings of the manifold.  Despite these effects, supervised approaches based on LLE have also been developed \citep{Chao2019RecentAdvancesSupervisedDimRed,Li2009SupManifoldLearning}.  Existing approaches can be summarized by the LDA idea, that points in the same class should be embedded more closely to each other while points in opposing classes should be well-separated in the low-dimensional space.  This notion is realized by altering the dissimilarity matrix used to construct the neighborhood graph.     


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Laplacian Eigenmaps (LE)} \label{sec:Laplacian_Eigenmaps}
\paragraph{Classical LE}
Similar to LLE, Laplacian Eigenmaps \citep{Belkin2003LaplacianEigenmaps}, or \textit{Spectral Embedding}, is a nonlinear dimensionality reduction technique which aims to preserve local structure of data \citep{Raducanu2012SupervisedNonlinearDimReduction,VanDerMaaten2009DRReview}.  Using \textit{spectral graph theory}, LE computes low-dimensional representations of data in which the dissimilarities between datapoints and their neighbors (according to an affinity measure) are minimized.  The name \textit{Laplacian Eigenmaps} is derived by the use of Laplacian regularization in the optimization procedure \citep{Thorstensen2009ManifoldThesis}. Given a set of $N$ samples $\bm{X} = \{\bm{x}_n\}^{N}_{n=1} \subset \mathbb{R}^{D}$, the first step of LE is to define a \textit{neighborhood graph on the samples}.  This graph, also called an \textit{affinity} or \textit{adjacency} matrix can be constructed in a variety of ways, such as $K$-nearest neighbor, $\epsilon$-ball, full mesh, or by weighting each edge $\bm{x}_m \sim \bm{x}_n$ by a symmetric affinity function $W_{mn} = K(\bm{x}_m;\bm{x}_n)$, typically a radial basis or heat kernel:
\begin{align}
	\bm{W}_{mn}= w_{mn} = \exp \left ( - \frac{|| \bm{x}_m - \bm{x}_n ||^{2}}{\beta}  \right )
\end{align}

\noindent
where the kernel bandwidth $\beta$ is typically set as the variance of the dataset \citep{Raducanu2012SupervisedNonlinearDimReduction,Thorstensen2009ManifoldThesis}.

The goal is to uncover the latent data representations $\{ \bm{z}_n \}^{N}_{n=1} \subset \mathbb{R}^{d}$ where $d \ll D$ which minimizes the objective 
\begin{align}
	J(\bm{W},\bm{Z}) = \frac{1}{2} \sum_{m,n}^{}||\bm{z}_{m} - \bm{z}_{n} ||^{2}w_{mn} = tr(\bm{Z}^{T}\bm{L}\bm{Z})
\end{align}

\noindent
with $\bm{W}$ denoting the symmetric affinity matrix, $\bm{D}$ the diagonal weight matrix whose entries are the sum of the rows (or columns since $\bm{W}$ is symmetric) of $\bm{W}$ (i.e. $d_{mm} = \sum_{n}w_{mn}$, and is $0$ otherwise).  The graph Laplacian matrix is provided as $\bm{L} = \bm{D} - \bm{W}$.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is the $N \times d$ embedding matrix and $tr(.)$ denotes the trace of a matrix. The $n^{th}$ row of matrix $\bm{Z}$ provides the vector $\bm{z}_n$, which is the latent representation of sample  $\bm{x}_n$. This objective discourages projecting similar points in the input feature space to disparate regions of the embedding space by enforcing heavy penalization. 

The latent sample coordinates $\bm{Z}$ are found as the solution to the optimization problem:
\begin{align}
	\min_{\bm{Z}} tr(\bm{Z}^{T}\bm{L}\bm{Z}) \quad s.t. \quad \bm{Z}^{T}\bm{D}\bm{Z} = \bm{I}, \bm{Z}^{T}\bm{L}\bm{e} = \bm{0}
\end{align}

\noindent
where $\bm{I}$ is the identity matrix and $\bm{e} = (1, \dots, 1)^{T}$.  The first constraint eliminates the trivial solution $\bm{Z} = \bm{0}$ (scaling) and the second constraint avoids the trivial solution $\bm{Z} = \bm{e}$ (uniqueness). By applying the Lagrange multiplier method and using the fact that $\bm{L}\bm{e} = \bm{0}$, the low-dimensional data representations can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{L}\bm{v} = \lambda \bm{D} \bm{v} \label{eq:trad_le_eig}
\end{align}
\noindent
The column vectors $\bm{v}_{1}, \dots, \bm{v}_{N}$ are the solutions of Equation \ref{eq:trad_le_eig}, ordered to the corresponding eigenvalues, in ascending order, $\lambda_{1} = 0 \leq \lambda_{2} \leq \dots \leq \lambda_{N} $. The embedding of the input samples given by the matrix $\bm{Z}$, is obtained by concatenating the eigenvectors of the $d$ smallest non-zero eigenvalues.  $\bm{Z}$ is a $N \times d$ matrix, where $d < N$ is the dimensionality of the embedded space.  From observation, it is clear that the embedding dimensionality is limited by the number of samples $N$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Linear LE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Linear LE (LPP)}
Because of the representation ability of LE, a linear approach called \textit{Locality Preserving Projections} (LPP) was proposed by \cite{He2003LPP}.  Similarly to LE, LPP builds a neighborhood graph which incorporates local information of the data.  LPP optimizes a similar objective to LE, however, it is assumed that the low-dimensional data representations come from a linear transformation of the high-dimensional input data.  The primary benefit of LPP over LE is that, while it shares many of the representation properties of LE, it is defined everywhere in the latent space as opposed to just over the training data points.  This allows for simple embeddings of out-of-sample test points.  Moreover, one can perform LPP in the original space or in a reproducing kernel Hilbert space through the use of Mercer kernels as described in Section \ref{sec:Kernels}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Supervised LE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Supervised LE (S-LE)}
In order to adopt LE for classification, \cite{Raducanu2012SupervisedNonlinearDimReduction} proposed a supervised LE which minimizes the margin between samples with similar class labels and maximizes the margin between samples with opposing class labels.  Supervised LE utilizes discriminative information contained in the class labels when finding the nonlinear embedding (spectral projection). 

In order to discover both geometrical and discriminative manifold structure, supervised LE splits the global graph into two components: the within-class graph $G_{w}$ and the between-class graph $G_{b}$.  To define the margin, they define two subsets, $N_{w}(\bm{x}_{n})$ and $N_{b}(\bm{x}_{n})$ for each sample $\bm{x}_{n}$.  These two subsets contain the neighbors of $\bm{x}_{n}$ sharing the same label and having different labels, respectively, which have a similarity higher than the average.
\begin{align}
	N_{w}(\bm{x}_n) = \{\bm{x}_m |l_{m} = l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_within_neighbor}
\end{align}

\begin{align}
N_{b}(\bm{x}_n) = \{\bm{x}_m |l_{m} \neq l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_between_neighbor}
\end{align}

\noindent
where $AS(\bm{x}_{n}) = \frac{1}{N} \sum_{n=1}^{N} \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$ denotes the average similarity of the sample  $\bm{x}_{n}$ to the rest of the data.  From Equations \ref{eq:s_le_within_neighbor} and \ref{eq:s_le_between_neighbor} it is clear that the neighborhoods for each data sample are not necessarily the same size.  As a result, this function constructs the affinity graph according to both the local density and similarity between data samples in the input feature space.

With the two sets defined, the within-class and between-class weight matrices $\bm{W}_{w}$ and $\bm{W}_{b}$ are formed from the adjacency graphs $G_{w}$ and $G_{b}$, respectively.  These weight matrices are defined as:
\begin{align}
	W_{w,mn} =
	\begin{cases}
		\exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right), \text{ if } \bm{x}_{n} \in N_{w}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{w}(\bm{x}_{n})  \\
		0, \text{ otherwise}
	\end{cases}
\end{align}

\begin{align}
	W_{b,mn} =
	\begin{cases}
		1, \text{ if } \bm{x}_{n} \in N_{b}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{b}(\bm{x}_{n}) \\
	0, \text{ otherwise}
	\end{cases}
\end{align}
\noindent
and the global affinity matrix, $\bm{W}$, can be written as:
\begin{align}
	\bm{W} = \bm{W}_{w} + \bm{W}_{b}
\end{align}

In order to obtain the low-dimensional representations $\bm{z}_n$ of the input data $\bm{x}_{n}$, the following objective functions can be optimized for $\bm{Z}$:
\begin{align}
	\min \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{w,mn} = tr(\bm{Z}^{T}\bm{L}_{w}\bm{Z})
\end{align}
\begin{align}
\max \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{b,mn} = tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z})
\end{align}
\noindent
where $\bm{L}_{w} = \bm{D}_{w} - \bm{W}_{w}$ and $\bm{L}_{b} = \bm{D}_{b} - \bm{W}_{b}$ indicate the corresponding graph Laplacians of the within-class and between-class affinity graphs, respectively.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ contains the low-dimensional representations of the input samples in its rows.

By merging the two objective functions, the final optimization problem is formulated as:
\begin{align}
	\arg\max_{\bm{Z}} \left \{  \gamma tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z}) + (1- \gamma)tr(\bm{Z}^{T}\bm{W}_{w}\bm{Z}) \right \} \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}
The term $\gamma$ is a scalar value in $[0,1]$ which determines the trade-off between pulling similar samples toward each other in the latent space and pushing heterogeneous points away.  A value of $\gamma = 1$ forces the objective to solely focus on maximizing the margin between dissimilar points.  Alternatively, a value of $\gamma = 0$ priorities the objective on embedding homogeneous samples in close spatial proximity. By defining matrix $\bm{B} = \gamma \bm{L}_{b} + (1 - \gamma)\bm{W}_{w}$, the problem becomes:
\begin{align}
	\arg\max_{\bm{Z}} \left ( \bm{Z}^{T}\bm{B}\bm{Z}  \right ) \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}

The low-dimensional embedding matrix $\bm{Z}$ can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v} \label{eq:sup_le_eig}
\end{align}

The column vectors $\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{N}$ are the generalized eigenvectors of 
Equation \ref{eq:sup_le_eig} arranged by descending eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \dots \lambda_{d}$.  Then the $N \times d$ embedding matrix $\bm{Z} =  [ \bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is provided by concatenating the obtained eigenvectors $\bm{Z} = [\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{d}]$.  Pseudo-code for S-LE is provided in Algorithm \ref{alg:SLE}.

\begin{algorithm}[h!]
	\caption{S-LE}
	\label{alg:SLE}
	\begin{algorithmic}[1]
		\Require {Dataset $\bm{X} = \{\bm{x}_{1}, \dots, \bm{x}_{N}\} \in \mathbb{R}^{N \times D}$, $\{l_{1}, \dots, l_{N} \} \in \{-1, +1\}^{N}$, parameters $k, \beta,\gamma, d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}_{w,mn} \gets \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 1$}
		\Else
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\EndIf
		\EndFor
		\State {Compute graph Laplacians $\bm{L}_{w} \gets \bm{D}_{w} - \bm{W}_{w}$, $\bm{L}_{b} \gets \bm{D}_{b} - \bm{W}_{b}$} 
		\State {Define $\bm{B} \gets \gamma \bm{L}_{b} + (1-\gamma)\bm{W}_{w}$}  
		\State {Solve the Eigenvector problem $\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v}$}
		\State {Sort Eigenvectors $\bm{v}$ and Eigenvalues $\lambda$ in decreasing order}
		\State{$\bm{Z} \in \mathbb{R}^{N \times d} \gets $ top $d$ Eigenvectors }        
	\end{algorithmic}
\end{algorithm}

The primary difference between the classic LE and S-LE is that traditional LE solely attempts to preserve the spatial relationships between samples, and thus, does not consider label information when learning the embeddings.  Alternatively, S-LE aims at aiding discriminant analysis by collapsing the distance between samples with the same label that are in close spatial proximity and pushing away spatial neighbors with differing class labels.  This is done through the utilization of two affinity graphs: the within-class and between-class graphs.  As with most graph-based methods, LE results vary highly according to the choice of neighborhood size.  However, choosing the size of $K$ or $\epsilon$ in advance can be very difficult. S-LE does not require user-defined graph parameters, other than those associated with the chosen affinity measure.  Instead, graph edges are chosen according to an adaptive neighborhood for each sample.   Both methods, however, suffer from inherent difficulties associated with nonlinear manifold learning, namely, selecting the intrinsic embedding dimensionality and handling out-of-sample extensions.

Despite these nuances, LE (and its variants) have been successfully applied in nonlinear dimensionality reduction tasks for facial recognition, spectral clustering and object classification \citep{VanDerMaaten2009DRReview}.

Apart from S-LE, other methods have been explored to integrate label information into Laplacian Eigenmaps.  A review of supervised dimensionality reduction methods by \cite{Chao2019RecentAdvancesSupervisedDimRed} explains that author's have optimized the affinity matrix using label information after constructing from spatial proximity, proposed deep learning-based approaches to achieve supervised LE and integrated label information into the affinity matrix construction process.  

The special feature exhibited by all Laplacian Eigenmap methods is the use of laplacian regularization, which enforces properties such as smoothness and provides a level of resistance toward the influences of outliers.  This useful feature has been applied in a variety of supervised and semi-supervised tasks, such as hyperspectral and synthetic aperture radar remote sensing classification \citep{Ratle2010ManRegHSI, Ren2017ManRegSAR}, classification of synthetic data \citep{Tsang2007ManifoldRegularization}, zero-shot learning \citep{Meng2018ManRegZeroShot} and reinforcement learning \citep{Li2015ManRegReinforcementLearning}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Hessian LE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hessian LLE} \label{sec:Hessian_LLE}
\textit{Hessian LLE} (HLLE) \citep{Donoho2003HessianEigenmaps} is a variant of LLE that minimizes the curvature of the high-dimensional manifold when embedding it into the low-dimensional space, under the constraint that the intrinsic manifold is locally isometric \citep{VanDerMaaten2009DRReview,Thorstensen2009ManifoldThesis}.  This is achieved by the eigenanalysis of the local Hessian matrix $\mathcal{H}$, which measures the curvature of the manifold around every data point.  HLLE begins by identifying the $k$-nearest neighbors for each sample $\bm{x}_{n}$ using Euclidean distance (local linearity).  Because of the linearity assumption, a $d$-dimensional basis for the local tangent space of each sample can be found by PCA or singular value decomposition (SVD) on each sample's corresponding neighbors.   Then, a local Hessian $\bm{H}^{n}$ of the manifold is estimated at each point $\bm{x}_{n}$ in the local tangent space.  The data Hessian matrix is constructed from the individual estimators derived from the local tangent coordinates and is given as
\begin{align}
	\mathcal{H}_{mo} = \sum_{n}\sum_{r}((\bm{H}^{n})_{rm}(\bm{H}^{n})_{ro})
\end{align}
The symmetric matrix $\mathcal{H}$ gives an estimate of the curvature of the high-dimensional data manifold.  The low-dimensional embedding coordinates $\bm{Z}$ of the input data are given by the eigenvectors of the corresponding $d$ smallest eigenvalues of $\mathcal{H}$.

HLLE shares many of the characteristics of Laplacian Eigenmaps, except that it replaces the manifold Laplacian with the manifold Hessian.  Because of this, HLLE suffers from the same weaknesses as Laplacian Eigenmaps \citep{VanDerMaaten2009DRReview}.  Despite its weaknesses, HLLE has been applied successfully to sensor localization tasks \citep{Patwari2004HLLESensorNetworks}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LTSA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Local Tangent Space Alignment (LTSA)} \label{sec:LTSA}
\textit{Local Tangent Space Alignment} (LTSA) \citep{Zhang2002LTSA} shares similarities with HLLE in that it describes local properties of high-dimensional data using the local tangent space of each data point \citep{VanDerMaaten2009DRReview}.  The basic idea of LTSA is to use the tangent space in the neighborhood of a data point to represent the local geometry of the data, and then to align the local tangent spaces to construct a global coordinate system for the nonlinear manifold.  In this way, the algorithm consists of two steps: 1.) constructing the principal manifold that is tangential to each data point and 2.) finding the global coordinate system that describes the set of data samples in a low-dimensional space.

Just as with Hessian LLE, LTSA begins by computing $d$-dimensional bases for the local tangent spaces of each of the data points $\bm{x}_{n}$.  This can be done by applying PCA or SVD on the neighbors of each sample.  This provides a linear transformation matrix $\bm{U}_{n}$ from the neighborhood of $\bm{x}_{n}$ to the local tangent space $\Theta_{n}$.  It is assumed that there exists a linear mapping $\bm{M}_{n}$ from the local tangent space coordinates $\bm{\theta}_{n}$ to the low-dimensional representations $\bm{z}_{n}$.  The linear transformation matrix and low-dimensional data representations are found by the following minimization problem \citep{Sorzano2014DRReview}:
\begin{align}
	\min_{\bm{M}_{n},\bm{Z}_{n}} \sum_{n=1}^{N} ||\bm{Z}_{n}\bm{H} - \bm{M}_{n}\Theta_{n} ||^{2}_{F}
\end{align}
\noindent
where $\bm{H}$ is a centering matrix.  \cite{Zhang2002LTSA} showed that the low-dimensional embedding coordinates $\bm{Z}$ can be constructed by the eigenvectors corresponding to the $d$ smallest nonzero eigenvalues of an alignment matrix $\bm{B}$.  

Similar to other sparse spectral manifold learning techniques, the objective function of LTSA is vulnerable to the presence of the trivial solution \citep{VanDerMaaten2009DRReview}.  However, LTSA has been successfully applied to applications in facial recognition \citep{Zhang2007LTSAFaceRecognition} and microarray data \citep{VanDerMaaten2009DRReview}.  Moreover, supervised approaches for LTSA were proposed by \cite{Li2005SupervisedLTSA} and \cite{Ma2010SupervisedLTTSA}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Diffusion Maps  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Diffusion Maps} \label{sec:Diffusion_Maps}
The success of ``kernel eigenmap approaches" such as LLE, LE, HLLE and LTSA led to the development of a manifold learning method called \textit{Diffusion Maps} (DM) \citep{Coifman2006DiffusionMaps}.  Diffusion maps is a framework that originates from the field of dynamical systems and is based on defining a Markov random walk on the graph of the data.  By performing a random walk for a number of time-steps, a measure for the proximity of the data points is obtained \citep{VanDerMaaten2009DRReview}. Essentially, a DM embeds data into a lower-dimensional space such that the Euclidean distance between points is approximated by the diffusion distance in the original feature space.

In the diffusion maps framework, a graph of the data is first constructed which measures the connectivity between all points in the graph.  The \textit{connectivity} of two samples $\bm{x}_{m}$ and $\bm{x}_{n}$ is defined as the probability of transitioning from $\bm{x}_{m}$ to $\bm{x}_{n}$ in one step of the random walk \citep{Hajek2015RandomProcesses}.  The weights and edges in the graph are computed by a heat-kernel (commonly called \textit{radial-basis function} RBF), which provides a weight matrix $\bm{W}$ with entries
\begin{align}
	\bm{W}_{mn} = w_{mn} = \kappa(\bm{x}_{m},\bm{x}_{n}) = \exp \left( - \frac{|| \bm{x}_{m} - \bm{x}_{n} ||^{2}}{\beta} \right)
\end{align}
\noindent
where $\beta$ is the bandwidth of the isotropic function.  By picking an appropriate kernel width, this constructs a matrix which measures the similarity within a certain neighborhood around each data point.  Outside of the neighborhood, the function quickly goes to zero.  Since this matrix should define the probabilities of jumping between data points, a sum-to-one constraint is enforced such that the one-step transition probability matrix is defined as
\begin{align}
	\bm{P}_{mn}^{(1)} = p_{mn}^{(1)} = \frac{w_{mn}}{\sum_{o=1}^{N}w_{mo}}
\end{align}
\noindent
This matrix represents the probability of a transition from one data sample to another in a single time-step. Relating DM to Laplacian Eigenmaps, this matrix can also be interpreted as the normalized graph Laplacian matrix \citep{Thorstensen2009ManifoldThesis}. Taking advantage of the Markov assumption, the forward transition probability matrix for $t$ time-steps $\bm{P}^{(t)}$ is given by $(\bm{P}^{(1)})^{t}$ \citep{Hajek2015RandomProcesses} and a \textit{diffusion distance} can be defined as
\begin{align} \label{eq:Diffusion_Distance}
	D^{(t)}(\bm{x}_{m}, \bm{x}_{n}) = \sum_{o=1}^{N} || \bm{P}_{mo}^{(t)} - \bm{P}_{on}^{(t)}||^{2}
\end{align}

From Equation \ref{eq:Diffusion_Distance}, it can be observed that the diffusion distance is small if there are many high-probability paths of length $t$ between two samples.  By integrating over all paths through the graph, DM is more robust to short-circuiting and noise perturbation than other approaches, such as the geodesic distance utilized in the  Isomap algorithm \citep{VanDerMaaten2009DRReview, Delaporte2008DiffusionMaps, Tenenbaum2000Isomap}.  The objective of a \textit{diffusion map} is then to embed the high-dimensional data coordinates into a lower-dimensional space such that the diffusion distance in the input space becomes Euclidean distance in the embedding space.  It was shown by \cite{Coifman2006DiffusionMaps} and \cite{Delaporte2008DiffusionMaps} that the low-dimensional data representations $\bm{Z}$ that retain the diffusion distances in the Euclidean embedding space can be formed by the eigenvectors of the $d$ largest, nontrivial eigenvalues of the eigenproblem 
\begin{align}
	\bm{P}^{(t)}\bm{v} = \lambda \bm{v}
\end{align}
\noindent
Since the graph is fully-connected, the largest eigenvalue is trivial (i.e. $\lambda_{1} = 1$) and its corresponding eigenvector $\bm{v}_{1}$ is discarded.  Thus, $\bm{Z} \in \mathbb{R}^{N \times d}$ is given by 
\begin{align}
	\bm{Z} = \begin{bmatrix}
		\lambda_{2} \bm{v}_{2}, \lambda_{3} \bm{v}_{3}, \dots, \lambda_{d+1} \bm{v}_{d+1}
	\end{bmatrix}
\end{align}

Diffusion maps have been successfully applied to tasks in shape matching and gene expression analysis \citep{VanDerMaaten2009DRReview}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Principal Curves %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Principal Curves, Surfaces and Manifolds} \label{sec:Principal_Manifolds}
\textit{Principal Curves} (PCs) were first proposed in a PhD thesis by Hastie in 1984 and are simply defined as lines or surfaces that pass through the ``middle of a cloud representing a data set" \citep{Kegl2008PrincipalManifoldsTextbook,Gorban2007ElasticMaps}.  In terms of probability distributions, a principal curve satisfies the self-consistency property, which implies that any point on the curve is the average of all data points projected onto it.  In fact, Principal Component Analysis (Section \ref{sec:PCA}) is a special case of PCs where the ``middle structure" of data is assumed as a straight line (or hyper-plane) instead of a curve.  Assuming data with zero mean, PCA looks for a hyper-plane passing through the origin with direction $\bm{w}_{1}$ with a linear mapping function $f(\bm{z}_{n}) = \hat{\bm{x}}_{n} = \bm{w}^{T}_{1}\bm{z}_{n}$ best approximates the data in the mean-square sense \citep{Sorzano2014DRReview, Murphy2012}.  The point $f(\bm{z}_{n})$  is the orthogonal projection onto the line parameterized by $\bm{w}_{1}$.  A new sample can be constructed as $\bm{x}^{'}_{n} = \bm{x}_{n} - f(\bm{z}_{n})$.  The previous procedure is repeated $d-1$ more times to discover the $d$ ``most important" lines for representing the entirety of the dataset.  Dimensionality reduction is achieved by replacing the data $\bm{x}_{n}$ with the corresponding collection of parameters $\bm{z}_{n}$ needed to project the sample onto its principal hyper-planes.  As noted by \cite{Sorzano2014DRReview}, the objective is simply a linear regression on the input data.  However, this process does not have to be restricted to lines (or hyper-planes).  In general, it can be assumed that a given sample $\bm{x}_{n}$ may be approximated by $\bm{x}_{n} = f(\bm{z}_{n}) + \bm{\epsilon}$ where $f(\bm{z}_{n})$ is an arbitrary (but sufficient) nonlinear mapping, and $\bm{\epsilon}$ is a term that captures the approximation error.   One could substitute for a fixed family of curves (parabolic, hyperbolic, exponential, polynomial, etc.).  This is exactly a nonlinear regressor, and several methods based on artificial neural networks have been proposed: Nonlinear PCA \citep{Kramer1991NonlinearPCA}, autoencoder networks \citep{Goodfellow2016DeepLearning} and self-organizing networks \citep{Kohonen1990SOM,Fritzke1995GrowingNeuralGas}.  This section will review popular methods for principal manifold estimation, including approaches based on deep-learning, self-organizing maps, neural gases and elastic maps. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Deep Learning  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Deep Learning} \label{sec:Deep_Learning}
Many natural phenomena behave in a nonlinear way, meaning that the observed data define a curved subspace in the original feature space \citep{Scholz2008NonlinearPCA}.  One method used to model this nonlinearity is to utilize a \textit{deep feedforward network}, also often called \textit{feedforward neural network} or \textit{multilayer perceptron} (MLP) \citep{Goodfellow2016DeepLearning}.  The goal of a feedforward network is to approximate a function $f$ which maps an input $\bm{x}$ to a desired output $\hat{\bm{y}}=f(\bm{x}, \bm{\theta})$.  These models are called ``networks" because they are typically represented as a composition of functions (usually perceptrons). In the standard perceptron model, the output is formed as a weighted combination of the inputs which is then passed through a nonlinear activation function.  Weights on the input signals are adjusted based on an error signal back-propagated through the network.  This model is similar to human neurons, where some input pathways (synapses) are strengthened for particular tasks.  The multilayer perceptron has been proven as a \textit{universal approximator} \citep{Haykin2009NeuralNetworks, Principe1999NeuralAdaptiveSystems}.  This means that (under certain conditions), neural models have the ability to approximate any continuous function.  Thus, feedforward neural networks have shown remarkable performance in countless applications \citep{Liu2017DeepLearningSurvey, Dhillon2019CNNSurvey, Abiodun2018DeepLearningSurvey, Shadid2019DeepLearningHealthCare, Alom2019DeepLearningSurvey, Tschannen2018RecentAdvancesAutoencoder, Yuan2019AutoencoderSurvey, Chen2019DeepAutoencoders}, including manifold learning through use of a special type of neural architecture called an \textit{autoencoder}.

\paragraph{Autoencoders and Nonlinear PCA} \label{sec:Autoencoders}

\textit{Autoencoder neural networks} (AE) fall into the category of neural architectures known as \textit{autoassociative networks} \citep{Rojas1996AssociativeNetworks}.  The goal of an autoencoder is, given an input sample, to produce an exact copy at its output.  Internally, the network has a hidden layer $\bm{h}$ that describes the latent code used to represent the input \citep{Goodfellow2016DeepLearning}.  Essentially, the network has two parts.  The first part represents the \textit{encoder} $\Phi_{encode}:\mathcal{X} \rightarrow \mathcal{Z}$, which maps the data sample coordinates $\bm{x} \in \mathbb{R}^{D}$ to the corresponding coordinates $\bm{z}$ in the $d$-dimensional subspace controlled by the architecture of the middle layer (or \textit{bottleneck}).  The second part of the network denotes the inverse function, or \textit{decoder} $\Phi_{decode}:\mathcal{Z} \rightarrow \hat{\mathcal{X}}$.   The job of the decoder is to learn an inverse mapping from $\bm{z}$ to the original data sample.  Thus, $\Phi_{decode}$ approximates the assumed data generation process.  In general,  the AE is constrained either through its architecture or through a sparsity constraint.  A loss function $\mathcal{L}(\bm{x},\hat{\bm{x}})$  measures how closely the AE can reconstruct the output.  A commonly used AE loss is MSE, which penalizes the estimated output from being different from the input, $\mathcal{L}(\bm{x},\hat{\bm{x}}) = ||\bm{x} -  \hat{\bm{x}}||^{2}$ \citep{Ball2017DLRemoteSensing}.  Figure \ref{fig:autoencoder} demonstrates an arbitrary feedforward autoencoder.  The first half of the network projects the input data into a lower-dimensional space which is controlled by the size of the bottleneck layer.  The decoder then attempts to reconstruct the input from the latent code.
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=1\textwidth]{"lit_review/manifold/autoencoder"}
		\caption[Autoencoder neural network]{Depiction of an autoencoder neural network.  The architecture consists of an encoder, a latent or bottleneck layer, and a decoder, which is the inverse transformation of the encoder.  The output of an autoencoder should be, as close as possible, a reproduction of the input.}
		\label{fig:autoencoder}
	\end{figure*}
\end{center}

Early investigation of modern autoencoders was performed by LeCun, Bourlard, Kamp, Hinton and Zemel beginning in 1986 \citep{Goodfellow2016DeepLearning, Bengio2014RepLearningReview} and showed that having hidden layers in both the encoder and decoder enables the network to learn a \textit{Nonlinear PCA} (NLPCA) \citep{Scholz2008NonlinearPCA}. Similarly, it has been shown that an autoencoder with only a single hidden layer will learn an equivalent transformation matrix to traditional PCA \citep{Bengio2014RepLearningReview}.

Autoencoders suffer from the same ailments as other neural networks, such as requiring large amounts of data to cope with extreme parameterization.  Moreover, vanilla AEs are unsupervised models and thus do not consider class information when determining the underlying latent features. So while AEs are capable of learning powerful feature representations, they may not be optimal for discrimination tasks.  Despite these drawbacks, autoencoders have been applied successfully in countless tasks \citep{Tschannen2018RecentAdvancesAutoencoder, Yuan2019AutoencoderSurvey, Chen2019DeepAutoencoders, Kosiorek2019StackedCapsuleAutoencoders}, including: feature representation learning, denoising, outlier detection, domain adaptation and anomaly detection in remote sensing \citep{Bengio2014RepLearningReview}.  Additionally, alternative autoencoder architectures have been developed, such as the variational AE (VAE) \citep{Ghaffarzadegan2018MILVAE, Dai2017VariationalAutoencoder} which assume the bottleneck layer to be the parameters of the latent code generating distributions and graph autoencoders (GAE) \citep{Wu2019SurveyGraphConvolutionalNeuralNetworks} which are capable of learning latent embeddings of graph-structured data.

\paragraph{Graph Convolutional Networks} \label{sec:Graph_CNN}
\textit{Geometric deep learning}, as described by \citep{Bronstein2017GeometricDeepLearning}, encapsulates emerging techniques which generalize deep neural models to non-Euclidean domains such as graphs and manifolds.  This umbrella of models includes \textit{graph autoencoders} (GAEs), which map graph nodes into a latent feature space and decode graph information from the latent node representations \citep{Wu2019SurveyGraphConvolutionalNeuralNetworks, Liu2018ConstrainedGraphVariationalAutoencoders, Pan2018AdversariallyRegGraphAutoencoder}.  Similarly to the standard autoencoder, GAEs can be used to learn lower-dimensional representations of the input data (graphs in this case) or to generate new data from latent representations.  The review by \cite{Wu2019SurveyGraphConvolutionalNeuralNetworks} provides a detailed summary of GAE models in the literature to date.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  SOM  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Self-Organizing Map (SOM)} \label{sec:SOM}

\textit{Self-Organizing Maps} (SOM) or \textit{Kohonen Networks} \citep{Kohonen1990SOM} are among the most well-known methods for discrete principal manifold estimation \citep{Yin2007SOM,Sorzano2014DRReview}.   SOMs belong to the category of neural networks which use a technique of  \textit{competitive learning} called \textit{self-organization} to learn spatially-organized intrinsic representations of features \citep{Rauber2002GHSOM}.  Self-organizing maps are initialized by a pre-defined set of neurons which are typically arranged in a 2 or 3-dimensional grid.  Each neuron is associated with a set of weights which represent their corresponding locations in the input space.   When presented an input pattern (or \textit{stimulus}), neurons compete among themselves for the representation of the input. Winning neurons strengthen their weights or relationships with this input using and adaption of \textit{Hebb's Rule}, which states that the change to a synaptic weight is proportional to the correlation between the input and its associated output \citep{Rumelhart1985CHL}.  As samples are introduced to the network during training, only a neighborhood of cells give an active response to the current input sample. These neurons are pulled toward the location of the input pattern according to how well they represent the input.  Given enough training iterations, the neurons in the lattice will disperse such that the spatial locations or coordinates of cells in the network correspond to different modes of the input distribution.  In this manner, Kohonen networks are able to utilize simple heuristics to form discrete topological mappings of the input space which represent the principal data manifold \citep{Yin2007SOM}. 

The self-organizing map is also a form of \textit{vector quantization} (VQ).  The purpose of VQ is to approximate a continuous probability density function $p(\bm{x})$ of input vectors $\bm{x}$ using a finite number of codebook vectors, $\bm{w}_{k}$, $k=1,2,\dots,K$.  After the ``codebook" is chosen, the approximation of $\bm{x}$ involves finding the reference vector, $\bm{w}_{c}$ closest to $\bm{x}$.  The ``winning" codebook vector for sample $\bm{x}$ satisfies the following:
\begin{align}
	|| \bm{x} - \bm{w}_c|| &= \min_{k}|| \bm{x} - \bm{w}_{k} ||
\end{align}	
\noindent
The algorithm operates by first initializing a spatial lattice of codebook elements (also called ``units"), where each unit's representative is in $\bm{w}_{k} \in \mathbb{R}^{D}$ where $D$ is the dimensionality of the input samples $\bm{x}$.  The training process proceeds as follows.  A random sample is selected and presented to the network at iteration $t$ and each unit determines its activation by computing dissimilarity (typically Euclidean distance).	 The unit who's codebook vector provides the smallest dissimilarity is referred to as the \textit{winner}.
\begin{align}
	c(t) = \argmin_{k} \mathcal{D}(\bm{x}(t),\bm{w}_{k}(t))
\end{align}
\noindent
Both the winning vector and all vectors within a neighborhood of the winner are updated toward the sample by 
\begin{align}
	\bm{w}_{k}(t+1) = \bm{w}_{k}(t) + \alpha(t) \cdot h_{ck}(t) \cdot [ \bm{x}(t) - \bm{w}_{k}(t) ] 
\end{align}
\noindent
where $\alpha(t)$ is a learning rate which decreases over time and $h_{ck}(t)$ is a neighborhood function which is typically unimodal and symmetric around the location of the winner which monotonically decreases with increasing distance from the winner.  A radial basis kernel is typically chosen for the neighborhood function as 
\begin{align}
	h_{ck}(t) = \exp{\left( -\frac{||\bm{w}_{c} - \bm{w}_{k} ||^{2}}{\beta(t)} \right)}
\end{align}
\noindent
where the top expression represents the Euclidean distance between units $c$ and $k$ with $\bm{w}_{k}$ representing the 2-D or 3-D location of unit $k$ in the lattice.  The neighborhood kernel's bandwidth is typically initialized to a value which covers a majority of the input space and decreases over time such that solely the winner is adapted toward the end of the training procedure. \\

The SOM essentially performs density estimation of high-dimensional data and represents it in a 2 or 3-D representation.  At test time, the dissimilarity between each unit in the map and an input sample are computed.  This dissimilarity can be used to effectively detect outliers, thus making the SOM a robust method which can provide confidence values for it's representation abilities of the principal manifold.  A downside of the SOM, however, is its lack of proven convergence criteria.  \\

Despite the lack of theoretical training guarantees, SOMs have been applied to thousands of applications \citep{Yin2007SOM}, which are too numerous to list here.  Examples, however, include tasks in: speech recognition, finance, computer network traffic analysis, manufacturing, image analysis \citep{Rauber2002GHSOM,Palomo2017GHNG}, robotics, control of diffusion processes, optimization problems, adaptive telecommunications, image compression, sentence understanding, radar classification of sea-ice \citep{Kohonen1990SOM}, cross-modal information processing, text and document mining, gene expression data analysis and discovery, novelty detection, computer animation, principal curve and surface discovery, data visualization \citep{Yin2007SOM}, hand-written word classification \citep{Chiang1997HandWrittenWords} and explosive hazard detection \citep{Frigui2009LandmineSOM}.  Moreover, numerous extensions to SOMs have been proposed, including adaptations which allow the lattice to grow and shrink to fit the input data space \citep{Rauber2002GHSOM}, an approach for space visualization \citep{Yin2007SOM} and a supervised approach known as \textit{Learning Vector Quantization} (LVQ) \citep{Kohonen1995LVQ}.\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  GNG  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Growing Neural Gas (GNG)} \label{sec:GNG}
Growing Neural Gas (GNG) Networks \citep{Fritzke1995GrowingNeuralGas} are similar to the standard SOM, except that network topology is not fixed \citep{Sorzano2014DRReview}.  This approach is called a ``neural gas" because the codebook vectors are allowed to move around the data space similar to the Brownian motion of gas molecules in a closed container \citep{Pena2007NeuralGasReview}.  The primary difference between GNG and SOM is that the network is allowed to shrink and grow to better fill the data space \citep{Palomo2017GHNG,Palomo2016GrowingNeuralForest}.  The dynamic topology is typically controlled by an edge age-based strategy.  Similarly to the SOM, growing neural gas networks train based on heuristics and have not been proven to converge to an optimal solution.  Additionally, constraints must be put into place to avoid over-fitting the data space.  Despite these nuances, GNGs have proven successful in a variety of tasks.  This is likely because the automatic topology learning exhibited by these networks allows them to learn complex manifolds which may have locally-different intrinsic dimensionality \citep{Kegl2008PrincipalManifoldsTextbook}.  Thus, GNG networks have been applied successfully to tasks in motion detection \citep{Sun2016GNGMotionDetection}, visualization of high-dimensional data, web mining, classification, image deblocking \citep{LopezRubio2011GHPGraphs}, computer vision, robotics, color quantization, clustering in videos \citep{Palomo2017GHNG} and foreground detection \citep{Palomo2016GrowingNeuralForest}.

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Elastic Maps  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Elastic Maps and Nets} \label{sec:Elastic_Maps}
\textit{Elastic Maps and Nets} \citep{Gorban2007ElasticMaps} (also called \textit{Principal Graphs}) are a mid-way between self-organizing maps (Section \ref{sec:SOM}) and the generative topographic mapping (Section \ref{sec:GTM}).   Essentially, elastic maps represent a mathematical analogy of a set of elastic springs embedded in the data space which is used to approximate a low-dimensional principal manifold.  Just as with the SOM, elastic maps are represented by a set of neurons (nodes), $\bm{w}_{k} \in \mathbb{R}^{D}$, $k=1,2,\dots,K$.  Each sample in the data set is assigned to a class based on its closest neuron 
\begin{align}
	C_{k} = \{\bm{x}|\bm{w}_{k} \text{ is the closest codebook vector to } \bm{x} \}
\end{align}
\noindent
The \textit{approximation energy} for the entire map measures the representation ability of each codebook vector to approximate the data assigned to it
\begin{align}
	U_{A} = \frac{1}{2}\sum_{k=1}^{K}\sum_{\bm{x}\in C_{k}} ||\bm{x} - \bm{w}_{k} ||^{2}
\end{align}
\noindent
This is analogous to the energy of the springs with unit elasticity which connect each data point to the codebook which is the most representative.  Additional structure is also enforced on the map to simulate stretching and bending.  Specifically, some pairs of nodes $(\bm{w}_{m}, \bm{w}_{n})$ are connected by \textit{elastic edges} and some triplets of neurons $(\bm{w}_{m}, \bm{w}_{n}, \bm{w}_{o})$ form \textit{bending ribs}.  The \textit{stretching energy} and \textit{bending energy} on these sets $E$ and $R$ are then defined by
\begin{align}
	U_{E} = \lambda \frac{1}{2} \sum_{(\bm{w}_{m}, \bm{w}_{n}) \in E} ||\bm{w}_{m} - \bm{w}_{n} ||^{2}
\end{align}
\begin{align}
	U_{R} = \mu \frac{1}{2} \sum_{(\bm{w}_{m}, \bm{w}_{n}, \bm{w}_{o}) \in R} ||\bm{w}_{m} - 2\bm{w}_{n} + \bm{w}_{o}||^{2}
\end{align}
\noindent
where $\lambda$ and $\mu$ are the stretching and bending moduli, respectively.  The total energy of the map is given as
\begin{align}
	U_{T} = U_{A} + U_{E} + U_{R}
\end{align}
\noindent
Essentially, the first term accounts for the fidelity of the data representations, the second term reinforces smoothness of the manifold by favoring similar neighbors and the third term imposes smoothness by stating that a codebook must be similar to the average of its neighboring nodes \citep{Sorzano2014DRReview}.  The position of the nodes $\{\bm{w}_{k}\}$ is given by the \textit{mechanical equilibrium} of the elastic map, or the locations which minimize the total energy $U_{T}$.  In practice, this is typically solved by the Expectation-Maximization algorithm, which guarantees a local minimum of $U_{T}$.

Unlike the SOM, but similarly to the GNG, elastic nets can add or delete neurons adaptively.  This allows them to fit very complex manifolds.  The elastic map approach has led to practical applications in data visualization, data recovery, visualization of genetic texts and recovery of geophysical time series \citep{Gorban2007ElasticMaps}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Latent Variable Models %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Probabilistic Latent Variable Models (LVM)} \label{sec:LVM}
The probabilistic interpretation of latent feature learning can be formulated as attempting to discover the generating distribution for the low-dimensional factors of variation (latent random variables) that describe a distribution over the observed, high-dimensional data \citep{Bengio2014RepLearningReview,Murphy2012}.
While many manifold learning and dimensionality reduction models are regarded as defining a projection from a $D$-dimensional feature space into a lower $d$-dimensional space, a latent variable model is defined by a mapping \textit{from} the latent space \textit{into} the high-dimensional data space \citep{Bishop1998GTM}.  The goal of latent variable models in terms of low-dimensional feature learning is to discover the parameters of an invertible mapping from the latent generating distribution to the high-dimensional data distribution which maximize the likelihood of the observed data.  

Probabilistic methods provide two possible modeling paradigms for inferring latent variables, directed and undirected graphical models, which differ in their parameterizations of the joint distribution $p(\bm{x}_{n},\bm{z}_{n})$.  The remainder of this section focuses on directed graphical models which follow a particular formulation called \textit{Factor Analysis}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  FA and PPCA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Factor Analysis (FA) and Probabilistic PCA (PPCA)} \label{sec:FA_and_PPCA}

\textit{Factor Analysis} (FA) is a special case of a directed latent variable model that assumes the distribution of high-dimensional random variables is induced by a generalized linear mapping of the latent random variables \citep{Murphy2012, Tipping1999PPCA, Rish2008SupDimRedGLM, Gonen2013BayesianSupDimRed}.  A common assumption is that the high-dimensional data are Gaussian distributed and the the conditional likelihood is given as
\begin{align} \label{eq:FA_likelihood}
	p(\bm{x}|\bm{z},\bm{\theta}) = \mathcal{N}(\bm{W}\bm{z} + \bm{\mu}, \bm{\Psi})
\end{align}
\noindent
where $\bm{W}$ is a $D \times d$ matrix known as the \textit{factor loading matrix} and $\bm{\Psi}$ is a $D \times D$ covariance matrix.  $\bm{\Psi}$ is assumed to be diagonal since the goal of of the model to to ``force" $\bm{z}$ to explain the correlation between the data instead of ``baking it in" to the covariance. $\bm{\theta}$ denotes the parameters of the model. Typically a prior is defined which captures any presumed knowledge about the distribution of $\bm{z}$.  The data likelihood conditioned solely on the parameters is obtained by integrating over the latent variables
\begin{align}
	p(\bm{x}|\bm{\theta}) = \int p(\bm{x}|\bm{z},\bm{\theta}) p(\bm{z}) d \bm{z}
\end{align}
\noindent
and the model parameters can be obtained by \textit{maximum likelihood}.


By assuming $\bm{\Psi} = \beta \mathbb{I}_{D}$, the factor analysis model provides a probabilistic formulation of PCA (PPCA) \citep{Tipping1999PPCA}.  A typical choice on this model is a Gaussian-Gaussian conjugate prior pair where the mean of the data likelihood is a linear function of the latent inputs.  As an example, the prior over the hidden data representations can be expressed as a Gaussian distribution
\begin{align}
p(\bm{z}_n) = \mathcal{N}(\bm{z}_n|\bm{\mu}_0, \bm{S}_0)
\end{align}
\noindent
and the data likelihood is provided as a multivariate Gaussian (given in Equation \ref{eq:FA_likelihood}).  Classical PCA assumes the data covariance to be $\bm{\Psi} = \sigma^2\bm{I}$ with $\beta \rightarrow 0$, thus the model is deterministic.  Alternatively, when $\beta > 0$,the projection is no longer orthogonal since it is shrunk toward the prior mean.  The trade-off is that the reconstructions of $\bm{x}_{n}$ will be closer to the data mean.


While the typical approach for fitting PCA is to use eigen-decomposition or Singular Value Decomposition (SVD), PPCA can be fit with the Expectation-Maximization (EM) algorithm (which may be more computationally efficient for high-dimensional data \citep{Murphy2012}).  Once the model parameters have been estimated, dimensionality reduction can be performed by applying the inverse mapping
\begin{align}
	\bm{z}_{n} = \bm{W}^{T}(\bm{x}_{n} - \bm{\mu})
\end{align} 
\noindent
The projections onto the discovered principal axes, however, may not be optimal in the mean-squared sense because they will not be orthogonal.

\cite{Murphy2012} showed examples on how supervision has been applied to PCA.  \textit{Supervised PCA} or \textit{Bayesian factor regression} is a model like PCA, except that the target variable (or label), $l_n$ is taken into account when learning the low-dimensional embedding.  For the case of binary classification, the Bayesian model can be decomposed into the following elements:
\begin{align}
	p(\bm{z}_n) = \mathcal{N}(0,\mathbb{I}_d)
\end{align}
\begin{align}
	p(l_n|\bm{z}_n) = \text{Ber}(\text{sigm}(\bm{w}^{T}_{l}\bm{z}_n))
\end{align}
\begin{align}
	p(\bm{x}_n|\bm{z}_n) = \mathcal{N}(\bm{W}_{x}\bm{z}_{n} + \bm{\mu}_{x}, \beta\mathbb{I}_D)
\end{align}
\noindent
where the distribution of a label given a latent sample is defined by a Bernoulli distribution.  By placing weighting terms on the components of the likelihood function, Supervised PCA can become discriminative.  

Moreover, Gaussian Processes (GPs) have been used to learn nonlinear mappings between the input and latent feature spaces \citep{VanDerMaaten2009DRReview, Lawrence2005PPCAGPLVModels}, effectively creating nonlinear PPCA.  A supervised GP latent factor model for dimensionality reduction was proposed by \cite{Gao2011SupGPLVMDimRed}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  GTM  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Generative Topographic Mapping (GTM)} \label{sec:GTM}
The \textit{Generative Topographic Mapping} (GTM) \citep{Bishop1998GTM} is a nonlinear latent variable model proposed as alternative to the self-organizing map.  Specifically, the GTM provides solutions to known deficiencies exhibited by SOMs, such as the lack of an objective function, proofs of convergence, theoretical basis for choosing a learning rate parameter and guarantee of topological ordering.  The GTM defines a nonlinear, parametric mapping from a $d$-dimensional latent space to a $D$-dimensional data space $\bm{x} \in \mathbb{R}^{D}$, where typically $d \ll D$ \citep{Pena2007NeuralGasReview}.  A continuous and differentiable function $f(\bm{x;\bm{W}})$ maps every point in the latent space to a point in the data space.  This function can be any arbitrary, parametric mapping such as a feedforward neural network.  To learn the parameters, GTM attempts to maximize the negative log-likelihood of the latent coordinates given the high-dimensional data and model parameters.  Assuming a probability distribution $p(\bm{z})$, the GTM derivation begins by assuming each high-dimensional sample $\bm{x}$ is generated from a nonlinear mapping with additive Gaussian noise
\begin{align}
	p(\bm{x}|\bm{z},\bm{W},\beta) = \left ( \frac{1}{2 \pi \beta} \right )^{D/2} \exp{\left ( -\frac{1}{2 \beta} ||f(\bm{z};\bm{W}) - \bm{x} ||^{2} \right )}
\end{align}
\noindent
where $\beta$ is the variance of the radially-symmetric Gaussian function centered on $f(\bm{z};\bm{W})$.  The density over the input data space is obtained by integrating over the latent space by
\begin{align}
		p(\bm{x}|\bm{W},\beta) = \int p(\bm{x}|\bm{z},\bm{W},\beta) p(\bm{z}) d \bm{z}
\end{align}
\noindent
which is inherently intractable.  By defining $p(\bm{z})$ as a set of $K$ equally weighted delta functions on a regular grid, 
\begin{align}
	p(\bm{z}) = \frac{1}{K} \sum_{k=1}^{K} \delta (\bm{z} - \bm{z}_{k})
\end{align}
\noindent
the integral turns into a summation
\begin{align}
		p(\bm{x}|\bm{W},\beta) = \frac{1}{K} \sum_{k=1}^{K} p(\bm{x} | \bm{z}_{k}, \bm{W}, \beta)
\end{align}
which represents the model as a constrained mixture of Gaussians.  The data log-likelihood is given by
\begin{align} \label{eq:GTM_likelihood}
	J(\bm{W}, \beta) = \sum_{n=1}^{N} \ln \left( \frac{1}{K} \sum_{k=1}^{K} p(\bm{x} | \bm{z}_{k}, \bm{W}, \beta) \right)
\end{align}
\noindent
Given a set of data $\bm{X} \in \mathbb{R}^{N \times D}$, the log-likelihood function in Equation \ref{eq:GTM_likelihood} can be maximized by the Expectation-Maximization algorithm.

There are two obvious limitations of the GTM.  First, the computational complexity grows exponentially with the assumed intrinsic dimensionality.  As noted by \cite{Pena2007NeuralGasReview}, however, this is typically not an issue if the goal is visualization.  Additionally, poor initialization can lead the algorithm into local optima. However, the GTM has been used extensively for visualization tasks, clustering and applications in molecular biology \citep{Bishop1998GTM, Pena2007NeuralGasReview, Sorzano2014DRReview, Kegl2008PrincipalManifoldsTextbook}.  Additionally, successful variations of the GTM have been developed, such as the Topographic Product of Experts, Harmonic Topographic Map, Topographic Neural Gas and Inverse-Weighted K-Means Topology-Preserving Map \citep{Pena2007NeuralGasReview}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Manifold Charting  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Manifold Charting} \label{sec:Manifold_Charting}
\textit{Charting} is the problem of assigning a low-dimensional coordinate system to data points in a high-dimensional sample space. \textit{Manifold Charting}, introduced by \cite{Brand2003ManifoldCharting}, constructs low-dimensional data representations by aligning mixtures of factor analyzers (MoFA) or mixtures of probabilistic PCA (MoPPCA) models.  Essentially, manifold charting minimizes a convex cost function measuring the amount of disagreement between the linear models on the global coordinates of the data. This is done in two steps: 1.) computing a mixture of locally linear models on the data and 2.) aligning the models in order to obtain the low-dimensional data representations.   Manifold charting begins by performing EM to learn a mixture of factor analyzers in order to obtain $M$ low-dimensional data representations $\bm{w}_{nm}$ and corresponding responsibilities $r_{nm}$ for each sample $\bm{x}_{n}$.  The responsibilities  $r_{nm}$ describe how much sample $\bm{x}_{n}$ belongs to model $m$, and satisfy $\sum_{m=1}^{M}r_{nm}=1$.  A linear mapping $\bm{M}$ is found from the local data representations $\bm{w}_{nm}$ to the global coordinates $\bm{z}_{n}$ that minimize the objective 
\begin{align}
	J(\bm{Z}) = \sum_{n=1}^{N}\sum_{m=1}^{M} r_{nm} ||\bm{z}_{n} - \bm{z}_{nm} ||^{2}
\end{align} 
\noindent
where $\bm{z}_{n} = \sum_{o=1}^{M}r_{no}\bm{z}_{nm}$ and $\bm{z}_{nm} = \bm{w}_{nm}\bm{M}$.  This objective function implies that all models for which a datapoint has a high responsibility should agree on the final coordinate for that datapoint.  As shown by \cite{VanDerMaaten2009DRReview}, the cost function can be reformulated as a generalized eigenvalue problem in which the $d$ smallest nonzero eigenvectors form the linear transformation matrix from the local data representations $\bm{W}$ to the globally-aligned, low-dimensional data representations $\bm{Z}$.

Manifold charting has been shown to be more robust to noise than alternative approaches such as Isomap and LLE \citep{Brand2003ManifoldCharting}, however, the main weakness of manifold charting is that fitting the MoFA is susceptible to the presence of local maxima in the log-likelihood function.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  t-SNE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{High-Dimensional Data Visualization} \label{sec:High_Dim_Data_Visualization}
While not directly related to classification, it is important for the reader to be aware of the current SOA in manifold learning.  At the time this document was written, much work was being performed in dimensionality reduction for the visualization of extremely large datasets consisting of hundreds (or thousands) of features.   Specifically, the remainder of this section discusses three related, SOA approaches for data visualization.  t-SNE, LargeVis and UMAP are compared in terms of their differences in use of manifold learning for the visualization of large, high-dimensional datasets.


\subsubsection{Stochastic Neighbor Embedding (SNE and t-SNE)} \label{sec:tSNE}
\textit{t-Distributed Stochastic Neighbor Embedding} \citep{VanDerMaaten2008tSNE} was proposed as an extension to \textit{Stochastic Neighbor Embedding} (SNE) \citep{Hinton2003SNE}. SNE is intended as a visualization tool for high dimensional datasets.  Essentially, SNE finds low-dimensional data coordinates by defining pairwise probabilities between all samples in the input data space.  It then assumes that the low-dimensional embedding coordinates should be distributed according to the same conditional probabilities.  SNE begins by defining conditional probabilities between all points in the training set which measure their likelihood of being spatial neighbors.  Given a datapoint $\bm{x}_{n}$, the probability of selecting another point $\bm{x}_{m}$ as its neighbor is given as
\begin{align} \label{eq:SNE_high_dim_data_probs}
	p_{m|n} = \frac{\exp(-||\bm{x}_{n} - \bm{x}_{m}  ||^{2}/2 \beta_{n})}{\sum_{o \neq p}\exp(-||\bm{x}_{p} - \bm{x}_{o}  ||^{2}/2 \beta_{n})}
\end{align}
\noindent
where $\beta_{n}$ is the variance of the RBF kernel centered on sample $\bm{x}_{n}$.  Therefore, points in a local neighborhood to sample $\bm{x}_{n}$ will have a higher probability of being selected as a neighbor.  Similarly, the pairwise conditional probabilities between all low-dimensional samples can be defined by
\begin{align}
	q_{m|n} = \frac{\exp(-||\bm{z}_{n} - \bm{z}_{m}  ||^{2})}{\sum_{o \neq p}\exp(-||\bm{z}_{p} - \bm{z}_{o}  ||^{2})}
\end{align}
\noindent
Since the similarity of a sample with itself is not important, $p_{n|n}$ and $q_{n|n}$ are set to zero.  The intuition behind this formulation is that, if the low-dimensional coordinates $\bm{z}_{n}$ and $\bm{z}_{m}$ correctly model the similarity between the high-dimensional data points $\bm{x}_{n}$ and $\bm{x}_{m}$, the conditional probabilities $p_{m|n}$ and $q_{m|n}$ will be equal \citep{VanDerMaaten2008tSNE}.  Thus, it is intuitive that the Kullback-Leibler (KL) divergence (which measures the cross-entropy between probability densities) would make an appropriate cost function between the high and low-dimensional probabilities.  The objective function of SNE is given by 
\begin{align}
	J = \sum_{n=1}^{N}KL(P_{n}||Q_{n})) = \sum_{n=1}^{N}\sum_{m=1}^{N} p_{m|n} \log \frac{p_{m|n}}{q_{m|n}}
\end{align} 
\noindent
where $P_{n}$ denotes the conditional probability distribution over all other high-dimensional datapoints given $\bm{x}_{n}$, and $Q_{n}$ represents the conditional probability distribution over all other low-dimensional embedding coordinates given $\bm{z}_{n}$.  Given that the KL divergence is asymmetric, it can be observed that using widely separated points in the embedding space to model points close in the input space accrues a large error.  However, the cost of using nearby points in the embedding space to represent points far in the data space is small.  Therefore, the SNE cost focuses on retaining the local structure of data in the embedding space.  This problem can be optimized using gradient descent.  

A problem with is that, due to the curse of dimensionality, points in ``middle distances" get squashed closely together in the low-dimensional embedding space.  This is known as the \textit{crowding problem}, and stems from the fact that in high-dimensional spaces, the surface of the a hypersphere (Guassian distribution) grows much more quickly with its radius as compared to a hypersphere in a low-dimensional space.  t-SNE handles this problem by forcing the optimization to ``spread-out" the medium distance points.  This is done by utilizing a symmetric KL-divergence and by giving the low-dimensional density a longer tail.  Using a Student t-distribution, the conditional neighbor probabilities of the low-dimensional data coordinates become
\begin{align} \label{eq:tSNE_low_dim_similarity}
	q_{m|n} = \frac{(1 + ||\bm{x}_{n} - \bm{x}_{m}  ||^{2})^{-1}}{\sum_{o \neq p}1 + ||\bm{x}_{p} - \bm{x}_{o}  ||^{2})^{-1}}
\end{align}
\noindent
and optimization of the KL-divergence is performed with gradient descent.

Whereas many manifold learning and dimensionality reduction methods in the literature can only capture global or local data structure, t-SNE is effectively able to capture both.  Thus, t-SNE has been applied successfully to 2D and 3D visualization of datasets consisting of millions of samples with hundreds of features \citep{Tang2016LargeVis, McInnes2018UMAP}.  However, t-SNE suffers from a few drawbacks. t-SNE is bottlenecked by the size of the dataset due to the construction of a $k$-NN graph.  Moreover, the efficiency of the graph visualization step deteriorates significantly as the data becomes large and the performance of t-SNE is highly dependent on its hyperparameters.  Also, since t-SNE directly optimizes the embeddings, it cannot be applied directly to new data.  t-SNE lead as the SOA data visualization tool for approximately ten years after its inception, but because of its deficiencies, LargeVis \citep{Tang2016LargeVis} and UMAP \citep{McInnes2018UMAP} were eventually proposed. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LargeVis  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{LargeVis} \label{sec:LargeVis}
\textit{LargeVis} was proposed by \cite{Tang2016LargeVis} for visualizing large-scale and high-dimensional data in a low-dimensional space (typically 2D or 3D).  Specifically, LargeVis addresses the scaling inefficiencies exhibited by t-SNE by adopting the ``a neighbor of my neighbor is my neighbor" approach for $k$-NN graph construction.  Essentially, the algorithm builds a few random projection trees to quickly construct a nearest neighbor graph.  Since the resulting graph is likely not very accurate, a search is performed for each node and neighbors of neighbors are also given connectivity.  This method is used to quickly build an approximate neighbor graph and is much more computationally efficient than constructing a traditional $k$-NN graph.  Weights and edges are assigned in the same manner as SNE (Equation \ref{eq:SNE_high_dim_data_probs}). Once an approximate $k$-NN graph is constructed, a probabilistic model is used to discover the low-dimensional embedding coordinates which preserve the similarities of samples in the high-dimensional space. The likelihood of the graph is given by 
\begin{align} \label{eq:largevis_obj}
	\begin{split}
		J &=  \prod_{(m,n) \in E}p(e_{mn}=1)^{w_{mn}} \prod_{(m,n) \in E}(1 - p(e_{mn}=1))^{\gamma}\\ 
		&\propto \sum_{(m,n) \in E} w_{mn} \log{p(e_{mn}=1)} +  \sum_{(m,n) \in \bar{E}} \gamma \log{(1 - p(e_{mn}=1))}
	\end{split}
\end{align}
\noindent
where $p(e_{mn}={w_{mn}}) = p(e_{mn}=1)^{w_{mn}}$ is the likelihood of observing a weighted edge, $\bar{E}$ is the set of vertices that are not observed and $\gamma$ is a weight assigned to the pairs of vertices without edges between them.  Maximizing the first part of Equation \ref{eq:largevis_obj} ensures that similar datapoints in the input data space will be close in the embedding space.  The second part of the equation models the likelihood of all vertex pairs without edges.  Maximizing this part ensures that dissimilar points are embedded far from each other.  The graph likelihood is maximized efficiently by asynchronous stochastic gradient descent.


LargeVis was able to provide 2D and 3D visualizations of millions of data points consisting of hundreds of features from datasets representing hand-written digits, social networks, text documents and images \citep{Tang2016LargeVis}.  While LargeVis provides a significant increase in computational efficiency as compared to t-SNE, it still places more importance on preserving local distances, as compared to a combination of local and global.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  UMAP  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Uniform Manifold Approximation and Projection (UMAP)} \label{sec:UMAP}
Similarly to LargeVis, \textit{Uniform Manifold Approximation and Projection} (UMAP) \citep{McInnes2018UMAP} attempts to address the deficiencies exhibit by t-SNE. Specifically, t-SNE has difficulties working with high dimensional data diectly due to memory limitations and can only practically embed data into two or three dimensions \citep{Tang2016LargeVis}. UMAP uses manifold approximation and local fuzzy simplicial set representations to construct a topological representation of the high dimensional data.  The layout of data in the low dimensional space is then optimized to minimize the error between the two topological representations.  While UMAP may appear wildly different from t-SNE at first glance, there are actually only a few key differences which make it more computationally efficient and scalable. 

As with t-SNE (Section \ref{sec:tSNE}) and LargeVis (Section \ref{sec:LargeVis}), UMAP can be described in two phases.  A weighted nearest neighbor graph is constructed in the first phase and a mimetic, low-dimensional representation is computed in the second.  The differences between t-SNE, LargeVis and UMAP amount to the specific details on how the neighborhood graphs are constructed and how the low-dimensional embeddings are computed.

t-SNE defines the high-dimensional input probabilities by normalizing and symmetrizing RBF kernel similarities, which utilizes Euclidean distance (Equation \ref{eq:SNE_high_dim_data_probs}).  The symmetrized neighbor probabilities are defined by 
\begin{align}
	p_{mn} = \frac{p_{m|n} + p_{n|m}}{2N}
\end{align}
\noindent
The similarities in the low-dimensional embedding space are then defined by a Student t-distribution with a single degree of freedom on the Euclidean distance (Equation \ref{eq:tSNE_low_dim_similarity}).  The objective function of t-SNE is given as the KL-divergence between the high and low-dimensional probability distributions
\begin{align}
	J_{t-SNE} = \sum_{m \neq n} p_{mn} \log{\frac{p_{mn}}{q_{mn}}} = \sum_{m \neq n} p_{mn} \log{p_{mn}} - p_{mn} \log{q_{mn}}
\end{align}
Because the computation of both $p_{mn}$ and $q_{mn}$ requires calculation over all pairs of training points, different approaches have been suggested to improve efficiency.  LargeVis, for example, considers only the approximate nearest neighbors for each sample and abandons the graph weight normalization.  Instead of KL-divergence, LargeVis maximizes the graph likelihood
\begin{align} 
	J_{LV} &= \sum_{m \neq n} p_{mn} \log{w_{mn}} + \gamma \sum_{m \neq n}\log{(1 - w_{mn})}
\end{align}
\noindent
where $p_{mn}$ and $w_{mn}$ are defined as in Barnes-Hut t-SNE.  $\gamma$ is a parameter which controls the repulsion force (second part of the objective) relative to the attractive force (first part).  The first part of this objective resembles the optimizable portion of the KL-divergence used by t-SNE, except with the substitution of $w_{mn}$ for $q_{mn}$.

UMAP's cost function can be described by the cross-entropy between $v$ and $w$
\begin{align}
	J_{UMAP} = \sum_{m \neq n} v_{mn} \log{\left( \frac{v_{mn}}{w_{mn}} \right)} + (1 - v_{mn}) \log{\left ( \frac{1 - v_{mn}}{1 - w_{mn}} \right )}
\end{align}
\noindent
which can be rearranged into two constant terms and two optimizable terms
\begin{align} \label{eq:UMAP_objective}
	J_{UMAP} = \sum_{m \neq n} v_{mn} \log{v_{mn}} + (1 - v_{mn}) \log{(1 - v_{mn})} - v_{mn} \log{w_{mn}} - (1 - v_{mn}) \log{(1 - w_{mn})}
\end{align}
\noindent
and has a similar form to LargeVis without the $\gamma$ term and the requirement of matrix-wise normalization in the high-dimensional space.  The high-dimensional data similarities $v_{m|n}$ in Equation \ref{eq:UMAP_objective} are the local fuzzy simplicial set memberships based on smoothed nearest neighbor distances
\begin{align}
	v_{m|n} = \exp{\left( \frac{-\mathcal{D}(\bm{x}_{m},\bm{x}_{n}) - \rho_{n}}{\beta_{n}} \right)}
\end{align}
\noindent
Just as with LargeVis, $v_{m|n}$ is calculated from only the $k$ approximate nearest neighbors and $v_{m|n}=0$ for all other samples.  $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ is an arbitrary distance between high-dimensional samples $\bm{x}_{m}$ and $\bm{x}_{n}$ which is not required to be Euclidean distance.  $\rho_{n}$ is the distance to the nearest neighbor of sample $n$ and $\beta_{n}$ controls the bandwidth of the exponential  distribution.  UMAP leverages ideas of earlier-developed approaches such as Laplacian Eigenmaps (Section \ref{sec:Laplacian_Eigenmaps}) which assume that data on the manifold are uniformly distributed.  While this constraint can easily be enforced in the embedding space, the training sets of remotely sensed data typically do not exhibit uniform distributions.  Because of this, using a fixed metric may leave samples unconnected in the graph.  $\rho_{n}$ plays an important role in that it ensures local connectivity by defining a locally adaptive kernel for each datapoint which allows the metric to vary between each point.

Symmetrization is performed  by the fuzzy set union using the probabilistic t-conorm
\begin{align}
	v_{mn} = (v_{m|n} + v_{n|m}) - v_{m|n}v_{n|m}
\end{align}
\noindent
and the low dimensional data similarities are given by 
\begin{align}
	w_{mn} = (1 + a||\bm{z}_{m} - \bm{z}_{n} ||^{2b}_{2})^{-1}
\end{align}
\noindent
where $a$ and $b$ are user-defined hyper-parameters.  Setting $a=b=1$ results in the Student t-distribution used by t-SNE.  With all terms defined, the objective function in Equation \ref{eq:UMAP_objective} can be optimized efficiently with stochastic gradient descent.
  
Experiments by \cite{McInnes2018UMAP} showed that UMAP was both faster than t-SNE (due to efficient neighbor search and lack of normalization) and was capable of preserving more global data structure.  While the KL-divergence cost used in t-SNE heavily penalizes mapping close points in the high-dimensional space with far points in the low-dimensional space, it is less concerned with mapping points far in the high-dimensional space with samples close in the embedding space.  This reveals that t-SNE places emphasis on locality preservation but does not guarantee that global structure will be retained through the embedding.  UMAP, on the other hand, penalizes for both cases through the use of a cross-entropy cost function.  This effectively allows UMAP to retain (with trade-offs) both local and global data structure. Additionally, whereas t-SNE which uses random initialization, UMAP initializes the low-dimensional data coordinates through the Laplacian Eigenmaps algorithm which assumes uniformity in the embedding space.  Removal of initialization randomness tends to make UMAP embeddings more repeatable.  All in all, UMAP was shown to be competitive with the SOA t-SNE and LargeVis algorithms for data visualization tasks while providing improved speed and flexibility.  UMAP has been used for visualization of data in bioninformatics, materials science and machine learning \citep{McInnes2018UMAP}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Discussion  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Summary of Manifold Learning Algorithms} \label{sec:Manifold_Learning_Discussion}

This literature review discusses a myriad of manifold learning techniques rooted in a variety of fundamental approaches.  However, this review barely scratches the surface of proposed techniques.  With such an expansive corpus of methods, it is often difficult to distinguish which approaches are ``best" for any given application.  This is, of course, application dependent and the optimal features for data visualization are likely very different from those for classification.  However, each method reviewed demonstrates a unique approach toward learning which brings its own strengths and weaknesses.  Table \ref{tab:Manifold_Learning_Comparison_Table} provides the fundamental strategies used by the manifold learning algorithms reviewed in this chapter, ranging from classic to SOA approaches.  While manifold learning techniques are inherently unsupervised, this review showed that many supervised and semi-supervised methods have been developed from base algorithms. However, the current state of the literature is greatly lacking in manifold learning and dimensionality reduction approaches designed for classification which can effectively address label uncertainty.  \textit{In fact, none of the methods reviewed in Section \ref{sec:Manifold_Learning} are inherently capable of reliably learning useful, discriminative low-dimensional features from weakly-supervised data.}  Section \ref{sec:weakly_sup_dim_reduction} discusses dimensionality reduction and manifold learning methods designed specifically for discrimination under the weak learning paradigm.


\begin{longtable}{ |p{0.25\textwidth}|p{0.6\textwidth}|p{0.08\textwidth}| } 
	\caption{Overview of manifold learning methods and their properties.}
	\label{tab:Manifold_Learning_Comparison_Table}\\
	\hline
	\multicolumn{3}{|c|}{\textbf{Overview of Unsupervised Manifold Learning}} \\
	\hline
	\multicolumn{3}{|c|}{\textbf{Linear}} \\
	\hline
	\textbf{Method} & \textbf{Strategy} & \textbf{Intro.}\\
	\hline
	PCA   & Preserves variance of data, global structure. Section \ref{sec:PCA} & 1901\\
	\hline
	MDS  & Preserves pairwise distances, global structure. Section \ref{sec:MDS}  & 1952\\
	\hline
	LDA  & Minimizes within-class distance and maximizes between-class distance, global structure. Section \ref{sec:LDA}  & 1936\\
	\hline
	NMF   & Decomposes data into low-dimensional coordinates and basis vectors, minimizes approximation error. Section \ref{sec:NMF} & 1994\\
	\hline
	NPP  & Linear variant of LLE, preserves local neighborhoods, local structure. Section \ref{sec:LLE}  & 2005\\
	\hline
	LPP  & Linear variant of LE, preserves local neighborhoods, local structure. Section \ref{sec:Laplacian_Eigenmaps}  & 2003\\
	\hline
	\multicolumn{3}{|c|}{\textbf{Nonlinear}} \\
	\hline
	\textbf{Method} & \textbf{Strategy} & \textbf{Intro.}\\
	\hline
	KPCA  & Performs PCA in a feature space defined by a kernel function, preserves variance of data, global structure.  Section \ref{sec:KPCA}  & 1999 \\
	\hline
	MVU  & Learns a kernel function to be used with KPCA that ``unfolds" the manifold, preserves variance of data, global structure. Section \ref{sec:MVU}  & 2004\\
	\hline
	Isomap  & Preserves geodesic distances, global structure.  Section \ref{sec:Isomap}  & 2000 \\
	\hline
	Sammon Mapping  & Preserves pairwise distances.  More emphasis is placed on samples that are close in the input space.  Section \ref{sec:sammon_mapping} & 1969 \\
	\hline
	LLE  & Preserves local neighborhoods, local structure. Section \ref{sec:LLE}  & 2000 \\
	\hline
	Laplacian Eigenmaps  & Preserves local neighborhoods, local structure.  Section \ref{sec:Laplacian_Eigenmaps}  & 2003 \\
	\hline
	Hessian LLE  & Minimizes curvature of high-dimensional manifold estimated from local neighborhoods. Section \ref{sec:Hessian_LLE} & 2003 \\
	\hline
	LTSA  &  Learns manifold in local tangent spaces then aligns to form global coordinate system.  Section \ref{sec:LTSA} & 2002\\
	\hline
	Diffusion Maps  & Preserves diffusion distance.  Section \ref{sec:Diffusion_Maps}  & 2006\\
	\hline
	\multicolumn{3}{|c|}{\textbf{Neural Networks}} \\
	\hline
	\textbf{Method} & \textbf{Strategy} & \textbf{Intro.}\\
	\hline
	Autoencoders  & Neural network where desired output is the input.  Network decreases in dimensionality then increases to input size.  Effectively performs nonlinear PCA. Section \ref{sec:Autoencoders}  & 1986 \\
	\hline
	SOM  & Distributes fixed lattice of neurons to cover input feature space using competitive learning. Section \ref{sec:SOM}   & 1990\\
	\hline
	GNG  & Distributes lattice of neurons to cover input feature space using competitive learning. The network is able to add and delete neurons to fill the data space. Section \ref{sec:GNG}& 1994\\
	\hline
	Elastic Maps and Nets  &  Uses map of neurons to cover the input feature space. Simulates network of springs and is optimized to the mechanical equilibrium of the system.  Section \ref{sec:Elastic_Maps}  & 2008 \\
	\hline
	\multicolumn{3}{|c|}{\textbf{Probabilistic}}\\
	\hline
	\textbf{Method} & \textbf{Strategy} & \textbf{Intro.}\\
	\hline
	PPCA  & Learns parameters of linear mapping from latent space to input space by maximizing data likelihood of a factor analysis model.  Section \ref{sec:FA_and_PPCA} & 1999\\
	\hline
	GTM  &  Learns a mapping from the latent space to the data space by assuming the data distribution as a constrained mixture of Gaussians.  Probabilistic alternative to the SOM. Section \ref{sec:GTM} & 1998\\
	\hline
	Manifold Charting  & Computes mixture of locally linear models on the data then aligns local low-dimensional models to find global coordinates. Section \ref{sec:Manifold_Charting} & 2003 \\
	\hline
	\multicolumn{3}{|c|}{\textbf{High-Dimensional Data Visualization}}\\
	\hline
	t-SNE  & Defines pairwise probabilities of samples being neighbors.  Minimizes the KL-divergence between high and low-dimensional data densities. Retains global and local data structure. Section \ref{sec:tSNE} & 2008 \\
	\hline
	LargeVis  & Efficiently constructs neighborhood graph weighted by conditional probabilities of being neighbors.  Maximizes the graph likelihood in the embedding space. Preserves local distances. Section \ref{sec:LargeVis} & 2016 \\
	\hline
	UMAP  & Uses fuzzy simplicial sets to define locally adaptive neighborhood graph.  Minimizes the cross-entropy between the high and low-dimensional similarity distributions. Preserves global and local structure.  Section \ref{sec:UMAP}  & 2018 \\
	\hline
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Weakly Supervised Manifold Learning  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weakly Supervised Manifold Learning and Dimensionality Reduction} \label{sec:weakly_sup_dim_reduction}
Although the specific feature vectors may be very high-dimensional, the underlying structure of a given dataset set is usually governed by only a few variables. Either implicitly or explicitly, most learning algorithms exploit this underlying structure to make learning and inference possible.
If it is available, relevant information for a specific task can generally be incorporated to provide supervision and improve the performance of unsupervised methods.  Supervised methods for nonlinear dimensionality reduction assume that the samples lie on a manifold parameterized by multiple latent factors.  However, different from traditional manifold learning (where the goal is to preserve the relationships between samples), these methods find the most discriminative low-dimensional representations for classification tasks \citep{Wu2015MILImageManifoldThesis}.  The optimal embedding uses class label information to  minimize distances between nearby points with the same class label while separating samples of different classes.  Although supervised manifold learning often outperforms unsupervised methods for classification tasks, this learning cannot be done directly in  MIL because of the uncertainty on the labels \citep{Carbonneau2016MILSurvey}.  Moreover, fully-annotated samples are often difficult or impossible to obtain in many remote sensing applications \citep{Zare2016MIACE}.  Even with the successes of manifold learning, most the of the previous work has mainly focused on either fully supervised or unsupervised learning.   Existing work in weakly supervised learning on manifolds has primarily considered the semi-supervised setting \citep{Zhang2008SpectralSemiSupManifoldLearning,Chen2018RobustSemiSupManifoldLearning,Zhang2014SemiSupManLearningFusion,Hong2019LearnableManifoldAlignment,Navaratnam2007JointManifoldSemiSupRegression,Stanley2019ManAlignmentFeatureCorrespondence,Tuia2015KernelManifoldAlignment,Wang2010MultiscaleManAlignment,Wang2011HeteroDomainAdaptationManAlignment}.  Methods in this category usually incorporate partially provided image labels and propagate the labels over the manifold approximated by the neighborhood graph on the images.  In a broad sense, however, different situations of weak supervision (specifically, Multiple Instance  Learning), have not been well studied \citep{Wu2015MILImageManifoldThesis}.  The existing MIL manifold learning in the literature can be broken into two paradigms: LDA-based approaches and sparse, orthogonal matrix-based techniques \citep{Zhu2018MIDRSparsity}. Thus, all existing approaches are linear, meaning they may not work well if the underlying bag manifold exhibits curvature.  Alternative weakly-supervised dimensionality reduction approaches have been proposed, however, they do not adhere to the constraints of MIL.  The current literature for weakly supervised dimensionality reduction is reviewed in the following sections, beginning with reviews of MIL DR approaches, namely: MIDR, MidLABS, CLFDA, MIDA and MI-FEAR.  Approaches using alternative definitions of weak learning are addressed at the end of the section.

\subsection{MIDR} \label{sec:MIDR}
The first true MIL manifold learning experimentation was performed by \cite{Sun2010MIDR} under the orthogonal matrix-based paradigm.  To show the need for MIL-specific methods, Sun showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, traditional Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  These results have been shown many times in the literature \citep{Chao2019RecentAdvancesSupervisedDimRed,Vural2018StudySupervisedManifoldLearning}, so they were to be expected.  However, they motivated the need for specialized manifold learning methods that are directly applicable with MIL.  Therefore,  \cite{Sun2010MIDR} proposed \textit{Multiple Instance Dimensionality Reduction} (MIDR), which optimizes an objective through gradient descent to discover sparse, orthogonal projection vectors in the latent space in conjunction with the Multiple Instance Logistic Regression classifier.  The goal of MIDR is to discover a projection matrix $\bm{W} \in \mathbb{R}^{D \times d}$ which will increase discriminability between positive and negative bags in the latent embedding space.  If given the $k^{th}$ training bag, $\bm{B}_{k} = \{ \bm{x}_{k,1}, \dots, \bm{x}_{k,n_{k}} \}$ with corresponding binary bag-level label $L_k \subset \{0,1\}$, MIDR attempts to find a matrix $\bm{W}$ such that the projection of $\bm{B}_{k} \subset \mathbb{R}^{D}$ by $\bm{W}^{T}\bm{B}_{k} \subset \mathbb{R}^{d}$ increases the separation between positive and negative bags. The intuition is that the probability of the $k^{th}$ bag being positive $\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k})$ should be close to one if it is positive and close to zero otherwise.  This can be achieved by minimizing the squared loss between the actual and predicted label of each bag
\begin{align}
	\min_{\bm{W}} \sum_{k=1}^{K} (\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k}) - L_{k})^{2}
	\label{eq:MIDR_orig_objective}
\end{align}
\noindent
Taking advantage of the standard assumption, the posterior probability of a bag can be written in terms of the posterior probabilities of its instances
\begin{align}
	\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k}) = \max_{n} \Pr(l_{k,n}=1|\bm{W}^{T}\bm{x}_{k,n}) 
\end{align}
Equation \ref{eq:MIDR_orig_objective} then becomes 
\begin{align}
	\min_{\bm{W}} \sum_{k=1}^{K} (\max_{n} \Pr(l_{k,n}=1|\bm{W}^{T}\bm{x}_{k,n}) - L_{k})^{2}
	\label{eq:MIDR_instance_objective}
\end{align}
\noindent
From the objective, it is clear that in order to minimize the squared loss, the distances between the key positive instances and all negative instances should be as large as possible. Additionally, $\bm{W}$ is required to be orthogonal in order to guarantee the resulting latent features are uncorrelated (to remove redundancy) as well as sparse (to improve interpretability).  This implies that the new feature representations of instances $\bm{x}_{kn}$ in bag $\bm{B}_{k}$ are formed by linear combinations of the features in the input feature space.  As defined by \cite{Zhu2018MIDRSparsity}, the MIDR optimization problem can be written succinctly as
\begin{align}
	\min_{\bm{W},\bm{\beta}} f(\bm{W},\alpha) + \gamma ||\bm{W} ||_{1}  \quad \text{s.t.} \quad \bm{W}^{T}\bm{W} = \mathbb{I}_{d}
	\label{eq:MIDR_objective}
\end{align}
\noindent
where $\gamma$ is a positive number which controls the balance between the sparsity term $||\bm{W}||_{1}$ and the fitting term $f(\bm{W},\alpha)=\sum_{k=1}^{K} (P_{k}(\bm{W},\bm{\beta}) - L_{k})^{2}$.  In this case, 
\begin{align}
	\begin{split}
	P_{k}(\bm{W},\bm{\beta}) &= softmax_{\alpha}(P_{k,1}(\bm{\bm{W}},\bm{\beta}), \dots, P_{k,n_{k}}(\bm{\bm{W}},\bm{\beta})) \\
	&= \frac{\sum_{n=1}^{n_{k}}P_{k,n}e^{\alpha P_{k,n}(\bm{W},\bm{\beta})}}{\sum_{n=1}^{n_{k}}e^{\alpha P_{k,n}(\bm{W},\bm{\beta})}}
	\end{split}
\end{align}
\noindent
is the softmax approximation over $n$ of $\max (P_{k,1}(\bm{\bm{W}},\bm{\beta}), \dots, P_{k,n_{k}}(\bm{\bm{W}},\bm{\beta}))$.  A popular way to estimate the posterior probability is by logistic regression
\begin{align}
	P_{k,n}(\bm{W},\bm{\beta}) = \Pr(l_{k,n}=1|\bm{W}^{T},\bm{x}_{k,n}) = \frac{1}{1+\exp(-\bm{\beta}^{T} \bm{W}^{T}\bm{x}_{k,n})}
\end{align}
\noindent
Pseudo-code for MIDR is provided in Algorithm \ref{alg:MIDR}.

\begin{algorithm}[h!]
	\caption{MIDR}
	\label{alg:MIDR}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\bm{L} =\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, sparsity parameter $\gamma$}
		\Ensure {Projection matrix $\bm{W}$}
		\While {Not converged}
		\State {Train multiple instance logistic regression $h$ using data $\bm{W}^{T}\bm{B},\bm{L}$}
		\For {$\bm{B}_{k} \in \bm{B}$}
		\State {$P_{k} \gets$ probability $h(\bm{B}_{k})$ }
		\EndFor
		\State {Optimize Equation \ref{eq:MIDR_objective} for new $\bm{W}$}
		\EndWhile            
	\end{algorithmic}
\end{algorithm}

\cite{Sun2010MIDR} used gradient descent to optimize the MIDR objective.  An alternating optimization scheme was employed that switched between estimating the parameters of the MI Logistic Regression and solving for a new sparse, orthogonal embedding  matrix.  It was found that the newly developed method outperformed unsupervised instance-level dimensionality reduction approaches (applied to bags) for bag-level classification.  MIDR was later revisited by \cite{Zhu2018MIDRSparsity} where the optimization problem was reformulated using the \textit{inertial proximal alternating linearized minimization} (iPALM) method.  The advantage of \textit{Multiple Instance Augmented Lagrangian Multiplier} (MI-ALM) \citep{Zhu2018MIDRSparsity} is that the problem variables can be managed separately and updated effectively.  Additionally, the global convergence of MIDR was proved. 

\subsection{MidLABS} \label{sec:MidLABS}
The other existing approaches for dimensionality reduction with multiple instance learning follow a LDA scheme.  \textit{Multi-Instance Dimensionality reduction by Learning a mAximum Bag margin Subspace} (MidLABS) \citep{Ping2010MILDRMaxMargin} applies LDA to find a projection vector which simultaneously maximizes between-class scattering and minimizes within-class scattering to separate positive and negative bags in the embedding space.  While most MIL approaches assume instances are independently and identically distributed (IID), MidLABS represents each bag as a neighborhood graph in order to take advantage of data structure and jointly constructs  the scatter matrices by evaluating the scattering between bags. MidLABS optimizes the following objective:
\begin{align}
\max_{\bm{w}} J(\bm{w})= \max_{\bm{w}} \frac{\bm{w}^{T}(\sum_{L_{i} \neq L_{j}}\bm{K}_{ij})\bm{w}}{\bm{w}^{T}(\sum_{L_{i} = L_{j}}\bm{K}_{ij})\bm{w}}
\end{align}  
\noindent
By defining a bag distance measure, $\bm{K}$, the the between-class and within-class scatter matrices can be constructed as
\begin{align}
\bm{S}_{b} = \sum_{L_{i} \neq L_{j}}\bm{K}_{ij}
\end{align}
\noindent
and
\begin{align}
\bm{S}_{w} = \sum_{L_{i} = L_{j}}\bm{K}_{ij}
\end{align}
\noindent
It can be observed that this problem follows LDA exactly, and can thus be solved as the generalized eigenvalue problem:
\begin{align}
\max_{\bm{w}} J(\bm{w})= \max_{\bm{w}} \frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}
\end{align}
\noindent
In order to take structural information into account, the customized bag distance measurement is defined by:
\begin{align}
	\bm{K}_{ij} = \frac{\sum_{a=1}^{n_{i}}\sum_{b=1}^{n_{j}} (\bm{x}_{ia} - \bm{x}_{jb})(\bm{x}_{ia} - \bm{x}_{jb})^{T}  }{n_{i}n_{j}} + C\frac{\sum_{c=1}^{m_{i}}\sum_{d=1}^{m_{j}} (\bm{e}_{ic} - \bm{e}_{jd})(\bm{e}_{ic} - \bm{x}_{jd})^{T}  }{n_{i}^{2}n_{j}^{2}}
\end{align}
\noindent
where $\bm{x}_{ia}$ is the $a^{th}$ instance in the $i^{th}$ bag, $n_i$ is the total number of instances in the $i^{th}$ bag, $\bm{e}_{ic}$ is the $c^{th}$ edge in the $i^{th}$ bag and $m_{i}$ is the total number of edges in the $i^{th}$ bag.  This kernel represents each bag as an $\epsilon$-graph, where instances are treated as nodes.  Edges are defined between two nodes if the Euclidean distance between them is lower than a threshold, $\epsilon$. \citep{Latham2015MIFeatureRankingThesis}.  Essentially, this measurement compares the closeness of bags by summing over all pairs of instances between the bags. Pseudo-code for MidLABS is provided in Algorithm \ref{alg:MidLABS}.

\begin{algorithm}[H]
	\caption{MidLABS}
	\label{alg:MidLABS}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$}
		\Ensure {Linear projection vector $\bm{w}$}
		\For{bag $\bm{B}_{k}$ in $\{ \bm{B}_{1}, \dots, \bm{B}_{K} \}$}   
		\State{$G_{k} \gets \epsilon$-graph for $\bm{B}_{k}$}
		\EndFor             
		\State{Define $\bm{K}_{ij} = \frac{\sum_{a=1}^{n_{i}}\sum_{b=1}^{n_{j}} (\bm{x}_{ia} - \bm{x}_{jb})(\bm{x}_{ia} - \bm{x}_{jb})^{T}  }{n_{i}n_{j}} + C\frac{\sum_{c=1}^{m_{i}}\sum_{d=1}^{m_{j}} (\bm{e}_{ic} - \bm{e}_{jd})(\bm{e}_{ic} - \bm{x}_{jd})^{T}  }{n_{i}^{2}n_{j}^{2}}$}    
		\State {$\bm{S}_{b} = \sum_{L_{i} \neq L_{j}}\bm{K}_{ij}$}
		\State{$\bm{S}_{w} = \sum_{L_{i} = L_{j}}\bm{K}_{ij}$}
		\State {Solve the generalized eigenvalue problem $\bm{S}_{b}\bm{w}=\lambda\bm{S}_{w}\bm{w}$}
		\State {Sort eigenvectors $\bm{w}$ by their eigenvalues $\lambda$}
		\State {$\bm{w} \in \mathbb{R}^{D \times 1} \gets$ top eigenvector }
	\end{algorithmic}
\end{algorithm}


\subsection{MIDA} \label{sec:MIDA}
In 2014, Chia et al. introduced Multiple-Instance Discriminant Analysis (MIDA) \citep{Chai2014MIDA}.  MIDA has the same objective as MidLABS, which is to discover a linear projection basis which separates bags in the embedding space.  Both MIDA and MidLABS  can be considered as MI extensions of LDA. However, the way these algorithms construct their scatter matrices is very different.  While MidLABS constructs its  scatter matrices at the bag level by directly evaluating the scattering amongst bags, MIDA  uses instance-level information to formulate the within-class and between-class scatter.  In other words, MIDA selects a prototype for each bag and utilizes the selected instances as bag representatives for constructing the scatter.  The mean of all negative instances is used as the negative class prototype. The difficulty with this approach is the ambiguity on which instances are truly positive.  The other major difference between MidLABS and MIDA is that MIDA does not consider structural information of data when formulating the scatter matrices. To select candidate positive prototypes, MIDA initializes a set by selecting the most-likely positive instances as those with the lowest density in a Gaussian likelihood estimated by all instances in the negative bags.  An iterative optimization procedure is applied which trades-off between candidate positive instance selection and learning the projection weights into the latent space which maximizes the separation between bags with different labels and minimizes the distances between bags with the same label.  Pseudo-code for MIDA  is provided in Algorithms \ref{alg:MIDA_init} and \ref{alg:MIDA_opt}.

\begin{algorithm}[h!]
	\caption{Initialization of MIDA}
	\label{alg:MIDA_init}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K} \}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameter $\beta$}
		\Ensure {Initialized positive prototypes $\bm{x}^{+} = \{\bm{x}^{+}_{1}, \dots, \bm{x}^{+}_{N^{+}} \}$}
		\For{bag $\bm{B}_{k}$ in $\bm{B}^{+}$}   
		\State{$\bm{x}^{+}_{k} \gets \arg \min_{\bm{x}_{kn}}$ $C\sum_{m=1}^{N^{-}}\sum_{o=1}^{N^{-}_{m}} \exp \left ( \frac{||\bm{x} - \bm{x}^{-}_{mo} ||^{2}_{2}}{\beta} \right )$ $\forall n = 1, \dots, N^{+}_{k}$}
		\EndFor             
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h!]
	\caption{Projection vector calculation process of MIDA}
	\label{alg:MIDA_opt}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, Initialized positive prototypes $\bm{x}^{+} = \{\bm{x}^{+}_{1}, \dots, \bm{x}^{+}_{N^{+}} \}$}
		\Ensure {Linear projection vector $\bm{w}$}    
		\State {$\bm{\mu}^{+} \gets \frac{1}{N^{+}} \sum_{k=1}^{N^{+}} \bm{x}^{+}_{k}$}
		\State {$\bm{\mu}^{-} \gets C\sum_{m=1}^{N^{-}}\sum_{o=1}^{N^{-}_{m}} \bm{x}^{-}_{mo}$}        
		\State{$\bm{S}_{w}^{+} \gets \sum_{k=1}^{N^{+}}(\bm{x}^{+}_{k} - \bm{\mu}^{+})(\bm{x}^{+}_{k} - \bm{\mu}^{+})^{T}$}
		\State{$\bm{S}_{w}^{-} \gets \sum_{m=1}^{N^{-}}(\bm{x}^{-}_{m} - \bm{\mu}^{-})(\bm{x}^{-}_{m} - \bm{\mu}^{-})^{T}$}
		\State {$\bm{S}_{w} \gets \bm{S}_{w}^{+} + \bm{S}_{w}^{-}$}
		\State {$\bm{S}_{b} \gets \sum_{k=1}^{N^{+}} \sum_{m=1}^{N^{-}} (\bm{x}^{+}_{k} - \bm{x}^{-}_{m})(\bm{x}^{+}_{k} - \bm{x}^{-}_{m})^{T}$}
		\State {Solve the generalized eigenvalue problem $\bm{S}_{b}\bm{w}=\lambda\bm{S}_{w}\bm{w}$}
		\State {Sort eigenvectors $\bm{w}$ by their eigenvalues $\lambda$}
		\State {$\bm{w} \in \mathbb{R}^{D \times 1} \gets$ top eigenvector of $\frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}$}
	\end{algorithmic}
\end{algorithm}

\subsection{CLFDA} \label{sec:CLFDA}
Finally, Citation Local Fisher Linear Discriminant Analysis (CLFDA) \citep{Kim2010LocalDRMIL} incorporates citation and reference information into local Fisher Discriminant Analysis, thus it can also be treated as a MI extension to LDA.  Contrary to the previously-mentioned approaches, CLFDA operates at the instance-level, attempting to find a subspace where positive and negative instances are maximally-separable \citep{Latham2015MIFeatureRankingThesis}.  CLFDA can be viewed as complimentary to MIDA \citep{Chai2014MIDA}.  Whereas, MIDA tries to seek true positive instances in positive bags, CLFDA attempts to detect incorrectly labeled instances (false positives) in positive bags.  CLFDA pre-labels all instances with the label of their bag  $l_{kn} = L_{k}, \forall n = 1,\dots,n_{k}$, then utilizes neighborhood information to detect false positive instances.  In other words, it uses the assumption that if an instance in a positive bag is close to many points in the negative bags, it is likely also negative.  CLFDA defines \textit{references} simply as the $R$ nearest neighbors to $\bm{x}_{n}$, while $citers$ are defined as the samples which have $\bm{x}_{n}$ in their $C$-nearest neighbors, or $citers(\bm{x}_{n}) = \{ \bm{x}_{m}| \bm{x}_{n} \in CNN(\bm{x}_{m}), m=1, \dots, N \}$.   The steps of CLFDA are to 1.) construct a $\max(R,C)$-NN graph, where the $\max(R,C)$-NN graph is a $K$-NN graph with $K=\max(R,C)$.  This is used to detect false positives and re-label them as negative.  If the ratio of instances from negative bags versus instances from positive bags in the references and citers of $\bm{x}_n$ exceeds a threshold $\tau$, then $\bm{x}_{n}$ is given a negative label. This is only done for instances from positive bags, as the SMI assumptions states that all instances in negative bags should be negative. 2.) Construct scatter matrices using the provided positive and negative instances. 3.) Find projection weights which simultaneously maximize the distances between positive and negative bags and minimizes the distances between bags with the same class labels using \textit{Local Fisher Discriminant Analysis} (LFDA).  LFDA is a version of LDA  designed to address multi-modal data distributions.  The primary difference between LDA and LFDA is that when maximizing and minimizing the between-class and within-class scatter matrices, respectively, the scatter contribution of a single instance pair is weighted by the locality of the pair.  This prevents instances from different clusters sharing the same label from being forced into  the same space.  This also allows multiple dimensions to be analyzed in the embedding space, whereas LDA is limited to a single dimension for binary classification.  Pseudo-code for CLFDA is given in Algorithm \ref{alg:CLFDA} \citep{Latham2015MIFeatureRankingThesis}.  A potential downside of CLFDA, however, is that it has been shown that pre-labeling all instances in positive bags as positive often leads to poor results, especially when there are large numbers of negative instances in the positive bags \citep{Chai2014MIDA}.

\begin{algorithm}[h!]
	\caption{CLFDA}
	\label{alg:CLFDA}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $C,R,\tau$}
		\Ensure {Projection matrix $\bm{W}$}
		\State {$\bm{X} \gets$ instances in $\bm{B}$}
		\State {$G \gets \max(R,C)$-nearest neighbor graph of $\bm{X}$}
		\For {$n = 1 \to N$}   
		\State {$R_{n} \gets R$-nearest references of instance $\bm{x}_{n}$}
		\State {$C_{n} \gets C$-nearest citers of instance $\bm{x}_{n}$}
		\State {$N_{n}^{-} \gets $-number of instances from negative bags in  $R_{n} + C_{n}$}
		\State {$N_{n}^{+} \gets $-number of instances from positive bags in  $R_{n} + C_{n}$}
		\If {$\frac{N_{n}^{-}}{N_{n}^{+}} \geq \tau$ or $\bm{x}_{n}$ is from a negative bag}
		\State {instance label $l_{n} \gets -1$}
		\Else
		\State {$l_{n} \gets +1$}
		\EndIf
		\EndFor             
		\State{$\bm{A}^{b} \gets$ between-class affinity matrix}
		\State{$\bm{A}^{w} \gets$ within-class affinity matrix}
		\State{Between-class scatter matrix $\bm{S}_{b} = \frac{1}{2}\sum_{m,n=1}^{N}\bm{A}_{mn}^{b}(\bm{x}_{m}-\bm{x}_{n})(\bm{x}_{m}-\bm{x}_{n})^{T}$}
		\State{Within-class scatter matrix $\bm{S}_{w} = \frac{1}{2}\sum_{m,n=1}^{N}\bm{A}_{mn}^{w}(\bm{x}_{m}-\bm{x}_{n})(\bm{x}_{m}-\bm{x}_{n})^{T}$}
		\State {$\bm{W} \in \mathbb{R}^{D \times d} \gets$ top $d$ eigenvectors of $\frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}$}
	\end{algorithmic}
\end{algorithm}

As previously mentioned, all existing MIL dimensionality reduction approaches in the literature are solely linear.  However, many modalities exhibit nonlinear variation.  Therefore, nonlinear manifold learning approaches should be developed which operate under the multiple instance learning framework. 

\subsection{MI-FEAR} \label{sec:MIFEAR}
While the previously mentioned approaches were based on feature extraction,  multiple instance dimensionality reduction has also been investigated under the feature selection paradigm.  Specifically, \cite{Latham2015MIFeatureRankingThesis} used feature ranking to determine the most important features for instance-level classification.  Feature ranking considers each feature independently according to a scoring function, thus revealing properties of the individual features by which they may be compared and ranked.  The challenge of learning a good feature ranking under an instance-space metric is that the instance labels are not available under the MIL framework.  \textit{Multiple-Instance Feature Ranking} (MI-FEAR) \citep{Latham2015MIFeatureRankingThesis} assumes one-sided noise by giving every instance the label of its corresponding bag.  This essentially transforms the weakly-supervised problem into a supervised one.  Pseudo-code for MI-FEAR is provided in Algorithm \ref{alg:MIFEAR}.

\begin{algorithm}[h!]
	\caption{MI-FEAR}
	\label{alg:MIFEAR}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\bm{L} = \{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, evaluation metric $\mathcal{L}$, learning algorithm $\mathcal{A}$, dimensionality of reduced-dimensional space $d$}
		\Ensure {$[ \theta_{i}$ for $ \theta_{i}$ in $V[1,\dots,d]  ]$}
		\State {$\bm{X}, \bm{Y}^{\gamma} \gets $ supervised dataset of bag-labeled instances using $(\bm{B}, \bm{L})$}
		\State {$V \gets $ [ ]}
		\For {feature $\theta_{i}$ in feature set $\Theta$}   
		\State {$\bm{X}^{\theta} \gets \bm{X}$ with all features, excluding $\theta_{i}$}
		\State {$h_{\theta} \gets $ output of $\mathcal{A}$ when trained on input $(\bm{X}^{\theta}, \bm{Y}^{\gamma})$}
		\State {$V[i] \gets$ performance of $h_{\theta}$ on input $(\bm{X}^{\theta}, \bm{Y}^{\gamma})$ according to $\mathcal{L}, \theta_{i}$}
		\EndFor             
		\State {$V \gets$ $\textsc{SortDecreasing}(V)$}
	\end{algorithmic}
\end{algorithm}
\noindent
Each feature $\theta$, is given a score based on the performance of a learned hypothesis $h_{\theta}$ operating on a single feature.  The performance is determined an evaluation metric $\mathcal{L}$.  Once a score has been assigned to every feature, the features are ranked according to relative importance and the top $d$ features are retained as the feature set.  As stated in Section \ref{sec:CLFDA}, the characteristic accuracy of MI-FEAR is based on noisy labels, where each instance is given its corresponding bag-level label.  This approach has proven non-ideal in the literature for determining accurate instance-level classification.  However, a benefit of feature selection is that, unlike feature extraction, the reduced-dimensional space retains its interpret-ability.  In other words, the representations of individual features do not change, meaning they keep their real-world relevance, if applicable.  On the other hand, feature selection requires that a subset of useful features for classification already exist in the training set.  Empirical results showed that consistently achieved lower instance-level classification accuracy than CLFDA and MidLABS, but had a significantly lower run-time.

\subsection{Comparison Table of MI Dimensionality Reduction Methods}
Table \ref{tab:MIDRComparison} shows a comparison between the multiple instance dimensionality reduction methods reviewed in Section \ref{sec:weakly_sup_dim_reduction}.

\begin{longtable}{ |p{2cm}|p{6cm}|p{2cm}|p{3cm}|  } 
	\caption{Summary of multiple instance dimensionality reduction approaches.}
	\label{tab:MIDRComparison}\\
	\hline
	\multicolumn{4}{|c|}{\textbf{Multiple Instance Dimensionality Reduction}} \\
	\hline
	\textbf{Method} & \textbf{Summary} & \textbf{Reduction Method} & \textbf{Classification Level}\\
	\hline
	MIDR  \newline (2010)  & Finds a sparse, orthogonal linear projection matrix optimized for bag-level logistic regression.  Section \ref{sec:MIDR}.   &Linear, orthogonal projection  &  bag-level\\
	\hline
	MidLABS \newline (2010) &  Finds linear projection vector using LDA defined from bag-similarity kernel. Section \ref{sec:MidLABS}.  & Linear, LDA   & bag-level\\
	\hline
	MIDA \newline (2014) &Learns linear projection vector using LDA defined from bag representative vectors. Section \ref{sec:MIDA}. & Linear, LDA &  instance-level\\
	\hline
	CLFDA \newline (2010)  &Learns linear projection matrix using local discriminant analysis defined from instance scatter.  Instance labels are provided as bag labels and refined. Section \ref{sec:CLFDA}. & Linear, LFDA &  instance-level\\
	\hline
	MI-FEAR \newline (2015) &  Incrementally leaves out feature and evaluates performance loss to provide feature score.  Section \ref{sec:MIFEAR}.  & Linear, Feature selection & instance-level\\
	\hline
\end{longtable}

\subsection{General Weak Supervision}
Alternative approaches to weakly supervised dimensionality reduction that do not follow the MIL framework have also been proposed.  For example, Wu studied weakly supervised manifold learning for manifold factorization using image-level labels, where the labels were the variation of interest.  The primary idea was to use weak labels to find image pairs that should be more similar after removing unwanted image variation.  \cite{Wu2015MILImageManifoldThesis} used an alignment method constrained by Hessian regularization to learn a manifold regression, such that new test images would be projected smoothly into a space near neighboring input images.  \cite{Gaur2017MILSemanticObjectCorrespondence} used weakly supervised manifold learning to perform dense semantic object correspondence. The objective of the semantic object correspondence problem is to compute dense association maps for a pair of images such that the same object parts get matched, even for very differently appearing object instances. The goal of Gaur and Manjunath's work was to learn a manifold such that features belonging to the same semantic object parts were projected closer to each other on the manifold. This was achieved by re-purposing deep convolutional features from a classification network, where the labels were weak segmentations of object parts.  These features were then projected onto a manifold using LDA. \textit{Hierarchical Agglomerative Clustering} (HAC) was performed in the embedding space and the labels were refined with respect to geodesic distance on the manifold.  This method was inspired by the \textit{manifold assumption}, which states that similar objects (even though disparate in the input feature space), should share an intrinsic manifold.  In this way, feature embeddings are learned by an optimization process which is rewarded for projecting features closer on the manifold if they have low feature-space dissimilarity.  Additionally, the optimization penalizes feature clusters whose geometric structure is inconsistent with the observed geometric structures of object parts.  

An alternative approach for discriminative dimensionality reduction with weak labels is through the application of metric embedding, which is discussed in detail in Section \ref{sec:MetricEmedding}.  While they can be fully supervised, metric embedding techniques often consider groups of samples (usually two or three), jointly, to learn an embedding function.  Under this type of weak supervision, only a notion of semantic similarity is needed,  as compared to direct class labels. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Metric Embedding %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metric Embedding} \label{sec:MetricEmedding}

The concepts of ``near" and ``far" are very powerful and useful utilities in everyday life.  They classify the relationship between two ``primitives" as being similar or dissimilar, as well as the degree of compatibility \citep{Thorstensen2009ManifoldThesis}. As an example, a medical doctor might consider a machine learning researcher and a software engineer as being similar (near), because they both perform research for computer applications.  However, the same researcher and engineer would likely consider their jobs as being very disparate (far) based on the details of their work.  In order to capture this abstraction of distance in a mathematical construct, a \textit{metric space} is defined.

 \theoremstyle{definition}
 \begin{definition}{Metric Space}
 A \textit{metric space} is an ordered pair $(\mathcal{X},\mathcal{D})$ where $\mathcal{X}$ is a set and $\mathcal{D}$ is a metric on $\mathcal{X}$, or $\mathcal{D}:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ such that $\forall \bm{x},\bm{y},\bm{z}\in\mathcal{X}$, the following holds:

 \end{definition}
 	\begin{enumerate}
	\item Non-negativity: $\mathcal{D}(\bm{x},\bm{y}) \geq 0$
	\item Identity: $\mathcal{D}(\bm{x},\bm{y}) = 0 \iff \bm{x} = \bm{y} $
	\item Symmetry: $\mathcal{D}(\bm{x},\bm{y}) = \mathcal{D}(\bm{y},\bm{x})$
	\item Triangle Inequality:  $\mathcal{D}(\bm{x},\bm{z}) \leq \mathcal{D}(\bm{x},\bm{y}) + \mathcal{D}(\bm{y},\bm{z})$
	\end{enumerate}
\noindent
The non-negativity rule states that the metric evaluated on two instances must have a positive value or be equal to zero.  From the Identity rule, we can see that the metric may only be defined as zero if the two instances being evaluated are exactly the same (thus the dissimilarity is zero).  The Symmetry rule states that a metric evaluated between two instances must be the same regardless of ordering.  Finally, the Triangle Inequality says, intuitively, that the direct distance between two instances $\bm{x}$ and $\bm{z}$ is smaller than the distance between $\bm{x}$ and $\bm{y}$ plus the distance between $\bm{y}$ and $\bm{z}$.  Both sides of the inequality will be equal if and only if $\bm{y}$ lies on the path between $\bm{x}$ and $\bm{z}$ on which the metric is defined.  \newline
The goal of \textit{metric embedding learning} it to learn a function $f_{\theta}(\bm{x}):\mathbb{R}^{D} \rightarrow \mathbb{R}^{d}$ which maps semantically similar points from the data input feature space of $\mathbb{R}^{D}$ onto \textit{metrically close} points in $\mathbb{R}^{d}$.  Similarly, $f_{\theta}$ should map semantically different points in $\mathbb{R}^{D}$ onto metrically distant points in $\mathbb{R}^{d}$.  The function $f_{\theta}$ is parameterized by $\theta$ and can be anything ranging from a linear transformation to a complex non-linear mapping as in the case of deep artificial neural networks \citep{Hermans2017DefenseTripletLoss}.  Let $\mathcal{D}(\bm{x},\bm{ y}): \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a metric function measuring similarity or dissimilarity in the embedded space.  For succinctness, $\mathcal{D}_{m,n} = \mathcal{D}(f_{\theta}(\bm{x_{m}}),f_{\theta}(\bm{x_{n}}))$
defines the dissimilarity between samples $\bm{x}_{m}$ and $\bm{x}_{n}$, after being embedded.  It should be noted that, unlike \textit{metric learning} where the objective is to learn an appropriate metric to measure dissimilarity between samples in opposing classes, metric embedding attempts to learn a transformation function such that samples in the embedding space adhere to a pre-defined measure of similarity \citep{Hermans2017DefenseTripletLoss}.  Metric embedding is often realized through weakly-supervised learning, where instead of labels, the data is accompanied with sets of preferences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Ranking Loss  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Ranking Loss}
	Unlike other loss function such as mean-squared error or cross-entropy whose objective is to directly compare labels, values or sets of values assigned to a given input, the objective of \textit{ranking loss} (also called \textit{margin loss}) functions is to measure relative distances between sets of inputs.  To use a ranking loss in a learning scenario, sets of inputs (usually two or three) are embedded through a transformation function into a defined metric space.  A metric is used to measure similarity (often Euclidean distance), and the transformation function is updated such that points which have a higher semantic similarity label are embedded more closely, and points which are dissimilar are pushed further apart, according to the metric.  In this framework, the actual values of the embedded features are ignored.  Instead, only the distances between them matter.  While ranking losses have been developed which consider many datapoints at a time \citep{Sohn2016NPairLoss}, the most popular ranking loss functions in the literature are based on \textit{contrastive} \citep{Koch2015SiameseNetworks} and \textit{triplet} \citep{Schroff2015FaceNet} losses.
	
	\subsubsection{Contrastive Loss}
	Let $\bm{x} \in \mathcal{X}$ be data with corresponding instance-level labels $l \in \{l_{1}, \dots, \l_{N} \}$.  The superscripts $\bm{x}^{a}, \bm{x}^{p}, \bm{x}^{n}$ are used to denote \textit{anchor}, \textit{positive} and \textit{negative} samples, respectively.  The anchor point is the sample of interest, while the positive is a sample of the same class (or deemed to be similar to the anchor) and the negative sample is a point of a different class (or dissimilar to the anchor).
	\textit{Contrastive loss} functions take pairs of examples as input and learns and embedding function to predict whether two inputs are from the same class or not.  Specifically, contrastive loss can be written as:
	\begin{align}
		\begin{split}
		\mathcal{L}^{\alpha}_{\text{cont}}(\bm{x}_{m},\bm{x}_{n};f_{\theta}) = &\bm{1}\{l_{m}=l_{n} \} \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n}))\\ + &\bm{1}\{l_{m} \neq l_{n} \} \max(0,\alpha - \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n})))
		\end{split}
	\label{eq:contrastive_loss}
	\end{align}
	\noindent
	where $\alpha$ is a margin parameter imposing a distance between different classes to be larger than $\alpha$.  Analyzing Equation \ref{eq:contrastive_loss}, it can be observed that this loss has two unique terms.  If the label of $\bm{x}_{m}$ and $\bm{x}_{n}$ are the same (anchor and positive pair), only the first half of the loss is computed.  This term is simply the dissimilarity between the samples in the embedding space.  Ideally, this term should equate to zero, thus implying the terms are close to each other in the embedding space.  If the labels of the samples are opposing (anchor and negative pair), the second term is computed.  This equates to the maximum value between zero and the difference between the margin constraint and the dissimilarity between the samples.  Thus, if the distance between  the anchor and negative is greater than the margin $\alpha$, the objective is met and the loss equates to zero.  Otherwise, the loss is positive.  Figure \ref{fig:contrastive_loss} demonstrates the idea of contrastive loss.  If given a positive anchor pair, the transformation function should embed the samples closer in the embedding space.  If given a negative pair, the opposite should occur.  It is obvious that with each sample pair the embedding function is only updated in one way (pushing or pulling) \citep{Sohn2016NPairLoss,Koch2015SiameseNetworks}.  
	
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/contrastive_negative"}
			\caption[Contrastive loss with anchor/negative pair]{Contrastive loss with an anchor/negative pair.  After embedding, the samples should be metrically farther away than in the input feature space. }
			\label{fig:contrastive_loss_negative}
		\end{figure*}
	\end{center}

	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/contrastive_positive"}
			\caption[Contrastive loss with anchor/positive pair]{Contrastive loss with an anchor/positive pair.  After embedding, the samples should be metrically closer than they were in the input feature space. }
			\label{fig:contrastive_loss}
		\end{figure*}
	\end{center}
	
	
	\subsubsection{Triplet Loss}
	Whereas contrastive loss considers pairs of samples, triplet loss jointly optimizes between three samples $\{\bm{x}^{a}, \bm{x}^{p}, \bm{x}^{n}\}$, the anchor, a positive and a negative \citep{Hermans2017DefenseTripletLoss,Schroff2015FaceNet, Deng2019ArcFaceAngularMarginLoss}. The fundamental idea behind triplet loss is that, for an input sample, we desire to shorten the distances between its embedded representation and those of similar examples, while simultaneously enlarging the distances between dissimilar examples \citep{Sohn2016NPairLoss}. Triplet loss can be written as:
	\begin{align}
	\mathcal{L}^{\alpha}_{\text{tri}}(\bm{x}^{a},\bm{x}^{p},\bm{x}^{n};f_{\theta}) = \max(0,\mathcal{D}(f_{\theta}(\bm{x}^{a}),f_{\theta}(\bm{x}^{p})) - \mathcal{D}(f_{\theta}(\bm{x}^{a}),f_{\theta}(\bm{x}^{n})) + \alpha )
	\label{eq:triplet_loss}
	\end{align}
	\noindent
	From Equation \ref{eq:triplet_loss}, it can be observed that the triplet loss is satisfied whenever the distance between the negative and anchor is greater than the distance between the corresponding positive sample and the anchor by a margin $\alpha$.  Whereas contrastive loss simply pushes or pulls, triplet loss does both, simultaneously.  This is depicted visually by Figure \ref{fig:triplet_loss}.
	
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/triplet"}
			\caption[Triplet loss]{Embedding according to triplet loss.  After embedding, the positive example should be closer to the anchor point than the negative example by a margin.}
			\label{fig:triplet_loss}
		\end{figure*}
	\end{center}

	The advantage of this formulation is that, while all points of the same class will eventually form a cluster, they are not required to collapse to a single point; they just need to be close to each other than to any point from a different class \citep{Hermans2017DefenseTripletLoss}.  A major drawback of triplet loss, however, is that as datasets grow larger, the possible number of triplets grows cubically.  This can cause practical issues as many of the triplets will likely already satisfy the margin constraints and will not contribute toward learning.  Therefore, \cite{Schroff2015FaceNet} argue that \textit{hard-mining} is often imperative for the success of triplet-based methods.   Essentially, three scenarios exists.  \textit{Easy triplets} are triplet sets which already satisfy the margin constraint, thus inducing zero loss.  \textit{Hard triplets} are sets where the negative sample is closer to the anchor than the positive.  The loss is positive (and greater than $\alpha$).  \textit{Semi-hard triplets} are sets where the negative point is more distant to the anchor than the positive, but it is not greater than the margin $\alpha$, so the loss is still positive (but smaller than $\alpha$).  Therefore, careful consideration should be taken in triplet construction to maximize the number of hard and semi-hard triplets shown to the learning algorithm. 
	
	Alternative approaches such as the ones by \cite{Sohn2016NPairLoss}, \cite{Deng2019ArcFaceAngularMarginLoss} and \cite{Xu2014LargeMarginWeaklySupervisedDR} use modifications of contrastive or triplet loss to obtain feature representations which adhere to a desired metric embedding.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Large Margin K-NN  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Large-Margin K-Nearest Neighbors (LMNN)}
		
		\cite{Weinberger2009LMNN} explored the topic of ranking loss with the explicit goal of performing $k$-nearest neighbor classification in the learned embedding space \citep{Hermans2017DefenseTripletLoss}.  This approach is based on the \textit{Large Margin Nearest Neighbor loss} for optimizing $f_{\theta}$, defined as:
		\begin{align}
			\mathcal{L}_{\text{LMNN}}(\theta) = (1-\mu)\mathcal{L}_{\text{pull}}(\theta) + \mu \mathcal{L}_{\text{push}}(\theta)
		\end{align}
		\noindent
		which is comprised of a \textit{pull}-term that pulls data points toward its \textit{target neighbors}, or its nearest neighbors from the same class, and a \textit{push}-term that pushes data points from a different class.  The term $\mu$ is a trade-off parameter used to control the priority of pushing or pulling. The loss terms are defined as:
		\begin{align} \label{eq:lmnn_pull}
			\mathcal{L}_{\text{pull}}(\theta) =  \sum_{m,n}\eta_{mn} \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n}))
		\end{align}
		\begin{align} \label{eq:lmnn_push}
		\mathcal{L}_{\text{push}}(\theta) = \sum_{m,n,o}\eta_{mn}(1-l_{m,o}) \max(0,\mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n})) - \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{o})) + \alpha )
		\end{align}
		\noindent
		where  $\eta_{mn} \in \{0,1\}$ is a binary indicator variable used to express whether input $\bm{x}_{n}$ is a target neighbor of input $\bm{x}_{m}$ and $\alpha$ is a margin enforcing distance between the classes.  Analyzing Equations \ref{eq:lmnn_pull} and \ref{eq:lmnn_push}, it can be observed that the pull term only penalizes large distances between inputs and target neighbors ($k$-nearest neighbors with the same class label).  The push term, however, is comparable to the triplet loss, which enforces that the dissimilarities between the anchor sample $\bm{x}_{m}$ and \textit{all negative samples} is greater than the dissimilarities between the anchor and its corresponding target neighbors by at least a margin $\alpha$.  Figure \ref{fig:lmnn} demonstrates the pushing and pulling idea captured by LMNN.  In the rest of the algorithm, a matrix $\bm{z}$ is learned which captures an appropriate Mahalanobis metric to measure the dissimilarity between samples, optimized for the LMNN loss.  
		
		
		\begin{center}
			\begin{figure*}[h]
				\centering
				\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/lmnn"}
				\caption[Large Margin $k$-NN]{Large Margin $k$-NN.  The goal of LMNN is to discover a metric which represents neighbors of the same class as being closer to the anchor points than neighbors with differing class labels, plus a margin.}
				\label{fig:lmnn}
			\end{figure*}
		\end{center}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Siamese Networks  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Siamese Neural Networks}
		\textit{Siamese Neural Networks} are a unique neural architecture proposed by \cite{Bromley1993SiameseNetworks} used to rank similarity between inputs.  Essentially, Siamese networks use a base architecture to perform feature encoding.  This base network is replicated to form twin networks which share parameters \citep{Koch2015SiameseNetworks,Koch2015SiameseNetworksThesis}. The two networks are tied at the output, where a distance metric is used to compare the embeddings of the inputs to each twin.  Figure \ref{fig:siamese_networks_conrastive} demonstrates this idea.  The base network can be any architecture (i.e. fully-connected, convolutional, recurrent) so  long as it produces a fixed-sized embedding vector at its output.  A common implementation is to embed sample pairs using a contrastive loss function \citep{Koch2015SiameseNetworks}.  In this scenario, pairs of samples (anchor and positive or anchor and negative) are passed into the twin networks.  An embedded representation is produced for each sample, and a dissimilarity metric (typically $L_{2}$ or $L_{1}$ norm) is applied to compare the samples.  Thus, the actual values of the features do not matter, and only the relative distance between them in the embedding space is compared.  Following the idea of contrastive loss, pairs with the same label should be embedded close to each other, while contrasting pairs should be mapped further apart in the embedding space.  While Siamese networks are traditionally applied as twins, they can be extended to take any number of inputs.  Another common implementation is a \textit{Siamese Triplet network} \citep{Hoffer2015DeepMetricLearning}.  As the name suggests, the base neural architecture is duplicated not once, but twice to form three networks with shared weights.  Triplet loss is used to update the network parameters.  As demonstrated by Figure \ref{fig:siamese_networks_triplet}, the network takes a triplet set of samples as input and an embedded representation is produced for each sample (through the same transformation because the weights are shared).  A metric function then compares the dissimilarities between the anchor and positive samples, as well as the anchor and negative.  A comparator is applied at the output to compute the triplet loss between the embedded representations.  The idea is that samples from the same class or with high semantic similarity should have a lower dissimilarity in the embedding space than samples  from opposing classes.
		
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/siamese_contrastive"}
			\caption[Siamese neural network with contrastive loss]{Siamese neural network structured from contrastive loss.  Two identical neural networks simultaneously embed an anchor and positive/negative sample.  After the shared network weights are learned, anchor/positive pairs should have a lower dissimilarity score than anchor/negative sample pairs.}
			\label{fig:siamese_contrastive}
		\end{figure*}
	\end{center}
		
		
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=1\textwidth]{"lit_review/Metric_Embedding/siamese_triplet"}
			\caption[Siamese neural network with triplet loss]{Siamese neural network structured from triplet loss.  Three identical neural networks simultaneously embed triplets of anchor,  positive and negative samples.  After the shared network weights are learned, anchor/positive pairs should have a lower dissimilarity score than anchor/negative pairs.}
			\label{fig:siamese_triplet}
		\end{figure*}
	\end{center}
	
		The embeddings produced by Siamese networks have  been shown to be powerful representations for discrimination tasks \citep{Chen2020ContrastiveLearning, Schroff2015FaceNet, Koch2015SiameseNetworks}.  In test, pairs or sets of images are passed through the contrastive network to simultaneously produce embeddings.  The pairing with the lowest dissimilarity would assign the class label to the test point.  For example, if classifying a test point into one of ten classes, ten pairs could be passed through the network (the test sample and a representative from each class), and the label of the test point would be assigned as the label of the representative from the best-ranking pair.   Alternatively, after all training samples are embedded through the triplet network, an alternative classification scheme such as $k$-NN or a SVM could be trained on the embedded representations.  Testing would simply consist of embedding a test sample through the network and using the test procedure of the external learner.
		
		
		\cite{Chen2020ContrastiveLearning} noted a few interesting observations regarding Siamese networks. First, data augmentation is often necessary when training Siamese networks to avoid overfitting.  Second, representations often benefit from normalization. Finally, contrastive learning benefits from larger batch sizes and longer training times than its supervised counterpart.
		
		Despite the nuances mentioned, Siamese networks have been applied  successfully to applications in object recognition \citep{Hoffer2015DeepMetricLearning}, one-shot learning \citep{Koch2015SiameseNetworks}, visual representation learning \citep{Chen2020ContrastiveLearning} and image retrieval \citep{Hoffer2015DeepMetricLearning}.
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  FaceNet %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{FaceNet}
		\textit{FaceNet} \citep{Schroff2015FaceNet} is a convolutional neural network which learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity.  The method is based on learning a Euclidean embedding per image using a deep convolutional neural network.  Once each sample has been embedded through the network,  facial recognition can be achieved through the use of a simple $k$-NN classifier.  FaceNet  directly trains its outputs to be a compact 128-dimensional embedding  using a triplet-based loss function based on LMNN. FaceNet is based on a Inception network architecture and  optimizes a triplet loss formulated as:
		\begin{align}
		\mathcal{L} = ||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} -||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2} + \alpha, \quad \forall (f(x^{a}_{i}),f(x^{p}_{i}),f(x^{n}_{i})) \in \mathcal{T}
		\end{align} 
		where $\alpha$ is the margin enforced between positive and negative pairs and $\mathcal{T}$ is the set of all possible triplets in the training set.  The idea is that the triplet loss does not only promote similar faces to be close to each other in the embedding space, but also enforces a margin between every other face in the dataset.  To address the hard-mining problem,  triplets can be mined online out of mini-batches.  This provides a trade-off between speed and utility toward training.  FaceNet is currently SOA for the facial recognition problem, and in response, similar networks have been explored in a variety of applications, including: metric learning, image classification and image retrieval \citep{Hoffer2015DeepMetricLearning}.
		
		

	
	



