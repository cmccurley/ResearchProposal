\chapter{Background}

This chapter provides a literature review on Manifold Learning, including classic approaches, supervised and semi-supervised methods and uses of manifolds for functional regularization.  A review of the Multiple Instance Learning framework for learning from weak and ambiguous annotations is provided. Additionally, this chapter reviews the existing literature in classification over graphs, focusing heavily on the utilization of graph convolutional neural networks.  A brief overview of competency aware machine learning methods is also included.  Reviews describe basic terminology and definitions.  Foundational approaches are described and advances are addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Learning}

	\subsection{Overview and short description}
	

\subsection{Linear Methods}

	\subsubsection{Principal Component Analysis (PCA) (Kernel PCA)}

	\subsubsection{Multidimensional Scaling (MDS)}

\subsection{Nonlinear Methods}

	\subsubsection{Graph-based Methods}
		Nonlinear dimensionality reduction methods typically rely on the use of adjacency graphs.  These graphs represent data structure pooled from local neighborhoods of samples.  An overview of computational graphs, as well as the two most prominent methods for graph construction in manifold learning are presented.
		
		\subsubsection{Graphs}
		
		\subsubsection{$K$-Nearest Neighbor Graph}
		
		\subsubsection{$\epsilon$ Neighborhood Graph}
		
		\subsubsection{Geodesic Distance Approximation}
	
	\subsubsection{Isomap}
	
	\subsubsection{Locally Linear Embedding (LLE)}
	
	\subsubsection{Laplacian Eigenmaps}
	
	\subsubsection{Hessian Eigenmaps}
	
	\subsubsection{Diffusion Maps}
	
	\subsubsection{Sammon Mapping}
	
	\subsubsection{Latent Variable Models}
	
	\subsubsection{Competitive Hebbian Learning}
	
	\subsubsection{Deep Learning}


\subsection{Supervised and Semi-Supervised Approaches}

\subsection{Manifold Regularization}

\subsection{Matrix Factorization}


%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

\subsection{Overview and short description}

\subsection{Multiple Instance Concept Learning}

\subsection{Multiple Instance Classification}

\subsection{Multiple Instance Regression}

\subsection{Multiple Instance Learning on Graphs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Graph Classification %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Processing on Graphs}

\subsection{What can we do?, Inference, link prediction, etc}

\subsection{Classic Approaches}

\subsection{Graph Convolutional Neural Networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Outlier Detection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Competency Aware Overview}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Sensor Fusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Sensor Fusion}
%
%	Sensor fusion is the process of combining multiple sensors to provide complimentary or reinforcing information about a scene under observation \cite{hackett1990multisensorfusion,zhang2010multisourceremotingsensingfusion}.  Sensor fusion can operate on three levels: data or sample level, feature level, and decision level.  This  means that raw data, extracted features, or classifier prediction outputs can all be leveraged through a fusion process \cite{Ruta2000OverviewClassifierFusionMethods,zhang2010multisourceremotingsensingfusion}. While the process of sensor fusion is, in fact, general, this dissertation focuses primarily on combining sensors of varying types and resolutions with the goal of providing support for classification and detection decisions. In this context, sample-level fusion aims to combine data from multiple sources into single-resolution data which are expected to be more informative than any of the inputs, individually.  Fusion on this level involves applying a transformation on the data directly to project, map or co-register data obtained from multiple sources.  Feature-level fusion combines attributes computed from each sensors' collection, such as shape, texture, frequency, local statistics, or other discriminative characteristics exhibited in the data.  Feature-level fusion transforms the data into a single representation space which can be used instead of the original parameterizations for further processing.  Decision-level fusion takes outputs from multiple classifier or real-valued prediction models and derives a singular, fused output.  Decision-level fusion aims to provide a more accurate and/or descriptive classification or regression result through selection, transformation, or consensus of the models being combined \cite{hackett1990multisensorfusion, zhang2010multisourceremotingsensingfusion,Du2017Thesis,Tulyakov2008ReviewClassifierCombinationMethods, Du2016MIChoquetIntegralFusion,Du2018MultiResolutionSensorFusion}.  Sensor fusion has been used in a wide expanse of applications, including: remote sensing and hyperspectral image registration, classification and visualization, \cite{Zhang2014SemiSupManLearningFusion,Liao2016ManAlignmentHSI,Yang2016ManifoldAlignmentMultitemporalHSI,Hong2018CommonSubspaceLearningHSI}, object detection \cite{Davenport2010JointManifoldsDataFusion}, explosive hazard detection \cite{Gader2004ChoquetIntegralLandmine,Smith2017ChoquetIntegralLandmine,Keller2001PredictiveSensorFusion,Gunatilaka2001FusionNoncoincidentlySampledLandmine,Frigui2010ContextDependentFusionLandmine},  handwriting recognition \cite{Gader1996FusionHandwrittenLetters}, pose estimation \cite{Navaratnam2007JointManifoldSemiSupRegression}, automatic target recognition in mid-wave infrared/ sonar imagery \cite{Shen2018ManifoldSensorFusionImageData}, land-use classification \cite{Hong2019LearnableManifoldAlignment}, super-resolution, medical and fault diagnosis, defense, meteorology, environmental monitoring, and economic forecasting   \cite{zhang2010multisourceremotingsensingfusion,Zitova2003SurveyImageRegistrationMethods}. 
%	
%	\textbf{Challenges in sensor fusion}
%	Features might not have the same dimensionality between sensors. The number/ length of features/ physical qualities vary  between modalities.
%	Data from different sensors must be put into equivalent forms to allow for fusion.  In order for data from multiple sources to be fused, there must be some method to relate data points from one sensor with corresponding data points from the other sensors.  Before sensor measurements can be combined, we must ensure that the measurements represent the same physical entity. Therefore, we need to check the consistency of sensor measurements.  Since each sensor is sensitive to a different modality, multiple sensors not only can provide multiple views of objects, but they can also impose more constraints to reduce the search	space during matching.
%
%
%	
%	Additionally, data fusion can provide confidence bounds on sensor's collections \cite{hackett1990multisensorfusion,Mohandes2018ClassifierCombinationTechniquesReview}.  For example, HSI sensors measure the radiance across a broad range of wavelengths at varying ground locations.  The measured radiance is a combination of radiation that is reflected and/or emitted by materials on the ground.  This inherently implies that environmental characteristics such as atmospheric conditions and diurnal cycle affect the amount of radiance that can be captured by the sensor \cite{Zare2008Thesis}.  Therefore, incorporation of meta-data such as time-of-day and weather conditions can provide bounds on the uncertainty contained in a given measurement and reinforce when to trust readings from particular spectral bands (i.e. IR during night).   These examples demonstrate how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce measurement uncertainty or to obtain a more complete understanding of a scene, thus aiding in classification or segmentation decisions.
%	\newline
%	
%	Information fusion approaches make two typical assumptions: (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered \cite{Shen2016SpatioTemporalSpectralFusion}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \cite{hackett1990multisensorfusion,Butenuth2007HeterogeneousGeospatialData}.  Assumption (2) states that precise training labels are available for each data point \cite{Du2017Thesis}. 
%	\newline 
%	
%	Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial, spectral, or temporal resolutions, it is usually non-trivial to convert all data to the same resolution or to map to the same grid and existing co-registration approaches often result in loss of sensor-specific information \cite{Shen2016SpatioTemporalSpectralFusion,Brigot2016CoregistrationForestRemoteSensingImages}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process since high-level information such as time-of-day, weather, environmental-setting (e.g. urban or forest), etc., would likely be collected at different spatial or temporal rates and would probably not align easily.  Typical approaches would address this problem by throwing out data points to match the lowest sampling rate.  However, this mapping is often noisy and results in loss of sensor-specific information.  This loss is often detrimental since classification improvement may only be achieved if the total information uncertainty in a problem is reduced \cite{Ruta2000OverviewClassifierFusionMethods}.  Moreover, it becomes difficult to distinguish between errors which evolve during the registration process and actual physical differences in a scene \cite{Zitova2003SurveyImageRegistrationMethods}.
%	\newline 
%	
%	\subsection{Classic Approaches}
%	
%%		\subsubsection{Choquet Integral}
%%		
%%		\subsubsection{Hierarchical Mixture of Experts}
%%		
%%		\subsubsection{Deep Learning}
%%		
%%		\subsubsection{Graph-Based}
%		
%	\subsection{Feature-level Fusion}
%	
%	\subsection{Manifold Alignment}
%		Manifold alignment is the process of matching two or more seemingly disparate datasets by mapping them to a joint latent space, while both preserving the qualities of each dataset and highlighting their similarities \cite{Wang2011ManifoldAlignment,Liao2016ManAlignmentHSI,Stanley2019ManAlignmentFeatureCorrespondence}.  
%	
%%	\subsection{Information Loss}
%%	
%%	\subsection{Geocoding}
%%	
%%	\subsection{Similarity Measures}
%%	
%%	\subsection{Transformation, Interpolation, Re-sampling}
%%	
%%	\subsection{Conflation}
%	

