\chapter{Background}

This chapter provides a literature review of the Multiple Instance Learning framework for learning from weak and ambiguous annotations.  A review is provided on Manifold Learning, including classic approaches, supervised and semi-supervised methods and uses of manifolds for functional regularization. Additionally, this chapter reviews the existing literature on metric embedding, focusing heavily on the utilization of contrastive and triplet-based loss evaluation.  Reviews describe basic terminology and definitions.  Foundational approaches are elaborated and advances are addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

Multiple Instance Learning (MIL) was originally proposed in \citep{Dietterich1996AxisParallelRectangles} as a method to handle inherent observation difficulties associated with drug activity prediction.  This problem, among many others, fits well into the framework of MIL where training labels are associated with sets of data points, called \textit{bags} instead of each individual data points, or \textit{instances}.  Under the \textit{standard MIL (SMI) assumption}, a bag is given a ``positive" label if it is known that  \textit{at least one} sample in the set represents pure or partial target.  Alternatively, a bag is labeled as ``negative" if does not contain any positive instances \citep{Carbonneau2016MILSurvey}.  Let $\bm{X}=[\bm{x}_1,\dots, \bm{x}_N] \in \mathbb{R}^{D \times N}$ be training data where $D$ is the dimensionality of an instance, $\bm{x}_n$, and $N$ is the total number of training instances.  The data is grouped into K \textit{bags}, $\bm{B} = \{\bm{B}_1, \dots, \bm{B}_K\}$, with associated binary bag-level labels, $\mathcal{L} = \{L_1, \dots, L_K \}$ where 
\begin{align}
	L_k = \begin{cases} 
	+1, & \exists \bm{x}_{kn} \in \bm{B}^{+}_{k} \ni  l_{kn} = +1\\
	-1, & l_{kn} = -1 \quad \forall \bm{x}_{kn} \in \bm{B}^{-}_{k} 
	\end{cases}
\end{align} and $\bm{x}_{kn}$ denotes the $n^{th}$ instance in positive bag $\bm{B}^{+}_{k}$ or negative bag $\bm{B}^{-}_{k}$ \citep{Zare2016MIACE} and $l_{kn} \in \{ -1, +1\}$ denotes the instance-level label on instance $\bm{x}_{kn}$.  Figure \ref{fig:bag_eg} demonstrates the concept of MIL bags.  The objective of learning under MIL is, given only bag-level label information, to fit a model which can perform one of the following tasks: classification, regression, ranking or clustering \citep{Carbonneau2016MILSurvey}.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[Multiple instance learning bags.]{Placeholder for examples of positive and negative bag concepts}
		\label{fig:bag_eg}
	\end{figure*}
\end{center}

\noindent

\subsection{Multiple Instance Learning with Manifold Bags}
It should be noted that while the aforementioned MIL formulation is standard in the literature, Baenko et al. introduced the framework of MIL using manifold bags  \citep{Babenko2011MILManifoldBags}.  Their work claimed that instead of finite sets of instances, bags are inherently better represented as low-dimensional manifolds in a high-dimensional feature space.  They considered the scenario of classifying whether or not an image contained a face.  In this scenario, the entire image was considered to be a bag, and patches of the image were considered as individual instances.  It was assumed that the collection of instances collectively formed a low-dimensional manifold.  This work laid the groundwork for learning with manifold bags and proved PAC learnability under this framework.  While it was worth mentioning this work as it aligns with the objective of manifold learning discussed in the proposed work, the remainder of this literature review focuses solely on the standard MIL formulation.

\subsection{Tasks}
Multiple instance learning in the literature can be broadly categorized into four tasks: classification, regression, ranking and clustering \citep{Carbonneau2016MILSurvey}.  MIL classification can be performed at either the bag or instance level.  The goal is to assign a class label to either the set of instances or the individual instances themselves.  MIL regression consists of assigning a real-valued label to a bag (or instance) instead of a class label.  A few methods have been proposed to rank bags or instances instead of assigning a class label or score.  This problem differs from regression because the goal is not to obtain an exact real-valued label, but to compare the magnitude of scores to perform sorting.  The clustering task consists of finding clusters or structure among a set of unlabeled bags.  Alternatively, clustering can be performed within bags in attempt to distinguish between the positive and negative instances.  Classification and ranking are the most pertinent tasks to the proposed work, and will thus be discussed in detail.

\subsection{Multiple Instance Classification}
The standard supervised learning task is to learn a classifier based on a training set of feature vectors, where each feature vector is paired with and associated class label.  In the \textit{Multiple Instance Classification} (MIC) task, the goal is to learn a classifier based on a training set of bags, where each bag is a set of feature vectors known as instances.  In this setting, each bag is paired with an associated binary class label; however, the labels of each instance in the sets are unknown. 
\subsubsection{Space Paradigms} The MIC problem has been formulated under three paradigms: Instance-Space, Bag-Space and Embedded-Space \citep{Amores2013MIClassification}.  Each paradigm is categorized according to how information presented in the MI data is exploited.  In the \textit{Instance-Space} (IS) paradigm, the discriminative information is considered to lie at the instance-level.  An instance-level classifier is trained to separate the true positive instances from the true negative instances.  Given an instance-level classifier, a bag-level classifier can be developed by simply aggregating the instance-level scores in a test bag.  This paradigm is based on local, instance-level information.  In the \textit{Bag-Space} (BS) paradigm, the discriminative information is considered to lie at the bag-level.  Under this paradigm, each bag is treated as a whole entity, and the learning process discriminates between entire bags.  This paradigm is based on global, bag-level information.  Considering that the bag space is a non-vector space, current BS methods make use of non-vectorial learning techniques which define distance metrics as a way to compare bags.  In the \textit{Embedded-Space} (ES) paradigm, each bag is mapped to a single feature vector which captures the relevant information about the entire bag.  Consequently, the learning problem transforms into a standard supervised problem, where each feature vector is paired with an associated (bag-level) label.  Similar to the BS paradigm, the ES paradigm is also based on global, bag-level information.  However, the difference between the two paradigms lies in the way bag-level information is extracted.  In the BS paradigm, information is extracted implicitly through the definition of the distance or kernel function.  Alternatively, information is extracted explicitly in the ES paradigm through the definition of the mapping function from the bag-space to the vector space.  Figure \ref{fig:mil_classification_space_paradigm} demonstrates the differences between standard supervised classification and the three MIL classification space paradigms. Apart from space paradigm, MIL classification methods in the literature can be organized according to the their primary approaches toward learning. A review of prominent MIL classification methods in the literature is provided, categorized according to learning approach.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[MIL classification space paradigm.]{MIL classification space paradigm.}
		\label{fig:mil_classification_space_paradigm}
	\end{figure*}
\end{center}

\subsubsection{MIL Classification Approaches}
MIL classification approaches in the literature can be categorized by the underlying principle used for learning.  The categories discussed in this review are: Axis-Parallel Concepts, Maximum Likelihood, Distance-Based, Maximum Margin,  Deep Learning, Probabilistic Graphical Methods, Dictionary Learning and Ensembles of Classifiers.  Each approach is reviewed in the following.

\paragraph{Learning Axis-Parallel Concepts:}
Learning Axis-Parallel Concepts are among the first group of methods used to solve MIL problems \citep{Dietterich1996AxisParallelRectangles}.  The foundation of this category is based on the method of \textit{Axis-Parallel Rectangles} (APR), proposed by Dietterich et al. in the 1990's.  An axis-parallel hyper-rectangle is a set of thresholds (one for each feature dimension) that is used to discriminate between two classes. It can also be viewed as an overlap or aggregation region of true positive instances in the feature space. The goal of APR is to find an axis-parallel hyper-rectangle in the feature space to represent the target concept \citep{Ghaffarzadegan2018MILVAE, Bocinsky2019SPIEMIACEInitialization, Jiao2017Thesis}.  A disadvantage of these approaches is that they can fail to neglect a majority of data in a large bag sample.    

\paragraph{Maximum Likelihood:}
Similar to traditional \textit{Maximum Likelihood Estimation} (MLE), the objective of maximum likelihood in the MIL setting is to train a classifier which maximizes the likelihood of the data.  The most prominent of these methods is \textit{Diverse Density} (DD) \citep{Maron1998DiverseDensity}, which considers each bag as a manifold describing the instances.  The goal of learning is to discover a prototype  from the training data which maximizes the DD measure.  Diverse Density is essentially a measure of the intersection of positive bags minus the union of the negative bags.  By maximizing DD, a point of intersection can be found which is close to at least one instance  from the positive bags while being as far as possible from all points in the negative bags \citep{Ghaffarzadegan2018MILVAE}.   Zhang et al. later proposed \textit{Expectation-Maximization Diverse Density} (EM-DD) \citep{Zhang2002EMDD}. EM-DD extends DD by viewing the relationships between instances and their corresponding bag's labels as latent variables.  In other words, it asks which instance in the set corresponds to the bag's label \citep{Du2017Thesis}. In the \textit{E-step}, an the instance from each bag which is the most probable for providing the bag its label is selected.  Then in the \textit{M-step}, a new concept point is found by maximizing DD with gradient ascent.  This process is iterated until a stopping criteria is met, however, it will stop naturally as there only a finite number of instance combination that the algorithm can pick.  Multiple instance maximum likelihood approaches have been used in a variety of problems, such as: stock selection, person identification, scene classification, hyperspectral target classification and explosive hazard detection \citep{Maron1998DiverseDensity, Maron1998MILSceneClassification, Zare2016MIACE, Zare2015MILLandmineEMI, Bocinsky2019SPIEMIACEInitialization, Mccurley2019SPIEWEMIComparison}.    

\paragraph{Distance-Based:}
A simple approach for classification in the supervised learning paradigm is to compare distances of test points to samples in the training set.  Gartner et al. defined the MI-Kernel \citep{Gartner2002MIKernels} by regarding each bag as a set of features and applying a set kernel directly to compare similarity. The \textit{Citation-$k$NN} algorithm \citep{Du2017Thesis,Carbonneau2016MILSurvey,Zhou2003MIEnsemble,Kim2010LocalDRMIL} is a nearest-neighbor style classifier which borrows the idea of scientific citers and references when considering a bag's label.  \textit{References} are simply the nearest neighbors of a bag, while \textit{citers} are the bags which have the query bag in it's $C$-nearest neighbors.  The vanilla citation-$k$NN uses the \textit{minimal Hausdorff distance} to measure bag similarity.  The Hausdorff distance between two bags $\bm{B}$ and $\bm{B}'$ is defined as:
\begin{align}
	H(\bm{B},\bm{B}') = \max \{ h(\bm{B},\bm{B}'),h(\bm{B}',\bm{B}) \}
\end{align}
\noindent
and
\begin{align}
	h(\bm{B},\bm{B}') = \max_{\bm{b}\in \bm{B}} \min_{\bm{b}' \in \bm{B}'}||\bm{b}-\bm{b}' ||
\end{align}
\noindent
where $\bm{b}$ and $\bm{b}'$ are instances in bags $\bm{B}$ and $\bm{B}'$, respectively.  Intuitively, the minimal Hausdorff distance is the smallest value $H$ such that every instance in $\bm{B}$ has a point of $\bm{B}'$ within distance $H$, and every instance in $\bm{B}'$ has an instance in $\bm{B}$ within a distance of $H$. Variants of citation-$k$NN include Bayesian citation-$k$NN and fuzzy citation-$k$NN \citep{Du2017Thesis}.  Additionally, the minimal Hausdorff distance has been substituted for the Earth Mover's Distance (EMD), Chamfer distance and specialized bag-distance kernels \citep{Amores2013MIClassification}.  

Two alternative distance-based MIL classifiers are MIGraph and miGraph \citep{Zhou2009miGraph}.  Both methods consider bags as graphs to capture interdependence between instances. The distance measures used to compare bags is what separates the two methods. MIGraph explicitly maps every bag to an undirected graph and uses a graph kernel or metric such as graph edit distance to discriminate between positive and negative bags.  Alternatively, miGraph implicitly constructs graphs by defining affinity matrices between instances and uses clique information to help distinguish positive from negative bags. 


\paragraph{Maximum Margin:}
The concept of weakly-supervised maximum margin learning has been explored under a variety of techniques, the primary being \textit{Support Vector Machines} (SVM) and \textit{Metric Embedding}, which will be discussed in Section \ref{sec:MetricEmedding}.  Andrews et al. proposed two SVM methods under the MIL framework, namely, mi-SVM and MI-SVM, for instance-level and bag-level classification,  respectively \citep{Andrews2011MISVM,Carbonneau2016MILSurvey,Du2017Thesis,Jiao2017Thesis}.  The goal of a SVM is to train a classifier to maximize the margin between a small subset of training examples (called \textit{support vectors}) and the decision hyper-plane \citep{Murphy2012}.  Both MIL SVM methods follow a similar procedure to EM-DD.  In MI-SVM, each positive bag is represented by a single instance which is considered to be the ``most positive instance" in the bag. Optimization alternates between learning a decision boundary with SVM and selecting the positive bag representatives given the new classifier. In contrast, mi-SVM considers all points in the positive bags while learning the decision boundary.  Every point in the positive bags is provided a positive label.  The instance labels are refined iteratively under the constraint of the standard MIL assumption, that at least one instance from each positive bag must lie on the positive side of the decision hyper-plane \citep{Cao2016VehicleDetectionMIL}.  MissSVM is a semi-supervised max-margin approach which considers the instances in positive bags as unlabeled, and enforces a constraint that at least one of them is positive \citep{Zhou2007MissSVM}.  DD-SVM  and \textit{Multiple-Instance Learning via Embedded Instance Selection} (MILES) are both embedding-space methods which convert MIL into a standard supervised problem \citep{Chen2006MILES}.  DD-SVM trains an SVM in a feature space constructed from a mapping defined by the local maximizers and minimizers of the DD function.  MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. A 1-norm SVM is applied to simultaneously select the important features and construct classifiers \citep{RuizMunoz2015MILBirdsongClassification}. Additionally, Xiao et al. developed an ensemble method in which the base classifier enforces a margin between optimal hyper-spheres while enclosing at least one instance from each positive bag inside the ball \citep{Xiao2017SphereMIL}. 


\paragraph{Neural Networks and Deep Learning:} \label{sec:mil_deep_learning}

Recent developments in deep learning have also made their way into the MIL literature under the assumption that useful features can be learned by the networks using only bag-level labels.  Gao et al. used \textit{convolutional neural networks} (CNN) with count-based region selection to perform weakly-supervised object localization \citep{Gao2017CountGuidedWeaklySupervisedLocalization}.  Ilse et al. modeled the MIL problem as learning the Bernoulli distribution over the bag labels, where the label probability was parameterized by a neural network \citep{Ilse2018AttentionBasedDeepMIL}.  Ilse's approach employs attention-based MIL pooling as a way to visualize which instances the network selects as being positive.  A multi-instance multi-scale CNN to detect regions of interest in medical images \citep{Li2019CNNMedicalImageClassification} was proposed by Li et al.  Li's work introduced ``top-k pooling" to aggregate feature maps of varying scales and spatial dimensions, allowing the model to be trained using weak, MIL annotations.  Wang et al. explored the use of a U-Net segmentation network architecture to obtain pixel-level ground-cover classification from image-level labels \citep{Wang2020WSDeepLearningRemoteSensing}.  A discriminative \textit{variational autoencoder} (VAE) was used by Ghaffarzadegan to maximize the difference between latent representations of positive and negative instances \citep{Ghaffarzadegan2018MILVAE}.  Tu et al. developed an end-to-end \textit{graph neural network} which treats each bag as a graph \citep{Tu2019MILGraphNN}.  Each bag is passed through the network to obtain a feature representation encapsulating the structural information present in the bag.  Deep learning MIL approaches have been explored in a wide variety of applications, including: retinal image classification \citep{Tu2019MILGraphNN}, histopathology classification \citep{Ilse2018AttentionBasedDeepMIL}, object localization \citep{Gao2017CountGuidedWeaklySupervisedLocalization} and region-of-interest proposal in medical images \citep{Li2019CNNMedicalImageClassification}.


\paragraph{Probabilistic Graphical Methods:}
\textit{Probabilistic graphical models} (PGMs) are powerful tools used to capture inter-relations between random variables and learn structured models. In some problems, data exhibits an underlying structure between instances or bags that is more complex than simple co-occurrence.  Capturing this structure may lead to better classification performance \citep{Carbonneau2016MILSurvey}.  Deselaers and Ferrari proposed a multi-instance conditional random field, MI-CRF \citep{Deselaers2010MICRF}.  In this method, bags are modeled as nodes in a CRF, where each node can take one of the instances in the bag as its state.  Classification corresponds to selecting one instance (positive bag) or selecting no instances (negative bag).  Instance selection is formulated as inference in the CRF.  This lets all bags to be considered jointly in training and testing.  Thus, bags are jointly classified based on unary instance classifiers and pairwise dissimilarity measurements.  Hajimirsadeghi and Mori introduced a max-margin classification scheme using Markov networks \citep{Hajimirsadeghi2017MIClassificationMarkovNetworks}.  Yuksel et al. developed a multiple instance \textit{Hidden Markov Model} (MI-HMM) for use in landmine detection \citep{Yuksel2015MIHMMLandmine,Yuksel2015MIHMMLandmine2}.  In this scenario, each bag is associated with a label, however, the bags can be composed of time sequences of variable length.  A noisy-OR relationship is assumed between the sequences within each bag and the joint probability of the bags of sequences and the corresponding labels for the bags is maximized with a stochastic expectation maximization.  PGMs have proven to work well on MIL problems exhibiting time series data and data with structural dependence.

\paragraph{Dictionary Learning:}
\textit{Dictionary Learning} is a method of learning how to reconstruct a dataset from a much smaller set of building blocks called \textit{atoms} \citep{Cook2015Thesis}.  Given a training data set, the goal is to learn the set of atoms and sparse weights which reconstruct the data.  At test, a bag or instance can be classified based on the atoms which effectively reconstruct the sample.  Multi-Instance Dictionary Learning (MIDL) uses bag-level information with a least angle regression to alternatively learn a dictionary which represents the training data and regression weights for classification.  Max-Margin Multiple Instance Dictionary Learning (MMDL) adopts the idea of the bag of words (BoW) model and trains a set of linear SVMs as codebooks \citep{Jiao2017Thesis}. Functions of Multiple Instances (FUMI) is a supervised technique which tries to learn target and background dictionaries such that a target instance can be written as a linear combination of a single target concept and multiple background atoms .  This formulation considers target instances that may contain portions of background signature, as with sub-pixel target detection in hyperspectral imagery.  A known problem with FUMI is that it does not work well with noisy labels.  To account for this problem, extended Functions of Multiple Instances (eFUMI), uses bag-level labels to identify the data \citep{Jiao2017Thesis,Cook2015Thesis}.  A function is built in to determine whether a point labeled as target actually contains a portion of the target dictionary atoms. Task-driven extended Functions of Multiple Instances (TD-eFUMI) adopts the MI aspect of eFUMI and Task-Driven Dictionary learning to simultaneously learn target and background dictionaries in conjunction with a classifier \citep{Cook2016LandmineTaskDriveneFUMI}. Zare et al. proposed the Multiple Instance Adaptive Cosine/Coherence Estimator and Spectral Matched Filter (MI-ACE and MI-SMF) \citep{Zare2016MIACE}.  These methods learn discriminative prototypes under the multiple instance learning framework to classify instances.  However, these methods inherently consider only a single target concept.  In order to capture intra-class variation among target instances, Multi-Target MI-ACE/SMF were proposed \citep{Bocinsky2019Thesis}.  Finally, Jaio et al. presented Multiple Instance Hybrid Estimator (MI-HE) to learn multiple target and background concepts by maximizing the probability that positive bags are labeled as positive and negative bags are labeled as negative under a noisy-OR model \citep{Jiao2018MIHE2,Bocinsky2019Thesis}.  Multiple instance dictionary learning problems have been applied successfully to sub-pixel hyperspectral target detection and landmine detection tasks \citep{Bocinsky2019Thesis, Zare2015MILLandmineEMI, Zare2016MIACE, Cook2016LandmineTaskDriveneFUMI, Jiao2018MIHE2}. 


\paragraph{Ensembles of Classifiers:}
\textit{Ensemble learning} paradigms train multiple versions of a base classifier and aggregate the results to achieve a stronger classifier than any of the individuals.  Ensemble methods are typically broken into two realms: \textit{bagging} and \textit{boosting}.  Bagging employs boostrap sampling to generate several training subsets from the original training set, then trains a learner on each data subset.  The predictions from each component learner are aggregated in order to provide a final class score.   Zhou et al. studied whether ensemble learning paradigms could be used to enhance MI learners by applying bagging on base MI learners, namely, Diverse Density and Citation K-NN \citep{Zhou2003MIEnsemble}.  Random Forest classifiers operate under the bagging paradigm and use a technique called \textit{divide-and-conquer}.  These classifiers deploy an ensemble of decision trees which iteratively divide the feature space and make simple thresholding decisions.  The predicted class labels provided by each tree in the forest are combined to give a final class label.  Leistner et al. proposed MIForest which combines the random forest learning algorithm with MIL \citep{Leistner2010MIForests}.  Since only bag-level labels are known, MIForest treats the label of each instance as a random variable defined over a space of probability distributions.  The instance labels are disambiguated by iteratively searching for distributions which minimize the overall classification error.  

The other paradigm of ensemble learning is called boosting. The goal of boosting is, using the entire training set, to find a  weighted combination of weak learners (that may perform only slightly better than chance), such that the combination produces a strong classifier with high classification accuracy \citep{Zhang2006MIBoosting}. Several MI boosting approaches have been proposed in the literature.  Section \ref{sec:MILBoost} discusses a popular variant, MIL-Boost, in detail.


\subsection{Multiple Instance Boosting (MIL-Boost)} \label{sec:MILBoost}

\paragraph{Gradient Boosting Overview}
In the standard supervised learning setting, the goal of binary classification is to learn a classification function $h:\mathcal{X} \rightarrow \mathcal{L}$ which maps data in $\mathcal{X}$ to a binary class label $\mathcal{L} \in \{-1,+1\}$.  The objective of boosting is to train a classifier of the form
\begin{align}
	\bm{h}(\bm{x}) = \sum_{t=1}^{T}\alpha_{t}h_{t}(\bm{x})
\end{align}
\noindent
where each $h_{t}:\mathcal{X} \rightarrow \mathcal{L}$ is a \textit{weak learner} whose performance may only be slightly above chance, and the weights $\alpha_{t}$ are each weak learners' relative importance \citep{Babenko2008MIBoosting}.  \textit{Boosting} combines multiple weak learners into a single \textit{strong} classifier with low classification error.  This is also known in the literature as \textit{ensembling}.  In each phase of training, samples classified incorrectly are given more weight in order to improve classification performance in the next iteration.  The response of each weak classifier $h_{t}$ is given as the maximum response over all instances in the training set:
\begin{align}
	h_{t} = \arg \max_{h} \sum_{n=1}^{N}w_{n}h(\bm{x}_{n})
\end{align}

\paragraph{MIL-Boost}
Boosting under the MIL framework was originally proposed by Zhang et al. and is known as \textit{MIL-Boost} \citep{Zhang2006MIBoosting}.  Under MIL, it is assumed that every instance has a true label $l_{kn} \in \{ -1,+1 \}$.  A bag is labeled positive if at least one of its instances are positive, and a bag is labeled negative if every instance is negative.  This means that the label of a bag is provided as the max label over all instances in the bag:
\begin{align}
	L_{k} = \max_{n}(l_{kn})
\end{align}
\noindent
In this setting, the goal is to learn a classifier $\bm{h}$ using only bag-level labels such that $\max_{n}(\bm{h}(\bm{x}_{kn})) = L_{k}$. The score given to a sample is $l_{kn} = \bm{h}(\bm{x}_{kn})$ and $\bm{h}(\bm{x}_{kn}) = \sum_{t=1}^{T}\alpha_{t}h_{t}(\bm{x}_{kn})$ which is a weighted sum of predicted labels from each of the weak classifiers.  Obviously, the sign of the prediction from the strong classifier provides the class label for the instance.

A natural objective is to minimize the negative log-likelihood between instances and their predicted labels.  This can also be done at the bag-level.  The probability that an instance is positive is given by the standard logistic function
\begin{align}
	p_{kn} = \frac{1}{1+\exp(-l_{kn})}
\end{align}
\noindent
and the probability that bag $k$ is positive is given by a ``noisy OR", $p_{k} = 1 - \prod_{n \in k}(1- p_{kn})$.  Therefore, the likelihood assigned to a set of training bags is given by:
\begin{align}
	\mathcal{L}(\bm{h}) = \prod_{k=1}^{K}p_{k}^{L_{k}}(1-p_{k})^{(1-L_{k})}
\end{align}
\noindent
where $L_{k} \in \{0, 1\}$ is the actual label of the bag.

Following the idea of gradient boosting, the weight on each training instance is given as the derivative of the log-likelihood with respect to a change on the score given to the instance.  Thus gradient descent can be used to update the the strong classifier.  Pseudo-code for MIL-Boost is provided in Algorithm \ref{alg:mil_boost}.

\begin{algorithm}
	\caption{MIL-Boost}
	\label{alg:mil_boost}
	\begin{algorithmic}[1]
	\Require {Dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$}
	\For{$t=1$ to $T$}                    
	\State {Compute weights $w_{kn} = -\frac{\mathcal{\partial L}}{\partial \bm{h}_{kn}}$}
	\State \begin{varwidth}[t]{\linewidth}
	{Train weak classifier $h_{t}$ using weights $|w_{kn}|$ } \par
		\hskip\algorithmicindent $h_{t} = \arg \min_{h}\sum_{kn}\bm{1}(h(\bm{x}_{kn} \neq L_{k}))|w_{kn}|$
	\end{varwidth}
	\State \begin{varwidth}[t]{\linewidth}
		{Find $\alpha_{t}$ via line search to minimize $\mathcal{L}(\bm{h})$} \par
		\hskip\algorithmicindent $\alpha_{t} = \arg \min_{\alpha} \mathcal{L}(\bm{h} + \alpha h_{t})$
	\end{varwidth}
	\State Update strong classifier $\bm{h} \gets \bm{h} + \alpha_{t} h_{t}$
	\EndFor
	\end{algorithmic}
\end{algorithm}

Each round of boosting consists of updating the weak learners according to the instances they misclassified and updating the strong classifier according to the new weights calculated over the weak learners.  The goal of MIL-Boost is to be able to correctly classify each instance in a test bag as being positive or negative such that the label of the bag is given as the maximum label over all instances in the bag.

\subsection{Multiple Instance Ranking}\label{sec:MI_Ranking}

Another primary task in MIL is \textit{ranking} \citep{Carbonneau2016MILSurvey}.  The ranking problem is different from classification because, instead of providing a binary class label, the objective is to order a set of bags \citep{Bergeron2008MIRanking,Bergeron2012FastBundleMILRanking} or instances \citep{Hu2008MIRanking} according to preference for a particular task.  Ranking is a key task under \textit{Preference Learning}, or learning to predict preferences on a set of alternatives, which are often represented in the form of an order relation \citep{Furnkranz2003RankingSummary}.  The statement that $\bm{x}$ is preferred to $\bm{x}'$ can be simply expressed as an inequality relation $f(\bm{x}) > f(\bm{x}')$, where $\bm{x}$ and $\bm{x}'$ are instances, and $f$ defines a preference function.  For example, given a news story about the Olympics, one might prefer to give it the label ``sports" rather than ``politics" or ``weather".  Alternatively, one might prefer to label one bag or instance as ``positive" over another.  In machine learning, the preference learning problem is often analyzed in two cases: \textit{learning instance preference} and \textit{learning label preference} \citep{Chu2005LearningPreferencesWithGaussianProcesses}.  Under the scenario of learning instance preferences, the training set consists of a set of pairwise preferences between instances.  The objective is to learn the underlying ordering from the set of pairwise distances (such as ordering bags from the ``most positive" to ``least positive").  Alternatively, the goal of label preference learning is to order a pre-defined set of labels for each individual instance (such as ordering the preference of ``positive" or ``negative" on each individual bag or instance) \citep{Dekel2004Ranking,Aiolli2004LearningPreferences}.   

Ranking under the multiple instance framework was proposed in \citep{Bergeron2008MIRanking} for predicting hydrogen atom grouping in computational chemistry. The method proposed by Bergeron et al. is called MIRank \citep{Bergeron2012FastBundleMILRanking}.  MI ranking differs from MIC in that the label for each bag is not known.  Instead, the MI ranking algorithms are provided preference information between pairs of bags. MIRank considers the partial ranking problem, which inherently exhibits three levels of structure: items (instances) belong to bags and bags belong to boxes.  The objective is to learn a ranking function that can identify the preferred bag in each box.  A concrete example where this framework can be applied is in learning positive target concepts.  For a set of video frames (boxes), one may want to predict the smaller image chips (bags) that have the highest probability of containing a target object (positive instances).  MIRank uses a linear prediction function to rank instances in individual bags.  The ranking of instances $\bm{x}_{i}$ and $\bm{x}_{j}$ in bags $I$ and $J$ is guided by the preference information between $I$ and $J$. The work in \citep{Hu2008MIRanking} introduced \textit{multiple-instance ranking} based on a max margin framework.  In this setting, images were represented by sets of regions and the goal was to rank images according to relevance to a keyword.  Assuming the preference relationship that $\bm{x}_m$ is preferable to $\bm{x}_n$ is denoted by $\bm{x}_m \succ \bm{x}_n$, the goal is to induce a ranking function $f:\mathbb{R}^{D} \to \mathbb{R}$ that fulfills the set of constraints
\begin{align}
	\forall \bm{x}_m \succ \bm{x}_n: f(\bm{x}_{m}) > f(\bm{x}_{n})
\end{align}
\noindent
The value of $f(\bm{x}_n)$ is referred to as the ranking score of $\bm{x}_n$ and is typically a linear function $f(\bm{x}_n) = \langle \bm{w},\bm{x}_{n} \rangle = \bm{w}^{T}\bm{x}_{n}$.  Adding slack variable $\zeta_{mn}$, the optimization problem can be solved with with the following objective:
\begin{align}
	\begin{split}
	\min_{\bm{w},\bm{\zeta}} \quad &\frac{1}{2} ||\bm{w} ||^{2} + \gamma \sum_{m,n} \zeta_{mn} \\
	\text{s.t.} \quad &\forall \bm{x}_m \succ \bm{x}_n: \langle \bm{w},\bm{x}_{m} \rangle \geq \langle\bm{w},\bm{x}_{n} \rangle + 1 -\zeta_{mn}\\
	&\forall m,n: \zeta_{mn} \geq 0\\
	\end{split}
\end{align}
\noindent
which is referred to as a \textit{ranking SVM}.  Assuming the optimal solution is $\bm{w}^{*}$, the ranking score of a test point $\bm{x}'$ is given as $f(\bm{x}') = \langle \bm{w}^{*}, \bm{x}' \rangle$.  This framework assumes each sample $\bm{x}_{n}$ is a single instance.  In multiple-instance ranking, we are given a set of preference relations between bag pairs and it is assumed that the score of a bag $\bm{B}_{k}$ is determined by the scores of the instances it contains
\begin{align}
	h(\bm{B}_{k}) = h \left( \{ f(\bm{x}_{n})\}_{n=1}^{N_{k}} \right)
\end{align}
\noindent
Under this formulation, the objective can be re-written to consider bag scores as
\begin{align}
\begin{split}
\min_{f\in\mathcal{H},\bm{\zeta}} \quad &\frac{1}{2} ||f ||^{2}_{\mathcal{H}} + \gamma \sum_{m,n} \zeta_{mn} \\
\text{s.t.} \quad &\forall \bm{B}_m \succ \bm{B}_n: h(\bm{B}_{m}) \geq h(\bm{B}_{n}) + 1 -\zeta_{mn}\\
&\forall m,n: \zeta_{mn} \geq 0\\
\end{split}
\end{align}
\noindent
A bag's score is determined by the scores of its instances.  Different functions for providing a bag score have been investigated, including using the max of instance scores, the mean of  instance scores and the softmax of instance scores.

Besides MIRank and multiple-instance ranking, Asif et al. recently proposed pyLEMMINGS, which implements locally linear MI ranking by learning a large margin discriminant function from bags with corresponding integer rankings  \citep{Asif2017LargeMarginMIRanking}.  While ranking has been successfully applied to text information retrieval, image retrieval \citep{Hu2008MIRanking} and bioinformatics \citep{Asif2017LargeMarginMIRanking}, ranking under the MIL framework is still a relatively unexplored area of research.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Learning}

Real-world remote sensing data such as hyperspectral imagery, ground-penetrating radar scans and sonar signals are naturally represented by high-dimensional feature vectors.  However, in order to handle such real-world data adequately, its dimensionality usually needs to be reduced \citep{VanDerMaaten2009DRReview,Belkin2004SemiSupLearningRiemannianManifolds}. The problem considered in this work is discovering feature representations that promote class discriminability for target or anomaly detection.  This is typically achieved in one of two ways.  First, features can be projected into a high-dimensional space (such as a Kernel Hilbert Space) using a kernel function. The second option, which is the focus of this work, is to transform the data into a new (often lower-dimensional) coordinate system which optimizes feature representations for discrimination \citep{Vural2018StudySupervisedManifoldLearning}.


The application of \textit{dimensionality reduction} (DR) has proven useful in myriad applications in the literature, such as: visualization of high-dimensional data, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and reducing the effects of the Curse of Dimensionality \citep{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.  In classification of object entities, it is often assumed that classes can be described by an \textit{intrinsic} subset of representative features which demonstrate geometrical structure \citep{Belkin2006ManReg}. These structures are called intrinsic \textit{manifolds}, and they represent the generating distributions of class objects exactly by the number of degrees of freedom in a dataset \citep{Thorstensen2009ManifoldThesis, Belkin2004SemiSupLearningRiemannianManifolds}.     Consider the example shown in Figure \ref{fig:manifold_eg}.  This classic example demonstrated in \citep{Thorstensen2009ManifoldThesis} shows samples from a pose-estimation dataset \textbf{CITE}.  While each individual image is represented by a vector of features (pixel intensities in this case) in $\mathbb{R}^{4096}$, the dataset only exhibits three degrees of freedom: 1 light variation parameter and 2 rotation angles.  Thus, it is intuitive that the dataset lies on a smooth, intrinsic submanifold spanning three dimensions which inherently capture the degrees of freedom in the data.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[Example pose data manifold.]{Placeholder for example of high D data lying on a low-dimensional sub-manifold.}
		\label{fig:manifold_eg}
	\end{figure*}
\end{center}

The goal of manifold Learning is then to discover embedding functions which take data from the input feature space and transform it into a lower-dimensional (ideally intrinsic) coordinate system (also called a \textit{latent space} in the literature) which captures the ``useful" properties of the data, while enforcing constraints such as smoothness (the transformation function should not produce sporadic images), continuity (no discontinuous points on the hyper-surface), topological ordering (neighbors in the input space should also be neighbors in the embedded space ) or class separability (samples from the same class should fall metrically close to each other in the embedded space and disparate classes should be distinctly far) \citep{Vural2018StudySupervisedManifoldLearning}.

This dissertation focuses on investigating the use of manifold learning to increase instance discriminability in the latent space, where labels are solely provided at the bag-level.   While there is an expansive literature in unsupervised manifold learning methods, this document will pay special attention to both strictly- and semi-supervised methods, since they are typically adaptations of unsupervised approaches, as well as manifold learning under the MIL framework.

\subsection{Definition and General Notation}
Most studies perform classification or regression after applying unsupervised dimensionality reduction.  However, it has been shown that there are advantages of learning the low-dimensional representations and classification/regression models simultaneously \citep{Chao2019RecentAdvancesSupervisedDimRed,Rish2008SupDimRedGLM}.  Considering classification as the main goal of dimensionality reduction, this section provides a summary of the current literature in the area. \newline

Given a data matrix $\bm{X} = [\bm{x}_1, \dots, \bm{x}_N]\in \mathbb{R}^{N \times D}$ where $N$ is the total number of samples and $D$ is the dimensionality of the input feature space, general dimensionality reduction seeks to find a representation $\bm{Z} \in \mathbb{R}^{N \times d}$ with $d \ll D$ that enhances the between-class separation while preserving the intrinsic geometric structure of the data\citep{Vural2018StudySupervisedManifoldLearning}.  In other words, it is assumed that the data lie on a smooth manifold $\mathcal{X}$, which is the image of some parameter domain $\mathcal{Z} \subset \mathbb{R}^{d}$ under a smooth mapping $\Psi : \mathcal{Z} \rightarrow \mathbb{R}^{D}$.  The goal of manifold learning is to discover an inverse mapping to the low-dimensional pre-image coordinates $\bm{z}_n \in \bm{z}$ corresponding to points $\bm{x}_n \in \bm{X}$.  The data matrices $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}]$ and $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ are of size $N \times D$ and $N \times d$, respectively.  Since these low-dimensional data representations are unknown, they are often referred to as \textit{latent} vectors and the span in $\mathbb{R}^d$ is sometimes called the \textit{latent feature space} or \textit{latent space} for brevity \citep{Murphy2012}. The primary difference between traditional, unsupervised manifold learning and supervised approaches is that, in supervised manifold learning, data matrix $\bm{X} $ is accompanied with a corresponding label vector $\bm{l} = [l_1, \dots, l_N]$ indicating the corresponding class labels of each sample in $\bm{X}$. \newline

Manifold learning methods can be subdivided into a wide taxonomy of approaches, with \textit{linear} and \textit{nonlinear} at the root. Nonlinear approaches can be further divided into purely global methods and approaches that capture global structure solely from local information.  We begin with a review of popular linear manifold learning techniques before moving into the realm of nonlinear approaches. Base, unsupervised methods are reviewed along with corresponding supervised and semi-supervised adaptations. 

\subsection{Linear Manifold Learning}
A review of linear manifold learning approaches is provided.  Linear approaches are advantageous over nonlinear because they allow for out-of-sample extensions.  In other words, linear transformation matrices are learned which can be easily applied on data not included in the training set.  However, linear approaches are limited in their abilities to capture irregular data surfaces \citep{Kegl2008PrincipalManifoldsTextbook}.  Principal Component Analysis (PCA), Multi-dimensional Scaling (MDS), Nonnegative Matrix Factorization (NMF) and Fisher's Linear Discriminant Analysis (LDA) are reviewed.  General approaches are discussed and supervised as well as nonlinear extensions are elaborated. Special focus is given to (LDA), as it is the only inherently supervised technique out of the included approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  PCA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Principal Component Analysis (PCA)} \label{sec:PCA}

\paragraph{Unsupervised PCA}
Principal Component Analysis (PCA) is arguably the most popular (and best-studied) technique for dimensionality reduction and manifold learning.  It attempts to learn an orthogonal projection of the input data into a lower-dimensional space, known as the principal subspace, such that the variance of the projected data is maximized \citep{Chao2019RecentAdvancesSupervisedDimRed}.  In other words, each \textit{principal axis}, or \textit{principal component}, of the learned coordinate system is orthogonal to the other principal components.  In summary, the problem of PCA is to discover basis vectors which linearly combine to reconstruct the data.  In practice, data in the input feature space are projected into a new coordinate system of $d$ dimensions, such that the variance along each principal axis is maximized and the reconstruction errors of the data are minimized in the mean-square sense \citep{Thorstensen2009ManifoldThesis}.  Let $V$ be a $d$-dimensional subspace of $\mathbb{R}^{D}$ and let  $\bm{w}_1, \dots, \bm{w}_D$ be an orthonormal basis of $\mathbb{R}^{D}$ such that $\bm{w}_1, \dots, \bm{w}_d$ is a basis of $V$.  The goal of PCA is to find an orthogonal set of basis vectors $\bm{w}_n \in \mathbb{R}^{D}$ and corresponding latent coordinates $\bm{z}_n \in \mathbb{R}^{d}$ such that the average reconstruction error is minimized \citep{Murphy2012}
\begin{align}
	J(\bm{W}, \bm{Z}) = \frac{1}{N}\sum_{n=1}^{N} ||\bm{x}_n - \hat{\bm{x}}_n ||^{2}
\end{align}

\noindent
where $\hat{\bm{x}}_n = \bm{W}\bm{z}_{n}$, subject to the constraint that $\bm{W}$ is \textit{orthonormal}, or that $\bm{w}_{i}^{T}\bm{w}_{j}=0,\forall i \neq j$ and $\bm{w}_{i}^{T}\bm{w}_{i}=1 $.  This is equivalently written as 
\begin{align}
		J(\bm{W}, \bm{Z}) = ||\bm{X} - \bm{W}\bm{Z} ||^{2}_{F}
\end{align}

\noindent
where $\bm{Z}$ is a $N \times d$ matrix with the $\bm{z}_{n}$ in its rows and $||\bm{A}||_{F}$ is the \textit{Frobenius norm} of matrix $\bm{A}$, defined by 
\begin{align}
	||\bm{A}||_{F} &= \sqrt{\sum_{m=1}^{M}\sum_{n=1}^{N}a^{2}_{mn}} = \sqrt{tr(\bm{A}^{T}\bm{A})} = ||\bm{A}(:)||_{2}
\end{align} 

\noindent
As noted by Murphy \citep{Murphy2012}, the optimal solution is obtained by setting $\hat{\bm{W}} = \bm{U}_{d}$, where $\bm{U}_{d}$ contains the eigenvectors corresponding to the $d$ largest eigenvalues of the mean-subtracted, empirical data covariance matrix, $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}(\bm{x}_{n}-\hat{\bm{\mu}})(\bm{x}_{n}-\hat{\bm{\mu}})^{T}$, where $\hat{\bm{\mu}}$ is the empirical data mean.  Therefore, the low-dimensional encoding of the data is given by $\bm{z}_n = \hat{\bm{W}}^{T}\bm{x}_n$, which is the orthogonal, linear projection of the data onto the column space spanned by the eigenvectors of the $d$ largest eigenvalues of the empirical data covariance.  

The example shown in figure \ref{fig:pca_eg} demonstrates the projection of 2-dimensional data onto the first principal axis.  As can be seen from the figure, the first principal axis corresponds to the direction of maximal variance of the data.  PCA from the viewpoint of variance maximization is often called the \textit{analysis view} of PCA \citep{Murphy2012}.
\begin{center}
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption[PCA example.]{Placeholder for PCA projection example.}
		\label{fig:pca_eg}
	\end{figure*}
\end{center}

PCA has been successfully applied to a large  number of domains such as face recognition, coin classification and seismic series analysis \citep{VanDerMaaten2009DRReview}. It's success is partially because of its convenience of use.  Embedding out-of-sample points with PCA is simple since the transformation is just a rotation and scaling. However PCA suffers from a few drawbacks.  First, the dimensionality of the covariance matrix is proportional to the dimensionality of the data points.  As a result, the computation of the eigenvectors may be infeasible or untrustworthy (singularity) for high-dimensional data.  Additionally, PCA focuses mainly on preserving large pairwise distances between data samples instead of retaining local relationships, which may be important in certain applications.  PCA also assumes Gaussian distributed data, which is unlikely in real-world applications. Finally, PCA is sensitive to feature magnitude. It is typical to standardized data before applying PCA as it can be misled by directions in which the variance is high simply because of the measurement scale.

Many extensions and alternative viewpoints have been made to PCA, including a nonlinear version \citep{Scholkopf1999KPCA}, a supervised version and looking at it as a factor analysis problem \citep{Tipping1999PPCA}.  A few adaptations to PCA are discussed in later sections.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  MDS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Multi-Dimensional Scaling (MDS)} 

\paragraph{Unsupervised MDS} \label{sec:MDS}
Most modern manifold learners have theoretical and algorithmic roots in one of three basic dimensionality reduction techniques: PCA, K-means and \textit{Multidimensional Scaling} (MDS). Whereas PCA looks for linear projection bases which are constructed from the eigenvectors of a data covariance or scatter matrix, MDS tries to find a linear projection that preserves pairwise distances as well as possible. This idea is demonstrated by Figure \ref{fig:mds_example}, where the pairwise distances between samples in the 3-dimensional space are preserved in the 2-dimensional embedding space. While MDS does not construct an embedded manifold explicitly, it holds the status of being the grandfather of ``one-shot" (non-iteratve) manifold learners, such as Isomap and Locally Linear Embedding (LLE), which are discussed later in this literature review \citep{Kegl2008PrincipalManifoldsTextbook}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Example of MDS distance preservation.]{Example of MDS distance preservation.}
		\label{fig:mds_example}
	\end{figure*}
\end{center}

The steps of MDS correspond exactly to those of PCA except that, instead of a scatter matrix $\bm{S}=\frac{1}{N}\bm{X}\bm{X}^{T}$, MDS operates with a positive semi-definite, dissimilarity matrix $\bm{D} \in \mathbb{R}^{N \times N}$, where $N$ is the number of data samples and a real, symmetric Gram matrix $\bm{K} = \bm{X}^{T}\bm{X}$ (inner-product matrix) where $\bm{K}_{mn}$ is the inner product between $\bm{x}_{m}$ and $\bm{x}_{n}$.  The problem of MDS is posed as finding $d$-dimensional Euclidean coordinates for each sample $\bm{x}_{n}$ in dataset $\bm{X}$ such that the Euclidean distances in the low-dimensional embedding space are proportional to the pairwise distances in the input space \citep{Thorstensen2009ManifoldThesis,Sorzano2014DRReview}. While the literature poses several cost functions for this task, this review focuses on classical MDS, which is described as follows:

First, the pairwise distance matrix $\bm{D}$ is computed such that 
\begin{align}
	\bm{D}_{mn} = \mathcal{D}_{\mathcal{X}}(\bm{x}_{m},\bm{x}_{n}) = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} = (\bm{x}_{m} - \bm{x}_{n})^{T}(\bm{x}_{m} - \bm{x}_{n}) 
\end{align}
\noindent
where $\mathcal{D}_{\mathcal{X}}(\cdot,\cdot)$ is a chosen dissimilarity metric (Euclidean distance for classical MDS).  Then, the double-centered Gram matrix $\bm{K}$ is computed by
\begin{align}
	\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}
\end{align}
\noindent
where 
\begin{align}
	\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}
\end{align}
\noindent
with $\mathbb{I}_{N}$ denoting the $N \times N$ identity matrix and $\bm{e}= (1, \dots, 1)^{T}$ the $N \times 1$ column vector of all ones.  Multiplying $\bm{D}$ on both sides by $\bm{H}$ performs \textit{double centering}, which subtracts the row and column means from $\bm{D}$ (and adds back the global mean which gets subtracted twice), so that both the row and column means of $\bm{K}$ are equal to zero.  The objective is to find $\bm{Z}= \{\bm{z}_{1}, \dots, \bm{z}_{N}\} \in \mathbb{R}^{d}$ which minimizes the objective 
\begin{align}
	J(\bm{D}_{\bm{X}},\bm{D}_{\bm{Z}}) = ||\bm{K}_{\bm{X}} - \bm{D}_{\bm{Z}} ||^{2} = \left |\left|-\frac{1}{2} \bm{H} (\bm{D}_{\bm{X}} - \bm{D}_{\bm{Z}}) \bm{H} \right |\right|^{2}
\end{align}
Similarly to PCA, this can be solved by a generalized eigenvalue problem 
\begin{align}
	\bm{K}v = \lambda v
\end{align}
\noindent
such that 
\begin{align}
	\bm{Z} = \bm{V}\bm{\Lambda}^{\frac{1}{2}}
\end{align}
\noindent
with $\bm{\Lambda}^{\frac{1}{2}} = diag(\sqrt{\lambda_{1}},\dots,\sqrt{\lambda_N})$ being a diagonal matrix with entries equal to the square roots of the eigenvalues of $\bm{K}$ sorted from largest to smallest ($\lambda_{1} \geq  \lambda_{2} \geq \dots \geq 0$), and $\bm{V} = \{ \bm{v}_{1}, \dots, \bm{v}_{N} \}$ the corresponding eigenvectors.  The low-dimensional embedding coordinates $\bm{Z} \in \mathbb{R}^{N \times d}$ are obtained by $\bm{Z} = \{\sqrt{\lambda_{1}}\bm{v_{1}}, \dots, \sqrt{\lambda_{d}}\bm{v_{d}} \}$ \citep{Chao2019RecentAdvancesSupervisedDimRed}.

It has been proven that the eigenvalues of Gram matrix $\bm{K}$ and covariance $\bm{S}$ are the same, and that the space spanned by MDS and PCA are the identical for any $d \leq rank(\bm{K}) = rank(\bm{S})$.  This implies that a rotation matrix $\bm{A}$ could be found such that $\bm{A}^{T}\bm{Z}_{MDS} = \bm{Z}_{PCA}$ \citep{Sorzano2014DRReview}.  Additionally, MDS can be computed even if the data observation matrix $\bm{X}$ is unknown.  All that is needed is the Gram matrix or a dissimilarity matrix.  This feature potentially allows MDS to be applied in a variety of data-sensitive and privacy-concerned scenarios. 

A pitfall of MDS is that it focuses on retaining global pairwise distances as opposed to local distances, which are typically much more important for capturing the geometry of the data \citep{VanDerMaaten2009DRReview}.  Several MDS variants have been proposed to address this weakness.  A popular variant is known as \textit{Sammon Mapping} and is discussed in Section \ref{sec:sammon_mapping}. 

\paragraph{Supervised MDS}
As with most manifold learning methods in the literature, MDS does not inherently consider class information when learning the embedding function.  In attempt to promote class separability in the  low-dimensional embedding space, Witten et al. proposed a \textit{Supervised Multidimensional Scaling} (SMDS) \citep{Witten2011SuperMDS}.  this method follows the idea of traditional MDS where the goal is to find low-dimensional coordinate or \textit{configuration points} $\bm{z}_{n} \in \mathbb{R}^{d}$, such that pairwise distances in the input feature space are preserved in the  embedding space.  Incorporating class label information, the goal of SMDS is to not only preserve distances, but ensure the coordinate values $z_{mk} > z_{nk}$ when $l_{m} > l_{n}, \quad \forall k = 1, \dots, d$, where $l$ are the instance-level labels and $d$ is the dimensionality of the embedding space.  Considering the binary target classification case, SMDS can be formulated as
\begin{align}
	\min_{\bm{Z}} \quad \frac{1}{2}(1-\alpha)\sum_{m=1}^{N}\sum_{n=1}^{N}(\bm{D}_{mn} - ||\bm{z}_{m} - \bm{z}_{n} ||^{2}) + \alpha \sum_{m:l_{m}=1} \sum_{n:l_n=2} \sum_{k=1}^{d}\left(\frac{\bm{D_{mn}}}{\sqrt{d}} - (z_{nk} - z_{mk})^{2} \right)
\end{align}
This objective has two terms.  The first is the traditional metric MDS \textit{stress}.  This term attempts to ensure that the Euclidean distances of two points in the embedding space is the same as the dissimilarity between the points in the input feature space.  The second term is the supervised term which enforces that each dimension of the embedded configuration points be larger if belonging to the class with the larger label, and smaller if belonging to the class with a smaller-valued label.  The term $\alpha \in [0,1]$ is a tuning parameter.  When $\alpha = 0$, the objective reduces to the MDS stress function.  As $\alpha$ increases, however, the objective becomes increasingly more supervised, focused on ensuring class separation of the training data.

A least square regression was applied to estimate the embedding function for out-of-sample  test points.  SMDS was successfully applied to tasks in data visualization, bipartite ranking and classification of prostate data and USPS handwritten digits.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  NMF  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Nonnegative Matrix Factorization (NMF)}
\textit{Nonnegative Matrix Factorization} (NMF) is a tool for linear dimensionality reduction that, given a set of data $\bm{X} \in \mathbb{R}^{N \times D}$, aims to decompose the matrix into a coefficient matrix $\bm{Z} \in \mathbb{R}^{N \times d}$ and basis matrix $\bm{W} \in \mathbb{R}^{d \times D}$ \citep{Gillis2014NMF}.  In this case, the columns of matrix $\bm{W}$ are basis elements and  the columns of matrix $\bm{Z}$  give the coordinates of data samples in the basis $\bm{W}$. Similar to PCA, the goal of NMF is to find a $\bm{Z}$ and $\bm{W}$ such that $\hat{\bm{X}} = \bm{Z}\bm{W}$ approximates the data as close as possible, given that every element must be nonnegative.  This notion is formalized by
\begin{align}
	\min_{\bm{Z},\bm{W}} || \bm{X} - \bm{Z}\bm{W} ||^{2}_{F} \quad s.t. \quad \bm{Z} \geq 0, \bm{W} \geq 0
\end{align}
\noindent
Another popular variant of NMF is to substitute the Frobenius norm for the Kullback-Leibler (KL) divergence, where $D(\bm{X}||\hat{\bm{X}}) = \sum_{m,n}(\bm{X}_{mn} \log \frac{\bm{X}_{mn}}{\hat{\bm{X}}_{mn}} + \bm{X}_{mn} - \hat{\bm{X}}_{mn})$.  Given that $d \ll D$, it is intuitive that $\bm{Z} \in \mathbb{R}^{N \times d}$ provides the desired low-dimensional representations of high-dimensional data $\bm{X}.$  Many approaches have been used to solve for the nonnegative matrices, including alternating least squares, projected gradient descent, coordinate descent and the Alternating Direction Method of Multipliers (ADMM) \citep{Chao2019RecentAdvancesSupervisedDimRed}.  NMF has been successfully applied to applications in image processing, text mining, hyperspectral imaging, air emission control, computational biology, blind source separation, single-channel source separation, clustering, music analysis and collaborative filtering \citep{Gillis2014NMF}.  

As described in \citep{Chao2019RecentAdvancesSupervisedDimRed}, two groups of supervised NMF have been proposed in the literature according to the way label information is utilized.  In \textit{direct supervised NMF} approaches, label information is incorporated directly into the loss function to promote learning of well-separated coordinate representations for samples in different classes.  Approaches taken under this framework include incorporating simple indicator variables to denote the class of a sample, integrating NMF with a SVM and formulating the problem under task-driven dictionary learning.  An alternative approach to the direct supervised NMF is \textit{discriminative NMF}.  Discriminative NMF approaches are based on \textit{Linear Discriminant Analysis} (LDA).  Essentially, the objective for this class of algorithm is to decompose the data matrix such that the between-class distances of the low-dimensional coordinates $\bm{Z}$ are maximized, while the within-class distances are minimized.  Supervised NMF approaches have been applied successfully to problems in acoustic separation, brain tumor detection and emotion classification.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LDA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fisher's Linear Discriminant Analysis (LDA)} \label{sec:LDA}

\paragraph{Classical LDA}
\textit{Linear Discriminant Analysis} (LDA) is a popular method for supervised, linear dimensionality reduction.  LDA currently forms the basis for Multiple Instance Learning dimensionality reduction methods exhibited in the literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity,Xu2011MI_Metric_Learning}. Whereas PCA tries to project data into a space which maximizes variance, LDA considers class label information and tries to find a transformation which both maximizes between-class (inter-class) dissimilarity and minimizes within-class (intra-class) scatter \citep{Yan2007GeneralGraphEmbeddingFramework,Chao2019RecentAdvancesSupervisedDimRed, Sun2010MIDR, Murphy2012}.   This is done by maximizing the ratio between the inter-class $\bm{S}_{b}$ and intra-class $\bm{S}_{w}$ scatter matrices, defined as:
\begin{align}
	\bm{S}_{w} = \sum_{k=1}^{K}\bm{S}_{k}
\end{align}
\begin{align}
	\bm{S}_{k} = \sum_{n \in C_{k}}(\bm{x}_{n} - \hat{\bm{\mu}_{k}})(\bm{x}_{n} - \hat{\bm{\mu}_{k}})^{T}
\end{align}
\begin{align}
	\bm{S}_{b} = \sum_{k=1}^{K}N_{k}(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})(\hat{\bm{\mu}_{k}} - \hat{\bm{\mu}})^{T}
\end{align}
\noindent
Here, $\bm{S}_{w}$ is the global within-class scatter matrix which is defined as the sum over each individual class' scatter matrix $\bm{S}_{k}$, and $\bm{S}_{k}$ is essentially an outer product between all samples belonging to class $C_{k}$ after subtracting the respective empirical class mean $\hat{\bm{\mu}_{k}}$.  This scatter matrix would be the class covariance if it was normalized by the number of samples $N_{k}$ in class $C_{k}$.  However, this normalization constant does not affect the final solution and can thus be ignored.  The between-class scatter $\bm{S}_{b}$ is defined by the sum of outer products of the differences between the empirical class means $\hat{\bm{\mu}_{k}}$ and the global data mean $\hat{\bm{\mu}}$, weighted by the number of samples in each class.  The objective of LDA is then to solve for $\bm{W}^{*}$ which maximizes the ratio $J(\bm{W})$:
\begin{align}
	\bm{W}^{*} = \arg\min_{\bm{W}} J(\bm{W}) =  \arg\max_{\bm{W}} \frac{|\bm{W}^{T}\bm{S}_{b}\bm{W}|}{|\bm{W}^{T}\bm{S}_{w}\bm{W}|}
\end{align}
It has been shown that the optimal projection matrix $\bm{W}^{*}$ is the one whose columns are the eigenvectors corresponding to the largest eigenvalues of the generalized eigenvalue problem 
\begin{align}
	\bm{S}_{b}\bm{w}=\lambda \bm{S}_{w}\bm{w} \Rightarrow \bm{S}_{w}^{-1}\bm{S}_{b}\bm{w} = \lambda\bm{w}
\end{align}
Since $\bm{S}_{b}$ is the sum of $K$ matrices of rank $\leq 1$, this implies that $\bm{S}_{b}$ will be of rank $(K-1)$ or less and only $(K-1)$ of the eigenvalues $\lambda$ will be non-zero.  
A low-dimensional coordinate representation $\bm{z}_{n} \in \mathbb{R}^{(K-1)}$ of sample $\bm{x}_{n} \in \mathbb{R}^{D}$ is given by the linear projection of $\bm{x}_{n}$ onto the hyper-plane parameterized by $\bm{W}^{*}$, $\bm{z}_{n} = \bm{W}^{*T}\bm{x}_{n}$  It should be noted that for LDA, the dimensionality of the latent space is not a free-parameter, but is always fixed at $d=(K-1)$, or one less than the number of classes present in the dataset.  Equivalently, LDA can be derived by maximum likelihood for normal class-conditional densities where the covariances for each class are assumed to be equivalent \citep{Murphy2012}.  For the special case of binary target classification, the LDA transformation will place every sample onto a single line in 1-dimension, and thus the LDA solution can be simplified:
\begin{align}
	\bm{w}^{*} = \arg\max_{\bm{w}} \frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}} = \bm{S}_{w}^{-1}(\hat{\bm{\mu}_{2}} - \hat{\bm{\mu}_{1}})
\end{align}  
where $\hat{\bm{\mu}_{1}}$ and $\hat{\bm{\mu}_{2}}$ are the empirical means for classes $1$ and $2$, respectively.  Figure \ref{fig:lda_example} demonstrates the differences between PCA and LDA.  While PCA projects data onto the axes exhibiting the maximal variation, LDA projects the data into a space which attempts to simultaneously enforce between-class separation and within-class compactness. 

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[LDA example.]{LDA example.}
		\label{fig:lda_example}
	\end{figure*}
\end{center}

Although LDA is the basis for large number of discriminative dimensionality reduction approaches, it does not guarantee class separation in the embedding space.  For example, LDA projects data into a space of at most $(K-1)$ dimensions, however, more features may be necessary for adequate class discrimination.  Additionally, LDA is a parametric method which assumes unimodal Gaussian likelihoods.  This implies that it may not be able to preserve complex data structure.  Finally, LDA will fail if the discriminatory information is contained in the variance of the data instead of the mean.  Despite these pitfalls, LDA  has been successfully applied to object detection and recognition tasks \citep{Wang2016OrthogonalLDA}  Many variations of LDA have been developed, such as Non-parametric LDA \citep{Fukunaga1983NonparametricLDA}, Orthonormal LDA \citep{Wang2016OrthogonalLDA}, Generalized LDA \citep{Baudat2000GeneralizedDiscriminantAnalysis} and Multilayer Perceptrons \citep{Webb1990MLPLDA}.  Additionally, LDA serves as the foundation for many of the Multiple Instance Learning dimensionality reduction approaches in the current literature \citep{Sun2010MIDR,Chai2014MIDA,Zhu2018MIDRSparsity}.

\subsection{Nonlinear Manifold Learning}

Linear methods such as PCA and MDS are convenient for projecting out-of-sample test points into the embedding space.  However, they are unable to capture the structure of data that are sampled from nonlinear manifolds \citep{Kegl2008PrincipalManifoldsTextbook}. This section will discuss a variety of nonlinear dimensionality reduction and manifold learning approaches.  All methods reviewed assume the data is distributed along a $d$-dimensional sub-manifold $\mathcal{X}$ embedded in $\mathbb{R}^{D}$.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Example of a nonlinear manifold.]{Example of a nonlinear manifold.}
		\label{fig:nonlinear_manifold}
	\end{figure*}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Kernels  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Kernelization} \label{sec:Kernels}
Although each of the manifold learning techniques previously discussed are inherently linear, nonlinear adaptations have been made.  One easily extendable approach is to utilize kernel functions as means to provide nonlinearity in the embeddings. 

\paragraph{Kernels} A \textit{kernel function} is a real-valued function of two arguments $\kappa(\bm{x},\bm{x}') \in \mathbb{R}$, for $\bm{x},\bm{x}' \in \mathcal{X}$, which maps vectors from the input feature space to a single value in $\mathbb{R}$.  The function is typically symmetric (i.e.  $\kappa(\bm{x},\bm{x}') =  \kappa(\bm{x}',\bm{x})$) and non-negative (i.e.  $\kappa(\bm{x},\bm{x}') \geq 0$), which implies that it can be interpreted as a measure of similarity \citep{Murphy2012}.  The notion of kernels is very useful in certain applications where data representation is not straightforward, such as representing text documents or molecular structures which can have variable length.  Additionally, this allows algorithms to operate directly on the kernel representations.  This is useful in data-sensitive scenarios where direct access to the data may not be available.

A popular choice of kernel in manifold learning is the \textit{radial basis function} (RBF) kernel, defined as:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \exp \left( - \frac{|| \bm{x} - \bm{x}' ||^{2}}{\beta} \right)
\end{align}
\noindent
where $\beta$ is the bandwidth of the isotropic function.  Another popular kernel for text classification is \textit{cosine similarity}, defined by:
\begin{align}
	\kappa(\bm{x},\bm{x}') = \frac{\bm{x}^{T}\bm{x}'}{||\bm{x}||_{2}||\bm{x}'||_{2}} 
\end{align}
\noindent
This kernel measures the cosine of the angle between vectors $\bm{x}$ and  $\bm{x}'$ after scaling them onto the unit hyper-sphere.  If  $\bm{x}$ and  $\bm{x}'$ are strictly positive vectors (counts in the bag-of-words model, for example), then the kernel provides values in $[0,1]$, where a value of $0$ means the feature vectors are orthogonal and, therefore, have no features in common, and a value of $1$ means the vectors are the same. 

Some of the nonlinear manifold learning methods in the literature require the kernel function to satisfy the requirement that the \textit{Gram matrix}
\begin{align}
	\bm{K} = 	
	\begin{bmatrix}
		\kappa(\bm{x}_{1},\bm{x}_{1}) &  \dots & \kappa(\bm{x}_{1},\bm{x}_{N}) \\
		 & \vdots  & \\
		\kappa(\bm{x}_{N},\bm{x}_{1}) & \dots & \kappa(\bm{x}_{N},\bm{x}_{N})
	\end{bmatrix}
\end{align}
\noindent
be positive definite for any set of inputs $\{\bm{x}_{n}\}_{n=1}^{N}$.  This type of kernel is called a \textit{Mercer kernel} or \textit{positive definite kernel}.  The importance of the Mercer Kernel is the following result, known as \textit{Mercer's theorem}.  This theorem states that if the Gram matrix is positive definite, its eigenvector decomposition can be written as
\begin{align}
	\bm{K} = \bm{U}^{T}\bm{\Lambda}\bm{U}
\end{align}
As derived by Murphy and Liu et al. \citep{Murphy2012,Liu2010KernelAdaptiveFiltering}, it then follows that each entry of $\bm{K}$ can be computed as 
\begin{align}
	\kappa(\bm{x},\bm{x}') = \phi(\bm{x})^{T}\phi(\bm{x}') = <\phi(\bm{x}),\phi(\bm{x}')>
	\label{eq:kernel_inner_product}
\end{align}
meaning that the entries of the kernel matrix can be defined by the inner product of some feature vectors that are implicitly defined by the eigenvectors $\bm{U}$.  If the kernel is Mercer, then there exists a function $\phi$ which maps $\bm{x} \in \mathcal{X}$ to $\mathbb{R}^{D}$ such that Equation \ref{eq:kernel_inner_product} holds.  Additionally, $\phi$ depends on the eigenfunctions of $\kappa$, meaning that $D$ is a potentially infinite dimensional space.
Additionally, instead of representing feature vectors in terms of kernels $\phi(\bm{x}) = [\kappa(\bm{x},\bm{x}_{1}), \dots, \kappa(\bm{x},\bm{x}_{N})]$, algorithms can instead work with the input feature vectors $\bm{x}$ by replacing all inner products $<\bm{x},\bm{x}'>$ with a call to the kernel function $\kappa(\bm{x},\bm{x}')$.  This is called the \textit{kernel trick}, and it turns out that many algorithms can be kernelized in this way.

Kernel functions play an important role in the dimensionality reduction literature for both applying nonlinearity to inherently linear problems and in defining similarity measures for graph-based manifold learning methods.  Specifically, nonlinear adaptations of the inherently-linear PCA \citep{Scholkopf1999KPCA}, MDS \citep{Webb2002KMDS} and LDA \citep{Ghojogh2019KDATutorial} algorithms have been formulated. The kernelization of PCA is briefly described in the following. 

\paragraph{Kernel PCA}
Section \ref{sec:PCA} showed how PCA could be used to compute linear low-dimensional embeddings of data.  This process involved finding the eigenvectors of the empirical data covariance matrix $\hat{\bm{S}} = \frac{1}{N}\sum_{n=1}^{N}\hat{\bm{x}}_{n}\hat{\bm{x}}_{n}^{T} = \frac{1}{N}\hat{\bm{X}}^{T}\hat{\bm{X}}$, where $\hat{\bm{x}}_{n} = \bm{x}_{n} - \hat{\bm{\mu}}$ is the mean subtracted feature vector.  However, PCA can also be computed by finding the eigenvectors of the inner product matrix $\bm{X}\bm{X}^{T}$ \citep{Murphy2012,Wang2014KPCAReview}.  This interpretation allows the production of nonlinear embeddings by taking advantage of the kernel trick.  This approach is known as \textit{Kernel PCA} (KPCA) \citep{Scholkopf1999KPCA}.  Assuming the data is mapped to a new feature space in $\mathbb{R}^{M}$ by a nonlinear transformation $\phi(\bm{x})$, PCA could be performed in the new feature space.  However, this computation can be extremely costly and inefficient.  Instead, kernel methods can be used to simply the computation.  Following the derivation defined in \citep{Wang2014KPCAReview}, first assume that the data in the new feature space has zero mean:
\begin{align}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n}) = 0	
\end{align}

\noindent
The covariance matrix of the projected features is a $M \times M$ matrix 
\begin{align} 
	\label{eq:kernel_covariance}
	\bm{S}_{\phi} =  \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}
\end{align}
\noindent
The eigenvectors $\bm{V}$ and eigenvalues $\lambda$ for $\bm{S}_{\phi}$ satisfy
\begin{align} 
	\label{eq:kernel_eigenvectors}
	\bm{S}_{\phi}\bm{v}_{k} = \lambda_{k} \bm{v}_{k}, \quad \forall k = 1, \dots, M
\end{align}
\noindent
From Equations \ref{eq:kernel_covariance} and \ref{eq:kernel_eigenvectors}, we have 
\begin{align}
	\label{eq:kernel_equivalent_system}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}\bm{v}_{k} = \lambda_{k} \bm{v}_{k}
\end{align}
\noindent
which can be re-written as
\begin{align}
	\label{eq:kernel_eigenvectors_and_weights}
	\bm{v}_{k} = \sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n})
\end{align}
\noindent
Substituting Equation \ref{eq:kernel_eigenvectors_and_weights} into Equation \ref{eq:kernel_equivalent_system}, gives 
\begin{align}
	\frac{1}{N} \sum_{n=1}^{N}\phi(\bm{x}_{n})\phi(\bm{x}_{n})^{T}\sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n}) = \lambda_{k} \sum_{n=1}^{N}\alpha_{kn}\phi(\bm{x}_{n})
\end{align}
\noindent
A $N \times N$ matrix $\bm{K}$ can be defined by 
\begin{align}
	\bm{K}_{mn} = \kappa(\bm{x}_{m},\bm{x}_{n}) = \phi(\bm{x}_{m})^{T}\phi(\bm{x}_{n})
\end{align}
\noindent
which simplifies the problem to 
\begin{align}
	\bm{K}^{2}\bm{\alpha}_{k} = N \lambda_{k}\bm{K} \bm{\alpha}_{k}
\end{align}
\noindent
where $\bm{\alpha}_{k}$ is a column vector with entries $\alpha_{k1}, \dots, \alpha_{kN}$.  Each $\bm{\alpha}_{k}$ can be found by solving the eigenvalue problem 
\begin{align}
	\bm{K}\bm{\alpha}_{k} = N \lambda_{k}\bm{\alpha}_{k}
\end{align}
\noindent
Then the projection of a test point $\bm{x}$ onto the $k^{th}$ principal component can be found by
\begin{align}
	z_{k}(\bm{x}) = \phi(\bm{x})^{T}\bm{v}_{k} = \sum_{n=1}^{N} \alpha_{kn}\kappa(\bm{x}_{n},\bm{x})
\end{align}
\noindent
This formulation assumes that the projected data has zero mean.  However, this is not generally the case and the mean cannot simply be subtracted in the projected space \citep{Murphy2012}.  Therefore, KPCA can be performed on the centered Gram matrix, defined by
\begin{align}
	\tilde{\bm{K}} = \bm{H}\bm{K}\bm{H}
\end{align}
\noindent
where
\begin{align}
	\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}
\end{align}
\noindent
is the $N \times N$ centering matrix.  Pseudo-code for KPCA is given in Algorithm \ref{alg:KPCA}.


\begin{algorithm}[h!]
	\caption{KPCA}
	\label{alg:KPCA}
	\begin{algorithmic}[1]
		\Require {Gram matrix of training data $\bm{K} \in \mathbb{R}^{N \times N}$, Gram matrix augmented with test data $\bm{K}_{*} \in \mathbb{R}^{N_{*} \times N}$, dimensionality of latent space $d$}
		\Ensure {Embedded data coordinates $\bm{Z} \in \mathbb{R}^{N \times d}$}
		\State {$\bm{H} \gets \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}$}   
		\State {$\tilde{\bm{K}} \gets \bm{H}\bm{K}\bm{H}$}   
		\State {$[\bm{U},\bm{\Lambda}] \gets \textsc{eig}(\tilde{\bm{K}})$}     
		\For{$n \in N$}
		\State{$\bm{v}_{n} \gets \bm{u}_{n}/\sqrt{\lambda_{n}}$}
		\EndFor
		\State {$\bm{H}_{*} \gets \frac{1}{N_{*}}\bm{e}_{*}\bm{e}^{T}$}
		\State {$\tilde{\bm{K}}_{*} \gets \bm{K}_{*} - \bm{H}_{*}\bm{K}_{*} - \bm{K}_{*}\bm{H}_{*} + \bm{H}_{*}\bm{K}_{*}\bm{H}_{*}$}  
		\State {$\bm{Z} \gets \tilde{\bm{K}}_{*}\bm{V}_{1:d}$}
	\end{algorithmic}
\end{algorithm}
\noindent

Whereas linear PCA is limited to $d<D$ components, KPCA can use up to $N$ components.  Using a nonlinear feature embedding function along with the kernel trick provides for an elegant solution for capturing global nonlinear data structure.  As can be seen in Algorithm \ref{alg:KPCA}, embedding out-of-sample test points is done by appending rows to the Gram matrix, where new entries are defined between the test points and training data points, but not between test points and other test points.  Embedded data coordinates are computed by multiplying the augmented Gram matrix with the top $d$ scaled eigenvectors of the training Gram matrix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Graphs  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Graph-based Methods}
Nonlinear manifold learning methods typically rely on the use of computational graphs.  These graphs represent data structure pooled from local neighborhoods of samples.  \textit{Spectral graph theory} focuses on constructing, analyzing and manipulating graphs.  It has proved useful for object representation, graph visualization, spectral clustering, dimensionality reduction and numerous other applications in chemistry, physics, signal processing and computer science \citep{Shuman2013SignalProcessingGraphs, Bengoetxea2002ThesisGraphMatching}.   An overview of computational graphs as well as prominent methods for graph construction in manifold learning are presented. Additionally, geodesic distance approximation from pairwise distances is reviewed.  It should be noted that the work in \citep{Yan2007GeneralGraphEmbeddingFramework} shows how each of the undermentioned graph-based manifold learning approaches can be succinctly described under a general graph-based framework for dimensionality reduction.  It is encouraged that readers turn to that work for additional information on the mathematical relationships between the algorithms, as well as how linearization, kernelization and tensorization are applied in the graph-based setting.

\paragraph{Terminology}
Many dimensionality reduction methods in the literature are interested in analyzing relationships between samples defined on an undirected, weighted graph $G = \{ \mathcal{V}, \mathcal{E}, \bm{W} \}$, which consists of a finite set of \textit{vertices}  $\mathcal{V}$ (also called \textit{nodes} or \textit{points}) with cardinality $\mathcal{V}=N$, a set of \textit{edges} $\mathcal{E} \subset \mathcal{V} \times \mathcal{V} = [\mathcal{V}]^{2}$ (also known as \textit{arcs} or \textit{lines}) and a weighted \textit{adjacency} or \textit{affinity} matrix $\bm{W}$ \citep{Shuman2013SignalProcessingGraphs, Livi2013GraphMatchingProblem, Bengoetxea2002ThesisGraphMatching}.  The the size or \textit{order} of a graph is defined by the number of nodes $|\mathcal{V}|$ and edges $|\mathcal{E}|$.  If two vertices in $G$, say $\bm{u},\bm{v} \in \mathcal{V}$, are connected by an edge $e \in \mathcal{E}$, this is denoted by $e=(\bm{u},\bm{v})$ and the two vertices are said to be \textit{adjacent} or \textit{neighbors}.  When edges do not have a direction, they are coined as undirected.  A graph solely containing this type of connection is termed as an \textit{undirected graph}.  When all edges have directions, meaning $(\bm{u},\bm{v})$ and $(\bm{v},\bm{u})$ are distinguishable, the graph is said to be \textit{directed}.  In the literature, the term \textit{arc} is typically used to denote connections between nodes in directed graphs, while \textit{edge} is used when they are undirected.  The graph-based methods included in this literature review focus on analyzing affinities between data samples in undirected graphs.  Moreover, a \textit{path} between any two nodes in $\bm{u},\bm{u'} \in \mathcal{V}$ is a non-empty sequence of $k$ different vertices $<\bm{v}_{0}, \bm{v}_{1}, \dots, \bm{v}_{k}>$ where $\bm{u}=\bm{v}_{0},\bm{u}'=\bm{v}_{k}$ and $(\bm{v}_{i-1},\bm{v}_{i})\in \mathcal{E}$, $i=1,2,\dots,k$.  Additionally, a graph is said to be \textit{acyclic} if there are no cycles between its edges, regardless of whether it is directed or undirected.  

When using graphs for dimensionality reduction, vertices usually represent features of individual samples, and edges express relationships between them.  The most straight-forward way to construct a graph is to instantiate edges between every vertex in the graph, where each edge is weighted by the distance between the vertices it connects according to a pre-defined metric.  This type of graph is called \textit{full mesh}. Weights on edges are captured in the graph adjacency matrix $\bm{W}$.  When weights are not naturally defined by an application, a common way to the define the weight of an edge connecting vertices $\bm{u} \sim \bm{u}'$ is by a symmetric affinity function $W_{\bm{u},\bm{u}'} = K(\bm{u};\bm{u}')$; typically a \textit{radial basis function (RBF)} or \textit{heat kernel}, defined as:
\begin{align}
\bm{W}_{\bm{u},\bm{u}'}= w_{\bm{u},\bm{u}'} = \exp \left ( - \frac{|| \bm{u} - \bm{u}' ||^{2}}{\beta}  \right )
\end{align}
\noindent
where $\beta$ is the non-negative \textit{bandwidth} of the kernel.  Vertices will have a nonzero weight only if they fall within the nonzero mapping domain of the kernel. Additionally, a threshold could be set to truncate the weights of neighbors far from individual samples.


\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Examples of graphs.]{Examples of K-nearest neighbor and $\epsilon$-ball graphs}
		\label{fig:examples_of_graphs}
	\end{figure*}
\end{center}


\paragraph{$\bm{K}$-Nearest Neighbor Graph}
In a $K$-nearest neighbor graph, every data point (vertex) $\bm{x}_n \in \bm{X}$ is connected by edges to its $K$-nearest neighbors, where $K \in \mathbb{Z}^{+}$ is fixed. An example of a $K$-nearest neighbor graph is depicted in a of Figure \ref{fig:examples_of_graphs}. The downside of this graph is that it might impose edges between neighbors that should not actually be connected, as in the case where a sample is metrically distant from all of its nearest neighbors. Although, this feature may actually be useful in domains such as outlier detection, where low adjacency weights indicate that the sample is far form the sampling distribution.  Two alternative $K$-nearest neighbor graphs, a  symmetric and mutual neighbors, might instead by utilized.  In the symmetric $K$-nearest neighbors graph, two vertices $\bm{u}$ and $\bm{u}'$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{or} $\bm{u}'$ is among the neighbors of $\bm{u}$.  The mutual $K$-nearest neighbors graph, however, only connects vertices $(\bm{u},\bm{u}')$ if $\bm{u}$ is among the $K$-nearest neighbors of $\bm{u}'$ \textit{and} $\bm{u}'$ is among the $K$-nearest neighbors of $\bm{u}$.  The weights on each edge are provided as the similarity of the adjacent nodes.

\paragraph{$\bm{\epsilon}$-Neighborhood Graph}
Another method for graph construction is to use $\epsilon$-neighborhoods (or $\epsilon$-balls).  In this graph, two vertices $(\bm{u},\bm{u}')$ are connected by an edge if and only if the distance between them is equal to or smaller than some value $\epsilon$, $\mathcal{D}_{\mathcal{U}}(\bm{u},\bm{u}') \leq \epsilon$.  This idea is represented in b of Figure \ref{fig:examples_of_graphs}.  In both the $K$-nearest and $\epsilon$-neighborhood graphs, a parameter controlling the number of edges in the graph, $K$ or $\epsilon$, must be chosen.   These parameters are highly influential for graph construction and can thus greatly affect dimensionality reduction quality.  Contrary to the $K$-nearest neighbor graph, an $\epsilon$-neighborhood will not create connections between distant vertices.  However, when the data is sampled sparsely from a highly-curved manifold, the $\epsilon$-neighbor graph will not be able to appropriately capture the geometry \citep{Thorstensen2009ManifoldThesis}.

\paragraph{Geodesic Distance Approximation}
The ultimate goal of manifold learning is to uncover an underlying low-dimensional sub-manifold which is embedded in $\mathbb{R}^{D}$.  Many dimensionality reduction methods in the literature discover projections of data into a low-dimensional space which preserve topological ordering of the data \citep{Kegl2008PrincipalManifoldsTextbook}.  These processes require a notion of distance between samples.  \textit{Euclidean distance} is a popular metric which captures the straight-line disparity between two points. As shown in Figure \ref{fig:geodesic_distance}, however, samples that are actually distant on the manifold may appear deceptively close in the high-dimensional input feature space, as measured by Euclidean distance \citep{Tenenbaum2000Isomap}. \textit{Geodesic distance}, also called \textit{curvilinear} or \textit{shortest-path distance}, Figure \ref{fig:geodesic_distance}, on the other hand, follows the curvature of a manifold and may provide a better measure of dissimilarity between data samples.  Geodesic distance can be estimated by the shortest path through a graph constructed by assuming the distances between neighbors is locally Euclidean \citep{Sorzano2014DRReview}.  This can be conceptualized by a simple example.  The Earth is a sphere and naturally has curvature.  Two people standing in a room, however, would estimate the distance between themselves by a straight line.  Thus, in a very local region on the Earth, the measure of curvature would be negligible and the true distances between objects could be estimated with Euclidean distance. The same concept is true for manifolds where, if data is sampled densely enough, geodesic distance can be approximated by the shortest-path through a neighborhood graph where the dissimilarities between neighbors is assumed to be locally Euclidean.  Geodesic distance can be estimated efficiently by methods such as Dijskstra's or Floyd's shortest-path algorithms \citep{Tenenbaum2000Isomap}. 
\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{example-image-a}
		\caption[Demonstration of geodesic distance]{Demonstration of geodesic distance}
		\label{fig:geodesic_distance}
	\end{figure*}
\end{center}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  ISOMAP  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Isomap} \label{sec:Isomap}
\paragraph{Traditional Isomap}
While MDS has proven to be successful in a variety of applications, it suffers from the fact that is solely aims to retain pairwise Euclidean distances and does not consider the distributions of neighboring samples.  This implies that MDS is not able to capture the geometry of high-dimensional data which lies on or near to a curved manifold, such as the Swiss roll dataset \citep{VanDerMaaten2009DRReview,Chao2019RecentAdvancesSupervisedDimRed}. Isometric Feature Mapping (Isomap) \citep{Tenenbaum2000Isomap} is a technique which resolves this problem by attempting to preserve pairwise geodesic distances between datapoints.  Isomap can be considered as a generalization of classical MDS in which the pairwise distance matrix is replaced by a matrix of pairwise geodesic distances approximated by distances in the graph \citep{Thorstensen2009ManifoldThesis}.  The classic, unsupervised algorithm consists of a few steps:

\begin{enumerate}
\item Given a set of input data $\bm{X} = \{\bm{x}_{n}\}^{N}_{n=1} \subset \mathbb{R}^{D}$, construct a sparse neighborhood graph (such as the $K$-nearest  or $\epsilon$-ball graphs discussed previously) where each edge is weighted by the Euclidean distance between the neighbors it connects:
\begin{align}
	\bm{W}_{mn} = w_{mn} = ||\bm{x}_{m} - \bm{x}_{n} ||^{2} 
\end{align}
where $\bm{W}$ is the graph adjacency matrix.  

\item Next, the geodesic distances between all pairs of samples is computed by finding the shortest paths between the points through the graph.  This is commonly done with Dijkstra's or Flyod's shortest-path algorithms \citep{Tenenbaum2000Isomap}.  

\item These geodesic distances form a pairwise distance matrix which is substituted into classical MDS as described in Section \ref{sec:MDS}.  This provides the low-dimensional embedding coordinates $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}] \in \mathbb{R}^{N \times d}$ of high-dimensional input data  $\bm{X} = [\bm{x}^{T}_{1}, \dots, \bm{x}^{T}_{N}] \in \mathbb{R}^{N \times D}$, where $d \ll D$.

\end{enumerate}

While Isomap has been successfully applied in the areas of financial analysis \citep{Ribeiro2008SupervisedIsomap}, facial and object recognition \citep{Zhang2018IsomapMultiManifold}, visualization and classification tasks \citep{Vlachos2002NonlinearDRClassification}, a few important weaknesses are prevalent.  First, Isomap may be topologically unstable.  That is, it may construct erroneous connections in the neighborhood graph.  This is known as short-circuiting, and it can severely impair the performance  of Isomap.  Several approaches have been proposed to nullify the short-circuiting problem, such as removing datapoints with large total flows or by removing nearest neighbors that violate local linearity of the neighborhood graph \citep{VanDerMaaten2009DRReview}.  Another weakness of Isomap is that it may not perform correctly if there are holes  in the manifold, as this causes the geodesic distances of some samples to appear further on the manifold than  they truly are.  A third weakness is that Isomap can fail if the manifold is non-convex.  Therefore, we see that Isomap can perform very well due to theoretical guarantees on qualities such as convergence, as long as the manifold  is isometric to a convex open set of $\mathbb{R}^{d}$, $\mathcal{D}_{\mathcal{X}}(\bm{u},\bm{u}') = \mathcal{D}_{\mathcal{Y}}(f(\bm{u}),f(\bm{u}')) $, meaning that the geodesic distances in the graph are almost equal to the Euclidean distances in the embedding space $\mathbb{R}^{d}$.  Continuing, an additional drawback of Isomap is the fact that it requires the decomposition of a large, dense Gram matrix which scales with the number of training data points.  If the dataset grows too large, a solution will no longer be tractable.  Furthermore, the constraint on $\mathcal{X}$ to be isometric to a convex open set of $\mathbb{R}^{d}$ is rarely met.  As mentioned in \citep{Thorstensen2009ManifoldThesis}, these problems may be circumvented by sparsifying  large datasets using landmarks, as with Landmark Isomap \citep{deSilva2002IsomapReview} and looking at conformal maps, as is done in Conformal Isomap \citep{deSilva2002ConformalIsomap}. Finally, as with most nonlinear manifold learning techniques, it is nontrivial to embed out-of-sample data points into the lower dimensional feature space.  

\paragraph{Supervised Isomap Approaches}
As with most traditional manifold learning methods in the literature, Isomap is not inherently well-suited for classification tasks.  However, supervised approaches which consider class label information have been adopted to increase class separability in the latent embedding space. The work by Vllachos et al. \citep{Vlachos2002NonlinearDRClassification} was the first to investigate a supervised adaptation of Isomap.  Two supervised Isomap procedures were proposed which combine Isomap with a nearest neighbor classifier. These methods, Iso+Ada and WeightedIso take label information into consideration to scale the computed Euclidean distances utilized by Isomap by a constant factor according to class label. The idea is to make points closer in the embedding space if they have the same class label and farther if they have opposing class labels. Ribeiro et al. \citep{Ribeiro2008SupervisedIsomap} proposed an enhanced supervised Isomap (ES-Isomap) in which the dissimilarity matrix is weighted according to rules which consider class label information.  The dissimilarity matrix (considered as the adjacency matrix), $\bm{W}$, which was the same used in the Supervised Isomap method \citep{Geng2005SupNonlinearDimRed}, is defined as:
\begin{align}
	\bm{W}(\bm{x}_{m},\bm{x}_{n}) = \begin{cases} \sqrt{1 - \exp{\frac{-\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}}, \quad l_m = l_n \\  \sqrt{ \exp{\frac{\mathcal{D}^{2}(\bm{x}_{m},\bm{x}_{n})}{\beta}}} - \alpha, \quad l_m \neq l_n\end{cases}
	\label{eq:supervised_isomap_dissimilarity}
\end{align}
\noindent
where $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ denotes the distance measure between samples $\bm{x}_{m}$ and $\bm{x}_{n}$, $\beta$ is used to prevent $\bm{W}(\bm{x}_{m},\bm{x}_{n})$ from increasing too quickly when $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ is large and is typically set according to the density of the data, $\alpha$ is a constant in $[0,1]$ which controls the dissimilarity between points in different classes and keeps the graph from becoming disconnected, and $l_{m}$ and $l_{n}$ are the corresponding class labels of samples  $\bm{x}_{m}$ and $\bm{x}_{n}$, respectively.  In Equation \ref{eq:supervised_isomap_dissimilarity}, the dissimilarity between two points is greater than or equal to one if their class labels are different and less than 1 if the points have the same class label.  Therefore, the between-class dissimilarity will always be larger than the within-class, which is an important property for classification tasks.

Li and Guo proposed Supervised Isomap with Explicit mapping (SE-Isomap) in \citep{Li2006SupervisedIsomap}.  SE-Isomap enforces discriminability on the matrix of geodesic distances, as compared to the Euclidean distance matrix used in the aforementioned approaches, to learn an explicit mapping to the low-dimensional embedding space.  Finally, Zhang et al. \citep{Zhang2018IsomapMultiManifold} developed a semi-supervised Isomap to utilize both labeled and unlabeled data points in training.  This method aims at minimizing pairwise distances of within-class samples in the same manifold while maximizing the distances over different manifolds.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LLE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Locally Linear Embedding (LLE)}
\textit{Locally Linear Embedding} (LLE) was first introduced by Roweis and Saul \citep{Roweis2000LLE} and it is, along with Isomap, a foundational graph-based approach for nonlinear manifold learning.  Whereas Isomap attempts to preserve global pairwise distances, LLE attempts to preserve solely local properties of the data \citep{VanDerMaaten2009DRReview}.  Since LLE looks solely at local neighborhoods around samples, it is much less sensitive to short-circuiting than Isomap.  Furthermore, the preservation of local properties allows the algorithm to effectively embed non-convex manifolds.  Essentially, LLE looks to represent each data sample as a linear combination of its $k$-nearest neighbors.  This fits a hyperplane through every datapoint and it's nearest neighbors, thus assuming the intrinsic manifold is locally linear.  The local linearity assumption implies that the reconstruction weights $\bm{w}_{n}$ of high-dimensional sample $\bm{x}_{n}$ are invariant to transformations such as translation, rotation and scaling.  Therefore, it is assumed that the same reconstruction weights $\bm{w}_{n}$ that can be used to represent $\bm{x}_{n}$ in the high-dimensional space will also reconstruct it's low-dimensional representation $\bm{z}_{n}$ from its corresponding neighbors in the low-dimensional space \citep{Roweis2000LLE, Sorzano2014DRReview, Chao2019RecentAdvancesSupervisedDimRed}.  Therefore, low-dimensional data representations $\bm{Z}$ are found by minimizing the objective
\begin{align} \label{eq:LLE_objective}
	J(\bm{W},\bm{Z}) = \sum_{n=1}^{N} \left | \left | \bm{z}_{n} - \sum_{k=1}^{K}w_{nk}\bm{z}_{k} \right | \right |^{2} \quad s.t. \quad \frac{1}{N}\bm{Z}^{T}\bm{Z}=\bm{\mathbb{I}}_{d}
\end{align} 
\noindent
The constraint on the covariance of the embedded data representations is included to avoid the trivial solution $\bm{Z} = \bm{0}$.  Solving for the low-dimensional data representations is done in three steps \citep{Thorstensen2009ManifoldThesis}.  First, a $k$-nearest neighborhood graph $G$ is constructed from the high-dimensional data $\bm{X}$.  Next, the reconstruction weight vector $\bm{w}_{n}$ that best represents each point  $\bm{x}_{n}$ as a linear combination of its $k$-nearest neighbors is found by optimizing the problem
\begin{align} \label{eq:LLE_Weight_Equation}
	\min_{\bm{W}}\sum_{n=1}^{N} \left | \left | \bm{x}_{n} - \sum_{k=1}^{K}w_{nk}\bm{x}_{k} \right | \right |^{2} \quad s.t. \quad \sum_{k=1}^{K}w_{nk} =1
\end{align}
\noindent
Optimization of Equation \ref{eq:LLE_Weight_Equation} can be solved directly by using the method of Lagrange Multipliers. Finally, the low-dimensional embedding coordinates are found by minimizing the quadratic error function for $\bm{Z}$ according to Equation \ref{eq:LLE_objective}.  Roweis and Saul \cite{Roweis2000LLE} showed that the coordinates that minimize the objective can be found as the eigenvectors corresponding to the $d$ smallest nonzero eigenvalues of the inner product $(\bm{\mathbb{I}}_{N}-\bm{W})^{T}(\bm{\mathbb{I}}_{N}-\bm{W})$ where $\bm{W}$ is a $N \times N$ sparse matrix whose entries are equal to the corresponding reconstruction weight if $\bm{x}_{m}$ and $\bm{x}_{n}$ are connected in the neighborhood graph and $0$ otherwise.

The popularity of LLE has led to the development of linear variants, namely \textit{Neighborhood Preserving Projections} (NPP) and \textit{Orthogonal Neighborhood Preserving Projections} \citep{Pang2005NPP,Kokiopoulou2007OrthoNPP}, and has been applied successfully to applications in super-resolution and sound localization \citep{VanDerMaaten2009DRReview}.  However, LLE tends to collapse large portions of the data close to each other in the embedding space due to the constraint on the covariance matrix.  This may also result in undesired scalings of the manifold.  Despite these effects, supervised approaches based on LLE have also been developed \citep{Chao2019RecentAdvancesSupervisedDimRed,Li2009SupManifoldLearning}.  Existing approaches can be summarized by the LDA idea, that points in the same class should be embedded more closely to each other while points in opposing classes should be well-separated in the low-dimensional space.  This notion is realized by altering the dissimilarity matrix used to construct the neighborhood graph.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Hessian LE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hessian Eigenmaps}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LTSA  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Local Tangent Space Alignment (LTSA)}

\cite{Zhang2002LTSA}, \cite{VanDerMaaten2009DRReview}, \cite{Sorzano2014DRReview}

\textit{Local Tangent Space Alignment} (LTSA) 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  LE  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Laplacian Eigenmaps (LE)} \label{sec:Laplacian_Eigenmaps}
\paragraph{Classical LE}
Similar to LLE, Laplacian Eigenmaps, or \textit{Spectral Embedding}, is a nonlinear dimensionality reduction technique which aims to preserve local structure of data \citep{Raducanu2012SupervisedNonlinearDimReduction,VanDerMaaten2009DRReview}.  Using \textit{spectral graph theory}, LE computes low-dimensional representations of data in which the dissimilarities between datapoints and their neighbors (according to an affinity measure) are minimized.  The name \textit{Laplacian Eigenmaps} is derived by the use of Laplacian regularization in the optimization procedure \citep{Thorstensen2009ManifoldThesis}. Given a set of $N$ samples $\bm{X} = \{\bm{x}_n\}^{N}_{n=1} \subset \mathbb{R}^{D}$, the first step of LE is to define a \textit{neighborhood graph on the samples}.  This graph, also called an \textit{affinity} or \textit{adjacency} matrix can be constructed in a variety of ways, such as $K$-nearest neighbor, $\epsilon$-ball, full mesh, or by weighting each edge $\bm{x}_m \sim \bm{x}_n$ by a symmetric affinity function $W_{mn} = K(\bm{x}_m;\bm{x}_n)$, typically a radial basis or heat kernel:
\begin{align}
	\bm{W}_{mn}= w_{mn} = \exp \left ( - \frac{|| \bm{x}_m - \bm{x}_n ||^{2}}{\beta}  \right )
\end{align}

\noindent
where the kernel bandwidth $\beta$ is typically set as the variance of the dataset \citep{Raducanu2012SupervisedNonlinearDimReduction,Thorstensen2009ManifoldThesis}.

The goal is to uncover the latent data representations $\{ \bm{z}_n \}^{N}_{n=1} \subset \mathbb{R}^{d}$ where $d \ll D$ which minimizes the objective 
\begin{align}
	J(\bm{W},\bm{Z}) = \frac{1}{2} \sum_{m,n}^{}||\bm{z}_{m} - \bm{z}_{n} ||^{2}w_{mn} = tr(\bm{Z}^{T}\bm{L}\bm{Z})
\end{align}

\noindent
with $\bm{W}$ denoting the symmetric affinity matrix, $\bm{D}$ the diagonal weight matrix whose entries are the sum of the rows (or columns since $\bm{W}$ is symmetric) of $\bm{W}$ (i.e. $d_{mm} = \sum_{n}w_{mn}$, and is $0$ otherwise).  The graph Laplacian matrix is provided as $\bm{L} = \bm{D} - \bm{W}$.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is the $N \times d$ embedding matrix and $tr(.)$ denotes the trace of a matrix. The $n^{th}$ row of matrix $\bm{Z}$ provides the vector $\bm{z}_n$, which is the latent representation of sample  $\bm{x}_n$. This objective discourages projecting similar points in the input feature space to disparate regions of the embedding space by enforcing heavy penalization. 

The latent sample coordinates $\bm{Z}$ are found as the solution to the optimization problem:
\begin{align}
	\min_{\bm{Z}} tr(\bm{Z}^{T}\bm{L}\bm{Z}) \quad s.t. \quad \bm{Z}^{T}\bm{D}\bm{Z} = \bm{I}, \bm{Z}^{T}\bm{L}\bm{e} = \bm{0}
\end{align}

\noindent
where $\bm{I}$ is the identity matrix and $\bm{e} = (1, \dots, 1)^{T}$.  The first constraint eliminates the trivial solution $\bm{Z} = \bm{0}$ (scaling) and the second constraint avoids the trivial solution $\bm{Z} = \bm{e}$ (uniqueness). By applying the Langrange multiplier method and using the fact that $\bm{L}\bm{e} = \bm{0}$, the low-dimensional data representations can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{L}\bm{v} = \lambda \bm{D} \bm{v} \label{eq:trad_le_eig}
\end{align}
\noindent
The column vectors $\bm{v}_{1}, \dots, \bm{v}_{N}$ are the solutions of Equation \ref{eq:trad_le_eig}, ordered to the corresponding eigenvalues, in ascending order, $\lambda_{1} = 0 \leq \lambda_{2} \leq \dots \leq \lambda_{N} $. The embedding of the input samples given by the matrix $\bm{Z}$, is obtained by concatenating the eigenvectors of the $d$ smallest non-zero eigenvalues.  $\bm{Z}$ is a $N \times d$ matrix, where $d < N$ is the dimensionality of the embedded space.  From observation, it is clear that the embedding dimensionality is limited by the number of samples $N$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Linear LE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Linear LE}
Because of the representation ability of LE, a linear approach called \textit{Locality Preserving Projections} (LPP) was proposed by He and Niyogi \citep{He2003LPP}.  Similarly to LE, LPP builds a neighborhood graph which incorporates local information of the data.  LPP optimizes a similar objective to LE, however, it is assumed that the low-dimensional data representations come from a linear transformation of the high-dimensional input data.  The primary benefit of LPP over LE is that, while it shares many of the representation properties of LE, it is defined everywhere in the latent space as opposed to just over the training data points.  This allows for simple embeddings of out-of-sample test points.  Moreover, one can perform LPP in the original space or in a reproducing kernel Hilbert space through the use of Mercer kernels as described in Section \ref{sec:Kernels}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Supervised LE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Supervised LE (S-LE)}
In order to adopt LE for classification,Raducanu and Dornaika \citep{Raducanu2012SupervisedNonlinearDimReduction} proposed a supervised LE which minimizes the margin between samples with similar class labels and maximizes the margin between samples with opposing class labels.  Supervised LE utilizes discriminative information contained in the class labels when finding the nonlinear embedding (spectral projection). 

In order to discover both geometrical and discriminative manifold structure, supervised LE splits the global graph into two components: the within-class graph $G_{w}$ and the between-class graph $G_{b}$.  To define the margin, they define two subsets, $N_{w}(\bm{x}_{n})$ and $N_{b}(\bm{x}_{n})$ for each sample $\bm{x}_{n}$.  These two subsets contain the neighbors of $\bm{x}_{n}$ sharing the same label and having different labels, respectively, which have a similarity higher than the average.
\begin{align}
	N_{w}(\bm{x}_n) = \{\bm{x}_m |l_{m} = l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_within_neighbor}
\end{align}

\begin{align}
N_{b}(\bm{x}_n) = \{\bm{x}_m |l_{m} \neq l_{n}, \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)  > AS(\bm{x}_{n}) \} \label{eq:s_le_between_neighbor}
\end{align}

\noindent
where $AS(\bm{x}_{n}) = \frac{1}{N} \sum_{n=1}^{N} \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$ denotes the average similarity of the sample  $\bm{x}_{n}$ to the rest of the data.  From Equations \ref{eq:s_le_within_neighbor} and \ref{eq:s_le_between_neighbor} it is clear that the neighborhoods for each data sample are not necessarily the same size.  As a result, this function constructs the affinity graph according to both the local density and similarity between data samples in the input feature space.

With the two sets defined, the within-class and between-class weight matrices $\bm{W}_{w}$ and $\bm{W}_{b}$ are formed from the adjacency graphs $G_{w}$ and $G_{b}$, respectively.  These weight matrices are defined as:
\begin{align}
	W_{w,mn} =
	\begin{cases}
		\exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right), \text{ if } \bm{x}_{n} \in N_{w}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{w}(\bm{x}_{n})  \\
		0, \text{ otherwise}
	\end{cases}
\end{align}

\begin{align}
	W_{b,mn} =
	\begin{cases}
		1, \text{ if } \bm{x}_{n} \in N_{b}(\bm{x}_{m}) \text{ or } \bm{x}_{m} \in N_{b}(\bm{x}_{n}) \\
	0, \text{ otherwise}
	\end{cases}
\end{align}
\noindent
and the global affinity matrix, $\bm{W}$, can be written as:
\begin{align}
	\bm{W} = \bm{W}_{w} + \bm{W}_{b}
\end{align}

In order to obtain the low-dimensional representations $\bm{z}_n$ of the input data $\bm{x}_{n}$, the following objective functions can be optimized for $\bm{Z}$:
\begin{align}
	\min \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{w,mn} = tr(\bm{Z}^{T}\bm{L}_{w}\bm{Z})
\end{align}
\begin{align}
\max \frac{1}{2} \sum_{m,n} || \bm{z}_{m} - \bm{z}_{n} ||^{2} W_{b,mn} = tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z})
\end{align}
\noindent
where $\bm{L}_{w} = \bm{D}_{w} - \bm{W}_{w}$ and $\bm{L}_{b} = \bm{D}_{b} - \bm{W}_{b}$ indicate the corresponding graph Laplacians of the within-class and between-class affinity graphs, respectively.  The matrix $\bm{Z} = [\bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ contains the low-dimensional representations of the input samples in its rows.

By merging the two objective functions, the final optimization problem is formulated as:
\begin{align}
	\arg\max_{\bm{Z}} \left \{  \gamma tr(\bm{Z}^{T}\bm{L}_{b}\bm{Z}) + (1- \gamma)tr(\bm{Z}^{T}\bm{W}_{w}\bm{Z}) \right \} \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}
The term $\gamma$ is a scalar value in $[0,1]$ which determines the trade-off between pulling similar samples toward each other in the latent space and pushing heterogeneous points away.  A value of $\gamma = 1$ forces the objective to solely focus on maximizing the margin between dissimilar points.  Alternatively, a value of $\gamma = 0$ priorities the objective on embedding homogeneous samples in close spatial proximity. By defining matrix $\bm{B} = \gamma \bm{L}_{b} + (1 - \gamma)\bm{W}_{w}$, the problem becomes:
\begin{align}
	\arg\max_{\bm{Z}} \left ( \bm{Z}^{T}\bm{B}\bm{Z}  \right ) \quad s.t. \quad \bm{Z}^{T}\bm{D}_{w}\bm{Z} = \bm{I}
\end{align}

The low-dimensional embedding matrix $\bm{Z}$ can be found by solving the generalized eigenvalue problem:
\begin{align}
	\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v} \label{eq:sup_le_eig}
\end{align}

The column vectors $\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{N}$ are the generalized eigenvectors of 
Equation \ref{eq:sup_le_eig} arranged by descending eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \dots \lambda_{d}$.  Then the $N \times d$ embedding matrix $\bm{Z} =  [ \bm{z}^{T}_{1}, \dots, \bm{z}^{T}_{N}]$ is provided by concatenating the obtained eigenvectors $\bm{Z} = [\bm{v}_{1}, \bm{v}_{2}, \dots, \bm{v}_{d}]$.

The primary difference between the classic LE and S-LE is that traditional LE solely attempts to preserve the spatial relationships between samples, and thus, does not consider label information when learning the embeddings.  Alternatively, S-LE aims at aiding discriminant analysis by collapsing the distance between samples with the same label that are in close spatial proximity and pushing away spatial neighbors with differing class labels.  This is done through the utilization of two affinity graphs: the within-class and between-class graphs.  As with most graph-based methods, LE results vary highly according to the choice of neighborhood size.  However, choosing the size of $K$ or $\epsilon$ in advance can be very difficult. S-LE does not require user-defined graph parameters, other than those associated with the chosen affinity measure.  Instead, graph edges are chosen according to an adaptive neighborhood for each sample.   Both methods, however, suffer from inherent difficulties associated with nonlinear manifold learning, namely, selecting the intrinsic embedding dimensionality and handling out-of-sample extensions.

Despite these nuances, LE (and its variants) have been successfully applied in nonlinear dimensionality reduction tasks for facial recognition, spectral clustering and object classification \cite{VanDerMaaten2009DRReview}.

Apart from S-LE, other methods have been explored to integrate label information into Laplacian Eigenmaps.  A review of supervised dimensionality reduction methods by Chao et al. \citep{Chao2019RecentAdvancesSupervisedDimRed} explains that author's have optimized the affinity matrix using label information after constructing from spatial proximity, proposed deep learning-based approaches to achieve supervised LE and integrated label information into the affinity matrix construction process.  

The special feature exhibited by all Laplacian Eigenmap methods is the use of laplacian regularization, which enforces properties such as smoothness and provides a level of resistance toward the influences of outliers.  This useful feature has been applied in a variety of supervised and semi-supervised tasks, such as hyperspectral and synthetic aperture radar remote sensing classification \citep{Ratle2010ManRegHSI, Ren2017ManRegSAR}, classification of synthetic data \citep{Tsang2007ManifoldRegularization}, zero-shot learning \citep{Meng2018ManRegZeroShot} and reinforcement learning \citep{Li2015ManRegReinforcementLearning}.



\subsubsection{Diffusion Maps}

\subsubsection{Sammon Mapping} \label{sec:sammon_mapping}
Classical scaling, a convex technique for multidimensional scaling, was introduced in Section \ref{sec:MDS}.  As discussed, a pitfall of MDS is that it focuses on retaining global pairwise distances as opposed to local distances, which are typically much more important for capturing the geometry of the data \citep{VanDerMaaten2009DRReview}.  Several MDS variants have been proposed to address this weakness.  A popular variant is \textit{Sammon Mapping} \citep{Sammon1966SammonMapping}.

The classical scaling cost function looks to minimize the difference between the pairwise distance matrices of input data and their corresponding low-dimensional representations.  This cost puts emphasis on retaining the global data structure.  Sammon mapping adapts the classical scaling cost function by weighting the contribution of each sample pair $(\bm{x}_{m},\bm{x}_{n})$ by the inverse of their pairwise distances in the high-dimensional input space $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$.  In this way, the objective becomes
\begin{align}
	J(\bm{X},\bm{Z}) = \frac{1}{\sum_{m,n}\mathcal{D}(\bm{x}_{m},\bm{x}_{n})}\sum_{m \neq n} \frac{(\mathcal{D}(\bm{x}_{m},\bm{x}_{n}) - \mathcal{D}(\bm{z}_{m},\bm{z}_{n}))^{2}}{\mathcal{D}(\bm{x}_{m},\bm{x}_{n})}
\end{align}
\noindent
where $\mathcal{D}(\bm{x}_{m},\bm{x}_{n})$ denotes the Euclidean distance between high-dimensional input samples $\bm{x}_{m}$ and $\bm{x}_{n}$, $\mathcal{D}(\bm{z}_{m},\bm{z}_{n})$ is the Euclidean distance between low-dimensional data coordinates $\bm{z}_{m}$ and $\bm{z}_{n}$ and the constant in the front is added to simplify the gradient of the objective.  This cost function gives more weight to preserving distances between samples that are close in the input space.   Minimization of the Sammon objective function is typically performed with gradient descent or a pseudo-Newton method \citep{Sorzano2014DRReview}.

A weakness (and strength) of Sammon mapping is that it will give much more importance to retaining a very small distance, say $10^{-5}$, as compared to  $10^{-4}$.  Despite this, Sammon mapping has been successfully applied to visualization tasks and has reported on applications using gene data and geospatial information \citep{VanDerMaaten2009DRReview}.


\subsubsection{Maximum Variance Unfolding (MVU)}

\subsection{Latent Variable Models}

\subsubsection{General Latent Variable Model (GLVM)}

\paragraph{Factor Analysis (FA)}

\paragraph{Probabilistic PCA (PPCA)}
PCA can also be analyzed from the viewpoint of factor analysis (FA), which is discussed later in this literature review.  The basic idea, however, is that data observations $\bm{x}_{n} \in \mathbb{R}^{D}$ are realizations of a probability distribution with a prior on the lower-dimensional latent variable $\bm{z}_n \in \mathbb{R}^{d}$.  A typical choice on this model is a Gaussian-Gaussian conjugate prior pair where the mean of the data likelihood is a linear function of the latent inputs.  As an example, the prior over the hidden data representations can be expressed as Gaussian distribution

\begin{align}
p(\bm{z}_n) = \mathcal{N}(\bm{z}_n|\bm{\mu}_0, \bm{S}_0)
\end{align}

\noindent
and the data likelihood is denoted as a multivariate Gaussian

\begin{align}
p(\bm{x}_n|\bm{z}_n,\bm{\theta}) = \mathcal{N}(\bm{W}\bm{z}_n + \bm{\mu}, \bm{\Psi})
\end{align}

\noindent
where $\bm{W}$ is a $D \times d$ \textit{factor loading matrix}, and $\bm{\Psi}$ is a $D \times D$ covariance matrix.  The objective of FA is to compute the posterior over the latent factors in hopes that they will reveal something interesting about the data \citep{Murphy2012}.  Classical PCA assumes the data covariance to be $\bm{\Psi} = \sigma^2\bm{I}$ with $\sigma^2 \rightarrow 0$, thus the model is deterministic.  Alternatively, when $\sigma^2 > 0$,the projection is no longer orthogonal since it is shrunk toward the prior mean.  The trade-off is that the reconstructions of $\bm{x}_n$ with be closer to the data mean.


While the typical approach for fitting PCA is to use the method of eigenvectors of Singular Value Decomposition (SVD), it can also be fit with Expectation-Maximization (EM).  This formulation may be more computationally efficient for high-dimensional data.  By using Gaussian Processes, PPCA may also be extended to learn nonlinear mappings between the input and latent feature spaces \citep{VanDerMaaten2009DRReview}.

\paragraph{Supervised PCA}

Supervision has also been applied to PCA.  Two such examples, Supervised PCA and Discriminative Supervised PCA, were provided by Murphy in \citep{Murphy2012}.  The two are briefly described. 

\subparagraph{Supervised PCA (Latent Factor Regression)}
\textit{Supervised PCA} or \textit{Bayesian factor regression} is a model like PCA, except that the target variable (or label), $l_n$ is taken into account when learning the low-dimensional embedding.  For the case of binary classification, the Bayesian model can be decomposed into the following elements:
\begin{align}
	p(\bm{z}_n) = \mathcal{N}(0,\bm{I}_d)
\end{align}
\begin{align}
	p(l_n|\bm{z}_n) = \text{Ber}(\text{sigm}(\bm{w}^{T}_{l}\bm{z}_n))
\end{align}
\begin{align}
	p(\bm{x}_n|\bm{z}_n) = \mathcal{N}(\bm{W}_{x}\bm{z}_{n} + \bm{\mu}_{x}, \sigma^{2}\bm{I}_D)
\end{align}

\subparagraph{Discriminative Supervised PCA} 


\subsubsection{Generative Topographic Mapping (GTM)}

\subsection{Competitive Hebbian Learning}

\subsection{Principal Curves}

\subsection{Deep Learning}

\paragraph{Autoencoders}


\paragraph{Graph Convolutional Networks}

\paragraph{M3DNet}

\paragraph{Paper in Papers channel}

\subsection{UMAP}

\subsection{Stochastic Neighbor Embedding (SNE and t-SNE)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Manifold Charting  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Manifold Charting}

\subsection{NCA}

\subsubsection{M3DNet}

\subsubsection{Relations}

\subsubsection{Strengths and Weaknesses}

\subsection{Overview of Unsupervised Manifold Learning Methods}

\begin{longtable}{ |p{0.3\textwidth}|p{0.6\textwidth}| } 
	\caption{Overview of unsupervised manifold learning methods and their properties.}
	\label{tab:Manifold_Learning_Comparison_Table}\\
	\hline
	\multicolumn{2}{|c|}{\textbf{Overview of Unsupervised Manifold Learning}} \\
	\hline
	\multicolumn{2}{|c|}{\textbf{Linear}} \\
	\hline
	\textbf{Method} & \textbf{Strategy}\\
	\hline
	PCA   & Preserves variance of data, global structure \\
	\hline
	MDS  & Preserves pairwise distances, global structure  \\
	\hline
	LDA  & Minimizes within-class distance and maximizes between-class distance, global structure  \\
	\hline
	NMF   & Decomposes data into low-dimensional coordinates and basis vectors, minimizes approximation error.  \\
	\hline
	NPP  & Linear variant of LLE, preserves local neighborhoods, local structure   \\
	\hline
	LPP  & Linear variant of LE, preserves local neighborhoods, local structure   \\
	\hline
	\multicolumn{2}{|c|}{\textbf{Nonlinear}} \\
	\hline
	\textbf{Method} & \textbf{Strategy}\\
	\hline
	Isomap  & Preserves geodesic distances, global structure   \\
	\hline
	Sammon Mapping  & Preserves pairwise distances.  More emphasis is placed on samples that are close in the input space.  \\
	\hline
	LLE  & Preserves local neighborhoods, local structure   \\
	\hline
	Laplacian Eigenmaps  & Preserves local neighborhoods, local structure   \\
	\hline
	Hessian Eigenmaps  & ~  \\
	\hline
	Diffusion Maps  & ~  \\
	\hline
	LTSA  & ~  \\
	\hline
	MVU  & ~   \\
	\hline
	\multicolumn{2}{|c|}{\textbf{Probabilistic}} \\
	\hline
	\textbf{Method} & \textbf{Strategy}\\
	\hline
	FA  & ~   \\
	\hline
	PPCA  & ~   \\
	\hline
	GTM  & ~  \\
	\hline
	\multicolumn{2}{|c|}{\textbf{Neural Networks}} \\
	\hline
	\textbf{Method} & \textbf{Strategy}\\
	\hline
	SOM  & ~   \\
	\hline
	GNG  & ~   \\
	\hline
	Elastic Nets  & ~   \\
	\hline
	Autoencoders  & ~   \\
	\hline
	SNE  & ~   \\
	\hline
	UMAP  & ~   \\
	\hline
	NCA  & ~   \\
	\hline
	Manifold Charting  & ~   \\
	\hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  Weakly Supervised Manifold Learning  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weakly Supervised Manifold Learning and Dimensionality Reduction} \label{sec:weakly_sup_dim_reduction}
Although the specific feature vectors being used in remote sensing applications can be very high-dimensional, the underlying structure of a given dataset set is usually governed by only a few variables. Either implicitly or explicitly, most learning algorithms exploit this underlying structure to make learning and inference possible.
If it is available, relevant information for a specific task can generally be incorporated to provide supervision and improve the performance of unsupervised methods.  Supervised methods for nonlinear dimensionality reduction assume that the samples lie on a manifold parameterized by multiple latent factors.  However, different from traditional manifold learning (where the goal is to preserve the relationships between samples), these methods find the most discriminative low-dimensional representations for classification tasks \citep{Wu2015MILImageManifoldThesis}.  The optimal embedding uses class label information to  minimize distances between nearby points with the same class label while separating samples of different classes.  Although supervised manifold learning often outperforms unsupervised methods for classification tasks, this learning cannot be done directly in  MIL because of the uncertainty on the labels \citep{Carbonneau2016MILSurvey}.  Moreover, fully-annotated samples are often difficult or impossible to obtain in many remote sensing applications \citep{Zare2016MIACE}.  Even with the successes of manifold learning, most the of the previous work has mainly focused on either fully supervised or unsupervised learning.   Existing work in weakly supervised learning on manifolds has primarily considered the semi-supervised setting \citep{Zhang2008SpectralSemiSupManifoldLearning,Chen2018RobustSemiSupManifoldLearning,Zhang2014SemiSupManLearningFusion,Hong2019LearnableManifoldAlignment,Navaratnam2007JointManifoldSemiSupRegression,Stanley2019ManAlignmentFeatureCorrespondence,Tuia2015KernelManifoldAlignment,Wang2010MultiscaleManAlignment,Wang2011HeteroDomainAdaptationManAlignment}.  Methods in this category usually incorporate partially provided image labels and propagate the labels over the manifold approximated by the neighborhood graph on the images.  In a broad sense, however, different situations of weak supervision (specifically, Multiple Instance  Learning), have not been well studied \citep{Wu2015MILImageManifoldThesis}.  The existing MIL manifold learning in the literature can be broken into two paradigms: LDA-based approaches and sparse, orthogonal matrix-based techniques \citep{Zhu2018MIDRSparsity}. Thus, all existing approaches are linear, meaning they may not work well if the underlying bag manifold exhibits curvature.  Alternative weakly-supervised dimensionality reduction approaches have been proposed, however, they do not adhere to the constraints of MIL.  The current literature for weakly supervised dimensionality reduction is reviewed in the following sections, beginning with reviews of MIL DR approaches, namely: MIDR, MidLABS, CLFDA, MIDA and MI-FEAR.  Approaches using alternative definitions of weak learning are addressed at the end of the section.

\subsection{MIDR} 
The first true MIL manifold learning experimentation was performed in \citep{Sun2010MIDR} under the orthogonal matrix-based paradigm.  To show the need for MIL-specific methods, Sun et al. showed that Principal Component Analysis (PCA) failed to incorporate bag-level label information and thus provided poor separation between positive and negative bags.  Additionally, traditional Linear Discriminant Analysis (LDA) was used to project bags into a latent space which maximized between-bag separation, while minimizing within-bag dissimilarity.  However, LDA often mixed the latent bag representations due to the uncertainty of negative sample distributions in the positive bags.  These results have been shown many times in the literature \citep{Chao2019RecentAdvancesSupervisedDimRed,Vural2018StudySupervisedManifoldLearning}, so they were to be expected.  However, they motivated the need for specialized manifold learning methods that are directly applicable with MIL.  Therefore, Sun et al. proposed \textit{Multiple Instance Dimensionality Reduction} (MIDR), which optimizes an objective through gradient descent to discover sparse, orthogonal projection vectors in the latent space in conjunction with the Multiple Instance Logistic Regression classifier.  The goal of MIDR is to discover a projection matrix $\bm{W} \in \mathbb{R}^{D \times d}$ which will increase discriminability between positive and negative bags in the latent embedding space.  If given the $k^{th}$ training bag, $\bm{B}_{k} = \{ \bm{x}_{k,1}, \dots, \bm{x}_{k,n_{k}} \}$ with corresponding binary bag-level label $L_k \subset \{0,1\}$, MIDR attempts to find a matrix $\bm{W}$ such that the projection of $\bm{B}_{k} \subset \mathbb{R}^{D}$ by $\bm{W}^{T}\bm{B}_{k} \subset \mathbb{R}^{d}$ increases the separation between positive and negative bags. The intuition is that the probability of the $k^{th}$ bag being positive $\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k})$ should be close to one if it is positive and close to zero otherwise.  This can be achieved by minimizing the squared loss between the actual and predicted label of each bag
\begin{align}
	\min_{\bm{W}} \sum_{k=1}^{K} (\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k}) - L_{k})^{2}
	\label{eq:MIDR_orig_objective}
\end{align}
\noindent
Taking advantage of the standard assumption, the posterior probability of a bag can be written in terms of the posterior probabilities of its instances
\begin{align}
	\Pr(L_{k}=1|\bm{W}^{T}\bm{B}_{k}) = \max_{n} \Pr(l_{k,n}=1|\bm{W}^{T}\bm{x}_{k,n}) 
\end{align}
Equation \ref{eq:MIDR_orig_objective} then becomes 
\begin{align}
	\min_{\bm{W}} \sum_{k=1}^{K} (\max_{n} \Pr(l_{k,n}=1|\bm{W}^{T}\bm{x}_{k,n}) - L_{k})^{2}
	\label{eq:MIDR_instance_objective}
\end{align}
\noindent
From the objective, it is clear that in order to minimize the squared loss, the distances between the key positive instances and all negative instances should be as large as possible. Additionally, $\bm{W}$ is required to be orthogonal in order to guarantee the resulting latent features are uncorrelated (to remove redundancy) as well as sparse (to improve interpretability).  This implies that the new feature representations of instances $\bm{x}_{kn}$ in bag $\bm{B}_{k}$ are formed by linear combinations of the features in the input feature space.  The MIDR optimization problem can be written succinctly as defined in \citep{Zhu2018MIDRSparsity}:
\begin{align}
	\min_{\bm{W},\bm{\beta}} f(\bm{W},\alpha) + \gamma ||\bm{W} ||_{1}  \quad \text{s.t.} \quad \bm{W}^{T}\bm{W} = \mathbb{I}_{d}
	\label{eq:MIDR_objective}
\end{align}
\noindent
where $\gamma$ is a positive number which controls the balance between the sparsity term $||\bm{W}||_{1}$ and the fitting term $f(\bm{W},\alpha)=\sum_{k=1}^{K} (P_{k}(\bm{W},\bm{\beta}) - L_{k})^{2}$.  In this case, 
\begin{align}
	\begin{split}
	P_{k}(\bm{W},\bm{\beta}) &= softmax_{\alpha}(P_{k,1}(\bm{\bm{W}},\bm{\beta}), \dots, P_{k,n_{k}}(\bm{\bm{W}},\bm{\beta})) \\
	&= \frac{\sum_{n=1}^{n_{k}}P_{k,n}e^{\alpha P_{k,n}(\bm{W},\bm{\beta})}}{\sum_{n=1}^{n_{k}}e^{\alpha P_{k,n}(\bm{W},\bm{\beta})}}
	\end{split}
\end{align}
\noindent
is the softmax approximation over $n$ of $\max (P_{k,1}(\bm{\bm{W}},\bm{\beta}), \dots, P_{k,n_{k}}(\bm{\bm{W}},\bm{\beta}))$.  A popular way to estimate the posterior probability is by logistic regression
\begin{align}
	P_{k,n}(\bm{W},\bm{\beta}) = \Pr(l_{k,n}=1|\bm{W}^{T},\bm{x}_{k,n}) = \frac{1}{1+\exp(-\bm{\beta}^{T} \bm{W}^{T}\bm{x}_{k,n})}
\end{align}
\noindent
Pseudo-code for MIDR is provided in Algorithm \ref{alg:MIDR}.

\begin{algorithm}[h!]
	\caption{MIDR}
	\label{alg:MIDR}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\bm{L} =\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, sparsity parameter $\gamma$}
		\Ensure {Projection matrix $\bm{W}$}
		\While {Not converged}
		\State {Train multiple instance logistic regression $h$ using data $\bm{W}^{T}\bm{B},\bm{L}$}
		\For {$\bm{B}_{k} \in \bm{B}$}
		\State {$P_{k} \gets$ probability $h(\bm{B}_{k})$ }
		\EndFor
		\State {Optimize Equation \ref{eq:MIDR_objective} for new $\bm{W}$}
		\EndWhile            
	\end{algorithmic}
\end{algorithm}

In \citep{Sun2010MIDR}, gradient descent was used to optimize the objective.  An alternating optimization scheme was employed that switched between estimating the parameters of the MI Logistic Regression and solving for a new sparse, orthogonal embedding  matrix.  It was found that the newly developed method outperformed unsupervised instance-level dimensionality reduction approaches (applied to bags) for bag-level classification.  MIDR was later revisited by Zhu et al. where the optimization problem was reformulated using the \textit{intertial proximal alternating linearized minimization} (iPALM) method \citep{Zhu2018MIDRSparsity}.  The advantage of \textit{Multiple Instance Augmented Lagrangian Multiplier} (MI-ALM) this approach is that the problem variables can be managed sperately and updated effectively.  Additionally, the global convergence of MIDR was proved. 

\subsection{MidLABS}
The other existing approaches for dimensionality reduction with multiple instance learning follow a LDA scheme.  \textit{Multi-Instance Dimensionality reduction by Learning a mAximum Bag margin Subspace} (MidLABS) \citep{Ping2010MILDRMaxMargin} applies LDA to find a projection vector which simultaneously maximizes between-class scattering and minimizes within-class scattering to separate positive and negative bags in the embedding space.  While most MIL approaches assume instances are independently and identically distributed (IID), MidLABS represents each bag as a neighborhood graph in order to take advantage of data structure and jointly constructs  the scatter matrices by evaluating the scattering between bags. MidLABS optimizes the following objective:
\begin{align}
\max_{\bm{w}} J(\bm{w})= \max_{\bm{w}} \frac{\bm{w}^{T}(\sum_{L_{i} \neq L_{j}}\bm{K}_{ij})\bm{w}}{\bm{w}^{T}(\sum_{L_{i} = L_{j}}\bm{K}_{ij})\bm{w}}
\end{align}  
\noindent
By defining a bag distance measure, $\bm{K}$, the the between-class and within-class scatter matrices can be constructed as
\begin{align}
\bm{S}_{b} = \sum_{L_{i} \neq L_{j}}\bm{K}_{ij}
\end{align}
\noindent
and
\begin{align}
\bm{S}_{w} = \sum_{L_{i} = L_{j}}\bm{K}_{ij}
\end{align}
\noindent
It can be observed that this problem follows LDA exactly, and can thus be solved as the generalized eigenvalue problem:
\begin{align}
\max_{\bm{w}} J(\bm{w})= \max_{\bm{w}} \frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}
\end{align}
\noindent
In order to take structural information into account, the customized bag distance measurement is defined by:
\begin{align}
	\bm{K}_{ij} = \frac{\sum_{a=1}^{n_{i}}\sum_{b=1}^{n_{j}} (\bm{x}_{ia} - \bm{x}_{jb})(\bm{x}_{ia} - \bm{x}_{jb})^{T}  }{n_{i}n_{j}} + C\frac{\sum_{c=1}^{m_{i}}\sum_{d=1}^{m_{j}} (\bm{e}_{ic} - \bm{e}_{jd})(\bm{e}_{ic} - \bm{x}_{jd})^{T}  }{n_{i}^{2}n_{j}^{2}}
\end{align}
\noindent
where $\bm{x}_{ia}$ is the $a^{th}$ instance in the $i^{th}$ bag, $n_i$ is the total number of instances in the $i^{th}$ bag, $\bm{e}_{ic}$ is the $c^{th}$ edge in the $i^{th}$ bag and $m_{i}$ is the total number of edges in the $i^{th}$ bag.  This bag distance measurement represents each bag by as an $\epsilon$-graph, where each instance is treated as a node.  An edge exists between two  nodes  if the Euclidean distance between them is lower than a threshold, $\epsilon$. \citep{Latham2015MIFeatureRankingThesis}.  This measurement compares the closeness of bags by summing over all pairs of instances between the bags. Pseudo-code for MidLABS is provided in Algorithm \ref{alg:MidLABS}.

\begin{algorithm}[H]
	\caption{MidLABS}
	\label{alg:MidLABS}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$}
		\Ensure {Linear projection vector $\bm{w}$}
		\For{bag $\bm{B}_{k}$ in $\{ \bm{B}_{1}, \dots, \bm{B}_{K} \}$}   
		\State{$G_{k} \gets \epsilon$-graph for $\bm{B}_{k}$}
		\EndFor             
		\State{Define $\bm{K}_{ij} = \frac{\sum_{a=1}^{n_{i}}\sum_{b=1}^{n_{j}} (\bm{x}_{ia} - \bm{x}_{jb})(\bm{x}_{ia} - \bm{x}_{jb})^{T}  }{n_{i}n_{j}} + C\frac{\sum_{c=1}^{m_{i}}\sum_{d=1}^{m_{j}} (\bm{e}_{ic} - \bm{e}_{jd})(\bm{e}_{ic} - \bm{x}_{jd})^{T}  }{n_{i}^{2}n_{j}^{2}}$}    
		\State {$\bm{S}_{b} = \sum_{L_{i} \neq L_{j}}\bm{K}_{ij}$}
		\State{$\bm{S}_{w} = \sum_{L_{i} = L_{j}}\bm{K}_{ij}$}
		\State {Solve the generalized eigenvalue problem $\bm{S}_{b}\bm{w}=\lambda\bm{S}_{w}\bm{w}$}
		\State {Sort eigenvectors $\bm{w}$ by their eigenvalues $\lambda$}
		\State {$\bm{w} \in \mathbb{R}^{D \times 1} \gets$ top eigenvector }
	\end{algorithmic}
\end{algorithm}


\subsection{MIDA}
In 2014, Chia et al. introduced Multiple-Instance Discriminant Analysis (MIDA) \citep{Chai2014MIDA}.  MIDA has the same objective as MidLABS, which is to discover a linear projection basis which separates bags in the embedding space.  Both MIDA and MidLABS  can be considered as MI extensions of LDA. However, the way these algorithms construct their scatter matrices is very different.  While MidLABS constructs its  scatter matrices at the bag level by directly evaluating the scattering amongst bags, MIDA  uses instance-level information to formulate the within-class and between-class scatter.  In other words, MIDA selects a prototype for each bag and utilizes the selected instances as bag representatives for constructing the scatter.  The mean of all negative instances is used as the negative class prototype. The difficulty with this approach is the ambiguity on which instances are truly positive.  The other major difference between MidLABS and MIDA is that MIDA does not consider structural information of data when formulating the scatter matrices. To select candidate positive prototypes, MIDA initializes a set by selecting the most-likely positive instances as those with the lowest density in a Gaussian likelihood estimated by all instances in the negative bags.  An iterative optimization procedure is applied which trades-off between candidate positive instance selection and learning the projection weights into the latent space which maximizes the separation between bags with different labels and minimizes the distances between bags with the same label.  Pseudo-code for MIDA  is provided in Algorithms \ref{alg:MIDA_init} and \ref{alg:MIDA_opt}.

\begin{algorithm}[h!]
	\caption{Initialization of MIDA}
	\label{alg:MIDA_init}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K} \}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameter $\beta$}
		\Ensure {Initialized positive prototypes $\bm{x}^{+} = \{\bm{x}^{+}_{1}, \dots, \bm{x}^{+}_{N^{+}} \}$}
		\For{bag $\bm{B}_{k}$ in $\bm{B}^{+}$}   
		\State{$\bm{x}^{+}_{k} \gets \arg \min_{\bm{x}_{kn}}$ $C\sum_{m=1}^{N^{-}}\sum_{o=1}^{N^{-}_{m}} \exp \left ( \frac{||\bm{x} - \bm{x}^{-}_{mo} ||^{2}_{2}}{\beta} \right )$ $\forall n = 1, \dots, N^{+}_{k}$}
		\EndFor             
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h!]
	\caption{Projection vector calculation process of MIDA}
	\label{alg:MIDA_opt}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, Initialized positive prototypes $\bm{x}^{+} = \{\bm{x}^{+}_{1}, \dots, \bm{x}^{+}_{N^{+}} \}$}
		\Ensure {Linear projection vector $\bm{w}$}    
		\State {$\bm{\mu}^{+} \gets \frac{1}{N^{+}} \sum_{k=1}^{N^{+}} \bm{x}^{+}_{k}$}
		\State {$\bm{\mu}^{-} \gets C\sum_{m=1}^{N^{-}}\sum_{o=1}^{N^{-}_{m}} \bm{x}^{-}_{mo}$}        
		\State{$\bm{S}_{w}^{+} \gets \sum_{k=1}^{N^{+}}(\bm{x}^{+}_{k} - \bm{\mu}^{+})(\bm{x}^{+}_{k} - \bm{\mu}^{+})^{T}$}
		\State{$\bm{S}_{w}^{-} \gets \sum_{m=1}^{N^{-}}(\bm{x}^{-}_{m} - \bm{\mu}^{-})(\bm{x}^{-}_{m} - \bm{\mu}^{-})^{T}$}
		\State {$\bm{S}_{w} \gets \bm{S}_{w}^{+} + \bm{S}_{w}^{-}$}
		\State {$\bm{S}_{b} \gets \sum_{k=1}^{N^{+}} \sum_{m=1}^{N^{-}} (\bm{x}^{+}_{k} - \bm{x}^{-}_{m})(\bm{x}^{+}_{k} - \bm{x}^{-}_{m})^{T}$}
		\State {Solve the generalized eigenvalue problem $\bm{S}_{b}\bm{w}=\lambda\bm{S}_{w}\bm{w}$}
		\State {Sort eigenvectors $\bm{w}$ by their eigenvalues $\lambda$}
		\State {$\bm{w} \in \mathbb{R}^{D \times 1} \gets$ top eigenvector of $\frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}$}
	\end{algorithmic}
\end{algorithm}

\subsection{CLFDA} \label{sec:CLFDA}
Finally, Citation Local Fisher Linear Discriminant Analysis (CLFDA) \citep{Kim2010LocalDRMIL} incorporates citation and reference information into local Fisher Discriminant Analysis, thus it can also be treated as a MI extension to LDA.  Contrary to the previously-mentioned approaches, CLFDA operates at the instance-level, attempting to find a subspace where positive and negative instances are maximally-separable \citep{Latham2015MIFeatureRankingThesis}.  CLFDA can be viewed as complimentary to MIDA \citep{Chai2014MIDA}.  Whereas, MIDA tries to seek true positive instances in positive bags, CLFDA attempts to detect incorrectly labeled instances (false positive) in positive bags.  CLFDA pre-labels all instances with the label of their bag  $l_{kn} = L_{k}, \forall n = 1,\dots,n_{k}$, then utilizes neighborhood information to detect false positive instances.  In other words, it uses the assumption that if an instance in a positive bag is close to many points in the negative bags, it is likely also negative.  CLFDA defines \textit{references} simply as the nearest neighbors to $\bm{x}_{n}$, while $citers$ are defined as the samples which have $\bm{x}_{n}$ in their $C$-nearest neighbors, or $citers(\bm{x}_{n}) = \{ \bm{x}_{m}| \bm{x}_{n} \in CNN(\bm{x}_{m}), m=1, \dots, N \}$.   The steps of CLFDA are to 1.) construct a $\max(R,C)$-NN graph, where the $\max(R,C)$-NN graph is a $K$-NN graph with $K=\max(R,C)$.  This is used to detect false positives and re-label them as negative.  If the ratio of instances from negative bags versus instances from positive bags in the references and citers of $\bm{x}_n$ exceeds a threshold $\tau$, then $\bm{x}_{n}$ is given a negative label. This is only done for instances from positive bags, as the SMI assumptions states that all instances in negative bags should be negative. 2.) Construct scatter matrices using the provided positive and negative instances. 3.) Find projection weights which simultaneously maximize the distances between positive and negative bags and minimizes the distances between bags with the same class labels using \textit{Local Fisher Discriminant Analysis} (LFDA).  LFDA is a version of LDA  designed to address multi-modal data distributions.  The primary difference between LDA and LFDA is that when maximizing and minimizing the between-class and within-class scatter matrices, respectively, the scatter contribution of a single instance pair is weighted by the locality of the pair.  This prevents instances from different clusters sharing the same label from being forced into  the same space.  This also allows multiple dimensions to be analyzed in the embedding space, whereas LDA is limited to a single dimension for binary classification.  Pseudo-code for CLFDA is given in Algorithm \ref{alg:CLFDA} \citep{Latham2015MIFeatureRankingThesis}.  A potential downside of CLFDA, however, is that it has been shown that pre-labeling all instances in positive bags as positive often leads to poor results, especially when there are large numbers of negative instances in the positive bags \citep{Chai2014MIDA}.

\begin{algorithm}[h!]
	\caption{CLFDA}
	\label{alg:CLFDA}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $C,R,\tau$}
		\Ensure {Projection matrix $\bm{W}$}
		\State {$\bm{X} \gets$ instances in $\bm{B}$}
		\State {$G \gets \max(R,C)$-nearest neighbor graph of $\bm{X}$}
		\For {$n = 1 \to N$}   
		\State {$R_{n} \gets R$-nearest references of instance $\bm{x}_{n}$}
		\State {$C_{n} \gets C$-nearest citers of instance $\bm{x}_{n}$}
		\State {$N_{n}^{-} \gets $-number of instances from negative bags in  $R_{n} + C_{n}$}
		\State {$N_{n}^{+} \gets $-number of instances from positive bags in  $R_{n} + C_{n}$}
		\If {$\frac{N_{n}^{-}}{N_{n}^{+}} \geq \tau$ or $\bm{x}_{n}$ is from a negative bag}
		\State {instance label $l_{n} \gets -1$}
		\Else
		\State {$l_{n} \gets +1$}
		\EndIf
		\EndFor             
		\State{$\bm{A}^{b} \gets$ between-class affinity matrix}
		\State{$\bm{A}^{w} \gets$ within-class affinity matrix}
		\State{Between-class scatter matrix $\bm{S}_{b} = \frac{1}{2}\sum_{m,n=1}^{N}\bm{A}_{mn}^{b}(\bm{x}_{m}-\bm{x}_{n})(\bm{x}_{m}-\bm{x}_{n})^{T}$}
		\State{Within-class scatter matrix $\bm{S}_{w} = \frac{1}{2}\sum_{m,n=1}^{N}\bm{A}_{mn}^{w}(\bm{x}_{m}-\bm{x}_{n})(\bm{x}_{m}-\bm{x}_{n})^{T}$}
		\State {$\bm{W} \in \mathbb{R}^{D \times d} \gets$ top $d$ eigenvectors of $\frac{\bm{w}^{T}\bm{S}_{b}\bm{w}}{\bm{w}^{T}\bm{S}_{w}\bm{w}}$}
	\end{algorithmic}
\end{algorithm}

As previously mentioned, all existing MIL dimensionality reduction approaches in the literature are solely linear.  However, many modalities exhibit nonlinear variation.  Therefore, nonlinear manifold learning approaches should be developed which operate under the multiple instance learning framework. 

\subsection{MI-FEAR}
While the previously mentioned approaches were based on feature extraction,  multiple instance dimensionality reduction has also been investigated under the feature selection paradigm.  Specifically, Latham used feature ranking to determine the most important features for instance-level classification \citep{Latham2015MIFeatureRankingThesis}.  Feature ranking considers each feature independently according to a scoring function, thus revealing properties of the individual features by which they may be compared and ranked.  The challenge of learning a good feature ranking under an instance-space metric is that the instance labels are not available under the MIL framework.  \textit{Multiple-Instance Feature Ranking} (MI-FEAR) assumes one-sided noise by giving every instance the label of its corresponding bag.  This essentially transforms the weakly-supervised problem into a supervised one.  Pseudo-code for MI-FEAR is provided in Algorithm \ref{alg:MIFEAR}.

\begin{algorithm}[h!]
	\caption{MI-FEAR}
	\label{alg:MIFEAR}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\bm{L} = \{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, evaluation metric $\mathcal{L}$, learning algorithm $\mathcal{A}$, dimensionality of reduced-dimensional space $d$}
		\Ensure {$[ \theta_{i}$ for $ \theta_{i}$ in $V[1,\dots,d]  ]$}
		\State {$\bm{X}, \bm{Y}^{\gamma} \gets $ supervised dataset of bag-labeled instances using $(\bm{B}, \bm{L})$}
		\State {$V \gets $ [ ]}
		\For {feature $\theta_{i}$ in feature set $\Theta$}   
		\State {$\bm{X}^{\theta} \gets \bm{X}$ with all features, excluding $\theta_{i}$}
		\State {$h_{\theta} \gets $ output of $\mathcal{A}$ when trained on input $(\bm{X}^{\theta}, \bm{Y}^{\gamma})$}
		\State {$V[i] \gets$ performance of $h_{\theta}$ on input $(\bm{X}^{\theta}, \bm{Y}^{\gamma})$ according to $\mathcal{L}, \theta_{i}$}
		\EndFor             
		\State {$V \gets$ $\textsc{SortDecreasing}(V)$}
	\end{algorithmic}
\end{algorithm}
\noindent
Each feature $\theta$, is given a score based on the performance of a learned hypothesis $h_{\theta}$ operating on a single feature.  The performance is determined an evaluation metric $\mathcal{L}$.  Once a score has been assigned to every feature, the features are ranked according to relative importance and the top $d$ features are retained as the feature set.  As stated in Section \ref{sec:CLFDA}, the characteristic accuracy of MI-FEAR is based on noisy labels, where each instance is given its corresponding bag-level label.  This approach has proven non-ideal in the literature for determining accurate instance-level classification.  However, a benefit of feature selection is that, unlike feature extraction, the reduced-dimensional space retains its interpret-ability.  In other words, the representations of individual features do not change, meaning they keep their real-world relevance, if applicable.  On the other hand, feature selection requires that a subset of useful features for classification already exist in the training set.  Empirical results showed that consistently achieved lower instance-level classification accuracy than CLFDA and MidLABS, but had a significantly lower run-time.

\subsection{Comparison Table of MI Dimensionality Reduction Methods}
Table \ref{tab:MIDRComparison} shows a comparison between the multiple instance dimensionality reduction methods reviewed in Section \ref{sec:weakly_sup_dim_reduction}.

\begin{longtable}{ |p{2cm}|p{6cm}|p{2cm}|p{3cm}|  } 
	\caption{Summary of multiple instance dimensionality reduction approaches.}
	\label{tab:MIDRComparison}\\
	\hline
	\multicolumn{4}{|c|}{\textbf{Multiple Instance Dimensionality Reduction}} \\
	\hline
	\textbf{Method} & \textbf{Summary} & \textbf{Reduction Method} & \textbf{Classification Level}\\
	\hline
	MIDR   & Finds a sparse, orthogonal linear projection matrix optimized for bag-level logistic regression   &Linear, orthogonal projection  &  bag-level\\
	\hline
	MidLABS&  Finds linear projection vector using LDA defined from bag-similarity kernel  & LDA   & bag-level\\
	\hline
	MIDA &Learns linear projection vector using LDA defined from bag representative vectors & LDA &  instance-level\\
	\hline
	CLFDA  &Learns linear projection matrix using local discriminant analysis defined from instance scatter.  Instance labels are provided as bag labels and refined. & LFDA &  instance-level\\
	\hline
	MI-FEAR &  Incrementally leaves out feature and evaluates performance loss to provide feature score  & Feature selection & instance-level\\
	\hline
\end{longtable}

\subsection{General Weak Supervision}
Alternative approaches to weakly supervised dimensionality reduction that do not follow the MIL framework have also been proposed.  For example, Wu studied weakly supervised manifold learning for manifold factorization using image-level labels, where the labels were the variation of interest.  The primary idea was to use weak labels to find image pairs that should be more similar after removing unwanted image variation.  Wu used an alignment method constrained by Hessian regularization to learn a manifold regression, such that new test images would be projected smoothly into a space near neighboring input images \citep{Wu2015MILImageManifoldThesis}.  Gaur et al. used weakly supervised manifold learning to perform dense semantic object correspondence \citep{Gaur2017MILSemanticObjectCorrespondence}. The objective of the semantic object correspondence problem is to compute dense association maps for a pair of images such that the same object parts get matched, even for very differently appearing object instances. The goal of Gaur's work was to learn a manifold such that features belonging to the same semantic object parts were projected closer to each other on the manifold. This was achieved by re-purposing deep convolutional features from a classification network, where the labels were weak segmentations of object parts.  These features were then projected onto a manifold using LDA. \textit{Hierarchical Agglomerative Clustering} (HAC) was performed in the embedding space and the labels were refined with respect to geodesic distance on the manifold.  This method was inspired by the \textit{manifold assumption}, which states that similar objects (even though disparate in the input feature space), should share an intrinsic manifold.  In this way, feature embeddings are learned by an optimization process which is rewarded for projecting features closer on the manifold if they have low feature-space dissimilarity.  Additionally, the optimization penalizes feature clusters whose geometric structure is inconsistent with the observed geometric structures of object parts.  

An alternative approach for discriminative dimensionality reduction with weak labels is through the application of metric embedding, which is discussed in detail in Section \ref{sec:MetricEmedding}.  While they can be fully supervised, metric embedding techniques often consider groups of samples (usually two or three), jointly, to learn an embedding function.  Under this type of weak supervision, only a notion of semantic similarity is needed,  as compared to direct class labels. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Metric Embedding %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metric Embedding} \label{sec:MetricEmedding}

The concepts of ``near" and ``far" are very powerful and useful utilities in everyday life.  They classify the relationship between two ``primitives" as being similar or dissimilar, as well as the degree of compatibility \citep{Thorstensen2009ManifoldThesis}. As an example, a medical doctor might consider a machine learning researcher and a software engineer as being similar (near), because they both perform research for computer applications.  However, the same researcher and engineer would likely consider their jobs as being very disparate (far) based on the details of their work.  In order to capture this abstraction of distance in a mathematical construct, a \textit{metric space} is defined.

 \theoremstyle{definition}
 \begin{definition}{Metric Space}
 A \textit{metric space} is an ordered pair $(\mathcal{X},\mathcal{D})$ where $\mathcal{X}$ is a set and $\mathcal{D}$ is a metric on $\mathcal{X}$, or $\mathcal{D}:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ such that $\forall \bm{x},\bm{y},\bm{z}\in\mathcal{X}$, the following holds:

 \end{definition}
 	\begin{enumerate}
	\item Non-negativity: $\mathcal{D}(\bm{x},\bm{y}) \geq 0$
	\item Identity: $\mathcal{D}(\bm{x},\bm{y}) = 0 \iff \bm{x} = \bm{y} $
	\item Symmetry: $\mathcal{D}(\bm{x},\bm{y}) = \mathcal{D}(\bm{y},\bm{x})$
	\item Triangle Inequality:  $\mathcal{D}(\bm{x},\bm{z}) \leq \mathcal{D}(\bm{x},\bm{y}) + \mathcal{D}(\bm{y},\bm{z})$
	\end{enumerate}
\noindent
The non-negativity rule states that the metric evaluated on two instances must have a positive value or be equal to zero.  From the Identity rule, we can see that the metric may only be defined as zero if the two instances being evaluated are exactly the same (thus the dissimilarity is zero).  The Symmetry rule states that a metric evaluated between two instances must be the same regardless of ordering.  Finally, the Triangle Inequality says, intuitively, that the direct distance between two instances $\bm{x}$ and $\bm{z}$ is smaller than the distance between $\bm{x}$ and $\bm{y}$ plus the distance between $\bm{y}$ and $\bm{z}$.  Both sides of the inequality will be equal if and only if $\bm{y}$ lies on the path between $\bm{x}$ and $\bm{z}$ on which the metric is defined.  \newline
The goal of \textit{metric embedding learning} it to learn a function $f_{\theta}(\bm{x}):\mathbb{R}^{D} \rightarrow \mathbb{R}^{d}$ which maps semantically similar points from the data input feature space of $\mathbb{R}^{D}$ onto \textit{metrically close} points in $\mathbb{R}^{d}$.  Similarly, $f_{\theta}$ should map semantically different points in $\mathbb{R}^{D}$ onto metrically distant points in $\mathbb{R}^{d}$.  The function $f_{\theta}$ is parameterized by $\theta$ and can be anything ranging from a linear transformation to a complex non-linear mapping as in the case of deep artificial neural networks \citep{Hermans2017DefenseTripletLoss}.  Let $\mathcal{D}(\bm{x},\bm{ y}): \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a metric function measuring similarity or dissimilarity in the embedded space.  For succinctness, $\mathcal{D}_{m,n} = \mathcal{D}(f_{\theta}(\bm{x_{m}}),f_{\theta}(\bm{x_{n}}))$
defines the dissimilarity between samples $\bm{x}_{m}$ and $\bm{x}_{n}$, after being embedded.  It should be noted that, unlike \textit{metric learning} where the objective is to learn an appropriate metric to measure dissimilarity between samples in opposing classes, metric embedding attempts to learn a transformation function such that samples in the embedding space adhere to a pre-defined measure of similarity \citep{Hermans2017DefenseTripletLoss}.  Metric embedding is often realized through weakly-supervised learning, where instead of labels, the data is accompanied with sets of preferences.

	\subsection{Ranking Loss}
	Unlike other loss function such as mean-squared error or cross-entropy whose objective is to directly compare labels, values or sets of values assigned to a given input, the objective of \textit{ranking loss} (also called \textit{margin loss}) functions is to measure relative distances between sets of inputs.  To use a ranking loss in a learning scenario, sets of inputs (usually two or three) are embedded through a transformation function into a defined metric space.  A metric is used to measure similarity (often Euclidean distance), and the transformation function is updated such that points which have a higher semantic similarity label are embedded more closely, and points which are dissimilar are pushed further apart, according to the metric.  In this framework, the actual values of the embedded features are ignored.  Instead, only the distances between them matter.  While ranking losses have been developed which consider many datapoints at a time \citep{Sohn2016NPairLoss}, the most popular ranking loss functions in the literature are based on \textit{contrastive} \citep{Koch2015SiameseNetworks} and \textit{triplet} \citep{Schroff2015FaceNet} losses.
	
	\subsubsection{Contrastive Loss}
	Let $\bm{x} \in \mathcal{X}$ be data with corresponding instance-level labels $l \in \{l_{1}, \dots, \l_{N} \}$.  The superscripts $\bm{x}^{a}, \bm{x}^{p}, \bm{x}^{n}$ are used to denote \textit{anchor}, \textit{positive} and \textit{negative} samples, respectively.  The anchor point is the sample of interest, while the positive is a sample of the same class (or deemed to be similar to the anchor) and the negative sample is a point of a different class (or dissimilar to the anchor).
	\textit{Contrastive loss} functions take pairs of examples as input and learns and embedding function to predict whether two inputs are from the same class or not.  Specifically, contrastive loss can be written as:
	\begin{align}
		\begin{split}
		\mathcal{L}^{\alpha}_{\text{cont}}(\bm{x}_{m},\bm{x}_{n};f_{\theta}) = &\bm{1}\{l_{m}=l_{n} \} \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n}))\\ + &\bm{1}\{l_{m} \neq l_{n} \} \max(0,\alpha - \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n})))
		\end{split}
	\label{eq:contrastive_loss}
	\end{align}
	\noindent
	where $\alpha$ is a margin parameter imposing a distance between different classes to be larger than $\alpha$.  Analyzing Equation \ref{eq:contrastive_loss}, it can be observed that this loss has two unique terms.  If the label of $\bm{x}_{m}$ and $\bm{x}_{n}$ are the same (anchor and positive pair), only the first half of the loss is computed.  This term is simply the dissimilarity between the samples in the embedding space.  Ideally, this term should equate to zero, thus implying the terms are close to each other in the embedding space.  If the labels of the samples are opposing (anchor and negative pair), the second term is computed.  This equates to the maximum value between zero and the difference between the margin constraint and the dissimilarity between the samples.  Thus, if the distance between  the anchor and negative is greater than the margin $\alpha$, the objective is met and the loss equates to zero.  Otherwise, the loss is positive.  Figure \ref{fig:contrastive_loss} demonstrates the idea of contrastive loss.  If given a positive anchor pair, the transformation function should embed the samples closer in the embedding space.  If given a negative pair, the opposite should occur.  It is obvious that with each sample pair the embedding function is only updated in one way (pushing or pulling) \citep{Sohn2016NPairLoss,Koch2015SiameseNetworks}.  
	
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=0.5\textwidth]{example-image-a}
			\caption[Pairwise Loss]{}
			\label{fig:contrastive_loss}
		\end{figure*}
	\end{center}
	
	
	\subsubsection{Triplet Loss}
	Whereas contrastive loss considers pairs of samples, triplet loss jointly optimizes between three samples $\{\bm{x}^{a}, \bm{x}^{p}, \bm{x}^{n}\}$, the anchor, a positive and a negative \citep{Hermans2017DefenseTripletLoss,Schroff2015FaceNet, Deng2019ArcFaceAngularMarginLoss}. The fundamental idea behind triplet loss is that, for an input sample, we desire to shorten the distances between its embedded representation and those of similar examples, while simultaneously enlarging the distances between dissimilar examples \citep{Sohn2016NPairLoss}. Triplet loss can be written as:
	\begin{align}
	\mathcal{L}^{\alpha}_{\text{tri}}(\bm{x}^{a},\bm{x}^{p},\bm{x}^{n};f_{\theta}) = \max(0,\mathcal{D}(f_{\theta}(\bm{x}^{a}),f_{\theta}(\bm{x}^{p})) - \mathcal{D}(f_{\theta}(\bm{x}^{a}),f_{\theta}(\bm{x}^{n})) + \alpha )
	\label{eq:triplet_loss}
	\end{align}
	\noindent
	From Equation \ref{eq:triplet_loss}, it can be observed that the triplet loss is satisfied whenever the distance between the negative and anchor is greater than the distance between the corresponding positive sample and the anchor by a margin $\alpha$.  Whereas contrastive loss simply pushes or pulls, triplet loss does both, simultaneously.  This is depicted visually by Figure \ref{fig:triplet_loss}.
	
	\begin{center}
		\begin{figure*}[h]
			\centering
			\includegraphics[width=0.5\textwidth]{example-image-a}
			\caption[Triplet Loss]{}
			\label{fig:triplet_loss}
		\end{figure*}
	\end{center}

	The advantage of this formulation is that, while all points of the same class will eventually form a cluster, they are not required to collapse to a single point; they just need to be close to each other than to any point from a different class \citep{Hermans2017DefenseTripletLoss}.  A major drawback of triplet loss, however, is that as datasets grow larger, the possible number of triplets grows cubically.  This can cause practical issues as many of the triplets will likely already satisfy the margin constraints and will not contribute toward learning.  Therefore, Schroff et al. argue that \textit{hard-mining} is often imperative for the success of triplet-based methods \citep{Schroff2015FaceNet}.   Essentially, three scenarios exists.  \textit{Easy triplets} are triplet sets which already satisfy the margin constraint, thus inducing zero loss.  \textit{Hard triplets} are sets where the negative sample is closer to the anchor than the positive.  The loss is positive (and greater than $\alpha$).  \textit{Semi-hard triplets} are sets where the negative point is more distant to the anchor than the positive, but it is not greater than the margin $\alpha$, so the loss is still positive (but smaller than $\alpha$).  Therefore, careful consideration should be taken in triplet construction to maximize the number of hard and semi-hard triplets shown to the learning algorithm. 
	
	Alternative approaches such as the ones in \citep{Sohn2016NPairLoss}, \citep{Deng2019ArcFaceAngularMarginLoss} and \citep{Xu2014LargeMarginWeaklySupervisedDR} use modifications of contrastive or triplet loss to obtain feature representations which adhere to a desired metric embedding.

		\subsection{Large-Margin K-Nearest Neighbors (LMNN)}
		
		Weinberger and Saul \citep{Weinberger2009LMNN} explored the topic of ranking loss with the explicit goal of performing $k$-nearest neighbor classification in the learned embedding space \citep{Hermans2017DefenseTripletLoss}.  This approach is based on the \textit{Large Margin Nearest Neighbor loss} for optimizing $f_{\theta}$, defined as:
		\begin{align}
			\mathcal{L}_{\text{LMNN}}(\theta) = (1-\mu)\mathcal{L}_{\text{pull}}(\theta) + \mu \mathcal{L}_{\text{push}}(\theta)
		\end{align}
		\noindent
		which is comprised of a \textit{pull}-term that pulls data points toward its \textit{target neighbors}, or its nearest neighbors from the same class, and a \textit{push}-term that pushes data points from a different class.  The term $\mu$ is a trade-off parameter used to control the priority of pushing or pulling. The loss terms are defined as:
		\begin{align} \label{eq:lmnn_pull}
			\mathcal{L}_{\text{pull}}(\theta) =  \sum_{m,n}\eta_{mn} \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n}))
		\end{align}
		\begin{align} \label{eq:lmnn_push}
		\mathcal{L}_{\text{push}}(\theta) = \sum_{m,n,o}\eta_{mn}(1-l_{m,o}) \max(0,\mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{n})) - \mathcal{D}(f_{\theta}(\bm{x}_{m}),f_{\theta}(\bm{x}_{o})) + \alpha )
		\end{align}
		\noindent
		where  $\eta_{mn} \in \{0,1\}$ is a binary indicator variable used to express whether input $\bm{x}_{n}$ is a target neighbor of input $\bm{x}_{m}$ and $\alpha$ is a margin enforcing distance between the classes.  Analyzing Equations \ref{eq:lmnn_pull} and \ref{eq:lmnn_push}, it can be observed that the pull term only penalizes large distances between inputs and target neighbors ($k$-nearest neighbors with the same class label).  The push term, however, is comparable to the triplet loss, which enforces that the dissimilarities between the anchor sample $\bm{x}_{m}$ and \textit{all negative samples} is greater than the dissimilarities between the anchor and its corresponding target neighbors by at least a margin $\alpha$.  Figure \ref{fig:lmnn} demonstrates the pushing and pulling idea captured by LMNN.  In the rest of the algorithm, a matrix $\bm{M}$ is learned which captures an appropriate Mahalanobis metric to measure the dissimilarity between samples, optimized for the LMNN loss.  
		
		
		\begin{center}
			\begin{figure*}[h]
				\centering
				\includegraphics[width=0.5\textwidth]{example-image-a}
				\caption[Large Margin $k$-NN]{}
				\label{fig:lmnn}
			\end{figure*}
		\end{center}
		
		
		\subsection{Siamese Neural Networks}
		\textit{Siamese Neural Networks} are a unique neural architecture proposed by Bromley et al. used to rank similarity between inputs \citep{Bromley1993SiameseNetworks}.  Essentially, Siamese networks use a base architecture to perform feature encoding.  This base network is replicated to form twin networks which share parameters \citep{Koch2015SiameseNetworks,Koch2015SiameseNetworksThesis}. The two networks are tied at the output, where a distance metric is used to compare the embeddings of the inputs to each twin.  Figure \ref{fig:siamese_networks_conrastive} demonstrates this idea.  The base network can be any architecture (i.e. fully-connected, convolutional, recurrent) so  long as it produces a fixed-sized embedding vector at its output.  A common implementation is to embed sample pairs using a contrastive loss function \citep{Koch2015SiameseNetworks}.  In this scenario, pairs of samples (anchor and positive or anchor and negative) are passed into the twin networks.  An embedded representation is produced for each sample, and a dissimilarity metric (typically $L_{2}$ or $L_{1}$ norm) is applied to compare the samples.  Thus, the actual values of the features do not matter, and only the relative distance between them in the embedding space is compared.  Following the idea of contrastive loss, pairs with the same label should be embedded close to each other, while contrasting pairs should be mapped further apart in the embedding space.  While Siamese networks are traditionally applied as twins, they can be extended to take any number of inputs.  Another common implementation is a \textit{Siamese Triplet network} \citep{Hoffer2015DeepMetricLearning}.  As the name suggests, the base neural architecture is duplicated not once, but twice to form three networks with shared weights.  Triplet loss is used to update the network parameters.  As demonstrated by Figure \ref{fig:siamese_networks_triplet}, the network takes a triplet set of samples as input and an embedded representation is produced for each sample (through the same transformation because the weights are shared).  A metric function then compares the dissimilarities between the anchor and positive samples, as well as the anchor and negative.  A comparator is applied at the output to compute the triplet loss between the embedded representations.  The idea is that samples from the same class or with high semantic similarity should have a lower dissimilarity in the embedding space than samples  from opposing classes.
		
		
		\begin{figure*}[h!]
			\hfill
			\begin{subfigure}[t]{0.5\textwidth}
				\includegraphics[height=2.8in]{example-image-a}
				\caption{Siamese network structured from contrastive loss.}
				\label{fig:siamese_networks_conrastive}
			\end{subfigure}%
			\centering
			~ 
			\begin{subfigure}[t]{0.5\textwidth}
				\centering
				\includegraphics[height=2.8in]{example-image-a}
				\caption{Siamese network structured from triplet loss.}
				\label{fig:siamese_networks_triplet}
			\end{subfigure}
		
			\caption[Siamese Neural Networks]{ }
			\label{fig:siamese_networks}%
		\end{figure*}
	
	
		The embeddings produced by Siamese networks have  been shown to be powerful representations for discrimination tasks \citep{Chen2020ContrastiveLearning, Schroff2015FaceNet, Koch2015SiameseNetworks}.  In test, pairs or sets of images are passed through the contrastive network to simultaneously produce embeddings.  The pairing with the lowest dissimilarity would assign the class label to the test point.  For example, if classifying a test point into one of ten classes, ten pairs could be passed through the network (the test sample and a representative from each class), and the label of the test point would be assigned as the label of the representative from the best-ranking pair.   Alternatively, after all training samples are embedded through the triplet network, an alternative classification scheme such as $k$-NN or a SVM could be trained on the embedded representations.  Testing would simply consist of embedding a test sample through the network and using the test procedure of the external learner.
		
		
		Chen et al. noted a few interesting observations regarding Siamese networks \citep{Chen2020ContrastiveLearning}. First, data augmentation is often necessary when training Siamese networks to avoid overfitting.  Second, representations often benefit from normalization. Finally, contrastive learning benefits from larger batch sizes and longer training times than its supervised counterpart.
		
		Despite the nuances mentioned, Siamese networks have been applied  successfully to applications in object recognition \citep{Hoffer2015DeepMetricLearning}, one-shot learning \citep{Koch2015SiameseNetworks}, visual representation learning \citep{Chen2020ContrastiveLearning} and image retrieval \citep{Hoffer2015DeepMetricLearning}.
		
		
		\subsection{FaceNet}
		
		
		FaceNet is a convolutional neural network which learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity \citep{Schroff2015FaceNet}.  The method is based on learning a Euclidean embedding per image using a deep convolutional neural network.  Once each sample has been embedded through the network,  facial recognition can be achieved through the use of a simple $k$-NN classifier.  FaceNet  directly trains its outputs to be a compact 128-dimensional embedding  using a triplet-based loss function based on LMNN. FaceNet is based on a Inception network architecture and  optimizes a triplet loss formulated as:
		\begin{align}
		\mathcal{L} = ||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} -||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2} + \alpha, \quad \forall (f(x^{a}_{i}),f(x^{p}_{i}),f(x^{n}_{i})) \in \mathcal{T}
		\end{align} 
		where $\alpha$ is the margin enforced between positive and negative pairs and $\mathcal{T}$ is the set of all possible triplets in the training set.  The idea is that the triplet loss does not only promote similar faces to be close to each other in the embedding space, but also enforces a margin between every other face in the dataset.  To address the hard-mining problem,  triplets can be mined online out of mini-batches.  This provides a trade-off between speed and utility toward training.  FaceNet is currently SOA for the facial recognition problem, and in response, similar networks have been explored in a variety of applications, including: metric learning, image classification and image retrieval \citep{Hoffer2015DeepMetricLearning}.
		
		
		
		
		
	
		
%	\subsection{Multiple Instance Dimensionality Reduction with Metric Embedding}
%	In the context of multiple instance learning, Xu et al. proposed a \textit{Multi-Instance Metric Learning} (MIMEL) \citep{Xu2011MI_Metric_Learning}.  As mentioned previously, the goal of metric learning (synonymous with some approaches  to manifold learning) is not to enforce distances based off a metric, but to learn the best possible metric which captures class differences.  While metric learning is not addressed in this work, Xu et al. utilized concepts which will likely be applicable, so it is worth mentioning.  For example, when learning the appropriate dissimilarity metric, a constraint was placed on bags such that bags of different classes should be separated by a margin in the embedding space.  Additionally, a weighting scheme was provided to propose ``key" instance in the positive bags.  This was based on a maximum likelihood estimation of negative instances.  
	

	
	



