\chapter{Background}

This chapter provides a literature review on Manifold Learning, including classic approaches, supervised and semi-supervised methods and uses of manifolds for functional regularization.  A review of the Multiple Instance Learning framework for learning from weak and ambiguous annotations is provided. Additionally, this chapter reviews the existing literature on metric embedding, focusing heavily on the utilization of contrastive and triplet-based loss evaluation.  Reviews describe basic terminology and definitions.  Foundational approaches are elaborated and advances are addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

Multiple Instance Learning (MIL) was originally proposed in \cite{Dietterich1996AxisParallelRectangles} as a method to handle inherent observation difficulties associated with drug activity prediction.  This problem, among many others, fits well into the framework of MIL where training labels are associated with sets of data points, called \textit{bags} instead of each individual data points, or \textit{instances}.  Under the \textit{standard MIL assumption}, a bag is given a ``positive" label if it is known that  \textit{at least one} sample in the set represents pure or partial target.  Alternatively, a bag is labeled as ``negative" if does not contain any positive instances \cite{Carbonneau2016MILSurvey}.  Let $\bm{X}=[\bm{x}_1,\dots, \bm{x}_N] \in \mathbb{R}^{d \times N}$ be training data where $d$ is the dimensionality of an instance, $\bm{x}_n$, and $N$ is the total number of training instances.  The data is grouped into K \textit{bags}, $\bm{B} = \{\bm{B}_1, \dots, \bm{B}_K\}$, with associated binary bag-level labels, $\mathcal{L} = \{L_1, \dots, L_K \}$ where 
\begin{align}
	L_k = \begin{cases} 
	1, & \exists \bm{x}_{kn} \in \bm{B}^{+}_{k} \ni  l_{kn} = 1\\
	-1, & l_{kn} = 0 \quad \forall \bm{x}_{kn} \in \bm{B}^{-}_{k} 
	\end{cases}
\end{align} and $\bm{x}_{kn}$ denotes the $n^{th}$ instance in positive bag $\bm{B}^{+}_{k}$ or negative bag $\bm{B}^{-}_{k}$ \cite{Zare2016MIACE} and $l_{kn}$ denotes the instance-level label on instance $\bm{x}_{kn}$.  Figure \ref{fig:bag_eg} demonstrates the concept of MIL bags.  The objective of learning under MIL is, given only bag-level label information, to fit a model which can classify bags as either being positive or negative (bag-level classification) or to predict the class labels of individual instances (instance-level classification).

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption{Placeholder for examples of positive and negative bag concepts}
		\label{fig:bag_eg}
	\end{figure*}
\end{center}

\subsection{Multiple Instance Space Paradigm}
Multiple Instance Learning via Embedded Instance Selection

\subsection{Multiple Instance Concept Learning}

\subsection{Multiple Instance Classification}

\subsection{Multiple Instance Boosting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Learning}

Real-world remote sensing data such as hyperspectral imagery, ground-penetrating radar scans and sonar signals are naturally represented by high-dimensional feature vectors.  However, in order to handle such real-world data adequately, its dimensionality usually needs to be reduced \cite{VanDerMaaten2009DRReview,Belkin2004SemiSupLearningRiemannianManifolds}. The problem considered in this work is discovering feature representations that promote class discriminability for target or anomaly detection.  This is typically achieved in one of two ways.  First, features can be projected into a high-dimensional space (such as a Kernel Hilbert Space) using a kernel function. The second option, which is the focus of this work, is to transform the data into a new (often lower-dimensional) coordinate system which optimizes feature representations for discrimination. \textbf{CITE supervised manifold learning paper... gonna have to search for it again}

The application of \textit{dimensionality reduction} (DR) has proven useful in myriad applications in the literature, such as: visualization of high-dimensional data, classification, redundancy removal, compression and data management, improving computational tractability and efficiency, and reducing the effects of the Curse of Dimensionality \cite{Bishop1998GTM,Nickel2017PoincareEmbeddings,Talmon2015ManifoldLearningInDynamicalSystems,Tenenbaum2000Isomap, Geng2005SupNonlinearDimRed, Palomo2017GHNG, Kohonen1990SOM,Kegl2008PrincipalManifoldsTextbook,Bengio2014RepLearningReview}.  In classification of object entities, it is often assumed that classes can be described by an \textit{intrinsic} subset of representative features which demonstrate geometrical structure \cite{Belkin2006ManReg}. These structures are called intrinsic \textit{manifolds}, and they represent the generating distributions of class objects exactly by the number of degrees of freedom in a dataset \cite{Thorstensen2009ManifoldThesis, Belkin2004SemiSupLearningRiemannianManifolds}.     Consider the example shown in Figure \ref{fig:manifold_eg}.  This classic example demonstrated in \cite{Thorstensen2009ManifoldThesis} shows samples from a pose-estimation dataset \textbf{CITE}.  While each individual image is represented by a vector of features (pixel intensities in this case) in $\mathbb{R}^{4096}$, the dataset only exhibits three degrees of freedom: 1 light variation parameter and 2 rotation angles.  Thus, it is intuitive that the dataset lies on a smooth, intrinsic submanifold spanning three dimensions which inherently capture the degrees of freedom in the data.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{example-image-a}
		\caption{Placeholder for example of high D data lying on a low-dimensional sub-manifold.}
		\label{fig:manifold_eg}
	\end{figure*}
\end{center}

The goal of manifold Learning is then to discover embedding functions which take data from the input feature space and transform it into a lower-dimensional (ideally intrinsic) coordinate system (also called a \textit{latent space} in the literature) which captures the ``useful" properties of the data, while enforcing constraints such as smoothness (the transformation function should not produce sporadic images), continuity (no discontinuous points on the hyper-surface), topological ordering (neighbors in the input space should also be neighbors in the embedded space ), or class separability (samples from the same class should fall metrically close to each other in the embedded space and disparate classes should be distinctly far) \textbf{CITE}.

This dissertation focuses on investigating the use of manifold learning to increase instance discriminability in the latent space, where labels are solely provided at the bag-level.   While there is an expansive literature in unsupervised manifold learning methods, this document will pay special attention to both strictly- and semi-supervised methods, since they are typically adaptations of unsupervised approaches, as well as manifold learning under the MIL framework.

\subsection{Supervised and Semi-Supervised Approaches}
Most studies perform classification or regression after applying unsupervised dimensionality reduction.  However, it has been shown that there are advantages of learning the low-dimensional representation and classification/regression models simultaneously \cite{Chao2019RecentAdvancesSupervisedDimRed,Rish2008SupDimRedGLM}.

\subsection{Global Approaches}
PCA and KPCA, MDS, LDA/KDA


\subsection{Locality Preservation}
\subsubsection{Graph-based Methods}
Nonlinear dimensionality reduction methods typically rely on the use of adjacency graphs.  These graphs represent data structure pooled from local neighborhoods of samples.  An overview of computational graphs, as well as the two most prominent methods for graph construction in manifold learning are presented.
Overview and terminology

\subsubsection{$K$ - Nearest Neighbor Graph}

\subsubsection{$\epsilon$ - Neighborhood Graph}

\subsubsection{Geodesic Distance Approximation}

\subsubsection{Isomap}

\subsubsection{Locally Linear Embedding (LLE)}

\subsubsection{Laplacian Eigenmaps}

\subsubsection{Hessian Eigenmaps}

\subsubsection{Diffusion Maps}

\subsubsection{Sammon Mapping}

\subsubsection{Latent Variable Models}

\subsubsection{Competitive Hebbian Learning}

\subsubsection{Deep Learning}


\subsubsection{UMAP}

\subsubsection{t-SNE}

\subsection{Multiple Instance Learning on Manifolds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Metric Embedding %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metric Embedding}

The concepts of ``near" and ``far" are very powerful and useful utilities in everyday life.  They classify the relationship between two ``primitives" as being similar or dissimilar, as well as the degree of compatibility \cite{Thorstensen2009ManifoldThesis}. As an example, a medical doctor might consider a machine learning researcher and a software engineer as being similar (near), because they both perform research for computer applications.  However, the same researcher and engineer would likely consider their jobs as being very disparate (far) based on the details of their work.  In order to capture this abstraction of distance, a \textit{metric space} is defined as a mathematical construction  of this vague generality.

 \theoremstyle{definition}
 \begin{definition}{Metric Space}

 \end{definition}
 	\begin{enumerate}
	\item Non-negativity
	\item Symmetry
	\item  Triangle Inequality
	\end{enumerate}

The goal of metric embedding learning it to learn a function $f_{\theta}(\bm{x}):\mathbb{R}^{D} \rightarrow \mathbb{R}^{K}$ which maps semantically similar points from the data input feature space of $\mathbb{R}^{D}$ onto \textit{metrically close} points in $\mathbb{R}^{K}$.  Similarly, $f_{\theta}$ should map semantically different points in $\mathbb{R}^{D}$ onto metrically distant points in $\mathbb{R}^{K}$.  The function $f_{\theta}$ is parameterized by $\theta$ and can be anything ranging from a linear transformation to a complex non-linear mapping as in the case of deep artificial neural networks \cite{Hermans2017DefenseTripletLoss}.


	\subsection{Ranking Loss}
	
	\subsubsection{Pairwise Loss}
	
	\subsubsection{Contrastive Loss}
	Definition of Contrastive Loss:
	
	
	\subsubsection{Triplet Loss}
	
	Definition of Triplet Loss:
	
	Triplet loss was extended in \cite{Sohn2016NPairLoss} to simultaneously optimize against N negative classes.

		\subsubsection{Large-Margin K-Nearest Neighbors (LMNN)}
		
		\subsubsection{FaceNet}
		
		FaceNet is a convolutional neural network which learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity \cite{Schroff2015FaceNet}. 
		
		\begin{align}
			||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} + \alpha < ||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2}, \quad \forall (f(x^{a}_{i}),f(x^{p}_{i}),f(x^{n}_{i})) \in \mathcal{T}
		\end{align} 
	
		\begin{align}
			\mathcal{L} = ||f(x^{a}_{i}) - f(x^{p}_{i})||^{2}_{2} -||f(x^{a}_{i}) - f(x^{n}_{i})||^{2}_{2} + \alpha
		\end{align}
		
		
		\subsubsection{Siamese Neural Networks}
	
	\subsection{Manifold Regularization}
	
	\subsection{Multiple Instance Metric Learning}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Outlier Detection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Competency Aware Overview}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Sensor Fusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Sensor Fusion}
%
%	Sensor fusion is the process of combining multiple sensors to provide complimentary or reinforcing information about a scene under observation \cite{hackett1990multisensorfusion,zhang2010multisourceremotingsensingfusion}.  Sensor fusion can operate on three levels: data or sample level, feature level, and decision level.  This  means that raw data, extracted features, or classifier prediction outputs can all be leveraged through a fusion process \cite{Ruta2000OverviewClassifierFusionMethods,zhang2010multisourceremotingsensingfusion}. While the process of sensor fusion is, in fact, general, this dissertation focuses primarily on combining sensors of varying types and resolutions with the goal of providing support for classification and detection decisions. In this context, sample-level fusion aims to combine data from multiple sources into single-resolution data which are expected to be more informative than any of the inputs, individually.  Fusion on this level involves applying a transformation on the data directly to project, map or co-register data obtained from multiple sources.  Feature-level fusion combines attributes computed from each sensors' collection, such as shape, texture, frequency, local statistics, or other discriminative characteristics exhibited in the data.  Feature-level fusion transforms the data into a single representation space which can be used instead of the original parameterizations for further processing.  Decision-level fusion takes outputs from multiple classifier or real-valued prediction models and derives a singular, fused output.  Decision-level fusion aims to provide a more accurate and/or descriptive classification or regression result through selection, transformation, or consensus of the models being combined \cite{hackett1990multisensorfusion, zhang2010multisourceremotingsensingfusion,Du2017Thesis,Tulyakov2008ReviewClassifierCombinationMethods, Du2016MIChoquetIntegralFusion,Du2018MultiResolutionSensorFusion}.  Sensor fusion has been used in a wide expanse of applications, including: remote sensing and hyperspectral image registration, classification and visualization, \cite{Zhang2014SemiSupManLearningFusion,Liao2016ManAlignmentHSI,Yang2016ManifoldAlignmentMultitemporalHSI,Hong2018CommonSubspaceLearningHSI}, object detection \cite{Davenport2010JointManifoldsDataFusion}, explosive hazard detection \cite{Gader2004ChoquetIntegralLandmine,Smith2017ChoquetIntegralLandmine,Keller2001PredictiveSensorFusion,Gunatilaka2001FusionNoncoincidentlySampledLandmine,Frigui2010ContextDependentFusionLandmine},  handwriting recognition \cite{Gader1996FusionHandwrittenLetters}, pose estimation \cite{Navaratnam2007JointManifoldSemiSupRegression}, automatic target recognition in mid-wave infrared/ sonar imagery \cite{Shen2018ManifoldSensorFusionImageData}, land-use classification \cite{Hong2019LearnableManifoldAlignment}, super-resolution, medical and fault diagnosis, defense, meteorology, environmental monitoring, and economic forecasting   \cite{zhang2010multisourceremotingsensingfusion,Zitova2003SurveyImageRegistrationMethods}. 
%	
%	\textbf{Challenges in sensor fusion}
%	Features might not have the same dimensionality between sensors. The number/ length of features/ physical qualities vary  between modalities.
%	Data from different sensors must be put into equivalent forms to allow for fusion.  In order for data from multiple sources to be fused, there must be some method to relate data points from one sensor with corresponding data points from the other sensors.  Before sensor measurements can be combined, we must ensure that the measurements represent the same physical entity. Therefore, we need to check the consistency of sensor measurements.  Since each sensor is sensitive to a different modality, multiple sensors not only can provide multiple views of objects, but they can also impose more constraints to reduce the search	space during matching.
%
%
%	
%	Additionally, data fusion can provide confidence bounds on sensor's collections \cite{hackett1990multisensorfusion,Mohandes2018ClassifierCombinationTechniquesReview}.  For example, HSI sensors measure the radiance across a broad range of wavelengths at varying ground locations.  The measured radiance is a combination of radiation that is reflected and/or emitted by materials on the ground.  This inherently implies that environmental characteristics such as atmospheric conditions and diurnal cycle affect the amount of radiance that can be captured by the sensor \cite{Zare2008Thesis}.  Therefore, incorporation of meta-data such as time-of-day and weather conditions can provide bounds on the uncertainty contained in a given measurement and reinforce when to trust readings from particular spectral bands (i.e. IR during night).   These examples demonstrate how it could be useful to incorporate and fuse the information provided by multiple sensor modalities to reduce measurement uncertainty or to obtain a more complete understanding of a scene, thus aiding in classification or segmentation decisions.
%	\newline
%	
%	Information fusion approaches make two typical assumptions: (1) If fusing multiple heterogeneous sources (varying types and resolutions), it is assumed that individual data points can be co-registered \cite{Shen2016SpatioTemporalSpectralFusion}. In other words, standard methods require data from $m$ different sensors to produce data with one-to-one correspondence, or that some form of pre-processing can transform all sources to the same resolution and perform matching \cite{hackett1990multisensorfusion,Butenuth2007HeterogeneousGeospatialData}.  Assumption (2) states that precise training labels are available for each data point \cite{Du2017Thesis}. 
%	\newline 
%	
%	Two problems arise from these assumptions.  First, when working with sensors operating at varying spatial, spectral, or temporal resolutions, it is usually non-trivial to convert all data to the same resolution or to map to the same grid and existing co-registration approaches often result in loss of sensor-specific information \cite{Shen2016SpatioTemporalSpectralFusion,Brigot2016CoregistrationForestRemoteSensingImages}.  Referring to the previous scene understanding example, it is intuitive that the addition of external meta-data could supply context which has the ability to aid in decision making or provide confidence bounds on each sensor's measurement capabilities.  This addition, of course, adds a new level of difficulty to the fusion process since high-level information such as time-of-day, weather, environmental-setting (e.g. urban or forest), etc., would likely be collected at different spatial or temporal rates and would probably not align easily.  Typical approaches would address this problem by throwing out data points to match the lowest sampling rate.  However, this mapping is often noisy and results in loss of sensor-specific information.  This loss is often detrimental since classification improvement may only be achieved if the total information uncertainty in a problem is reduced \cite{Ruta2000OverviewClassifierFusionMethods}.  Moreover, it becomes difficult to distinguish between errors which evolve during the registration process and actual physical differences in a scene \cite{Zitova2003SurveyImageRegistrationMethods}.
%	\newline 
%	
%	\subsection{Classic Approaches}
%	
%%		\subsubsection{Choquet Integral}
%%		
%%		\subsubsection{Hierarchical Mixture of Experts}
%%		
%%		\subsubsection{Deep Learning}
%%		
%%		\subsubsection{Graph-Based}
%		
%	\subsection{Feature-level Fusion}
%	
%	\subsection{Manifold Alignment}
%		Manifold alignment is the process of matching two or more seemingly disparate datasets by mapping them to a joint latent space, while both preserving the qualities of each dataset and highlighting their similarities \cite{Wang2011ManifoldAlignment,Liao2016ManAlignmentHSI,Stanley2019ManAlignmentFeatureCorrespondence}.  
%	
%%	\subsection{Information Loss}
%%	
%%	\subsection{Geocoding}
%%	
%%	\subsection{Similarity Measures}
%%	
%%	\subsection{Transformation, Interpolation, Re-sampling}
%%	
%%	\subsection{Conflation}
%	

