\chapter{Technical Approach}
In this chapter, we present the proposed method for discriminative dimensionality reduction using weakly-supervised learning. In the current state of the literature, few methods have been developed which address the problem of dimensionality reduction and manifold learning using weak labels under the multiple-instance learning framework.  Among the proposed methods, none address the possibility of nonlinearity in the underlying manifold representing bags or instances. To this end, this proposal presents a plan of research to fill this gap in the literature.  Specifically, we aim to provide a generalized framework for nonlinear dimensionality reduction under the multiple instance learning paradigm which promotes instance-level discriminability in the latent embedding space.  This proposal only considers the binary classification problem, which is motivated by target detection in remote  sensing. The approaches developed in this work will be directly compared to state-of-the-art approaches in the literature.  The remainder of this chapter is divided into three sections.  In the first section, we describe the initial work in investigating the efficacy of strictly supervised, nonlinear manifold learning methods for bag-level classification.  In the second section, we present the proposed approach for instance-level classification.  Finally, the third section presents the proposed datasets for applying and testing the developed approaches. 

\section{Nonlinear Manifold Learning for Bag-Level Classification}
In order to show potential for the proposed work to advance SOA in the literature, initial experiments were conducted to demonstrate proof of concept for nonlinear dimensionality reduction for bag-level classification.  Specifically, two algorithms were evaluated across a range of test parameters.  While Supervised Laplacian Eigenmaps (S-LE) and Supervised Enhanced Isomap (SE-Isomap) were reviewed in sections \ref{sec:Laplacian_Eigenmaps} and \ref{sec:Isomap}, respectively, pseudo-code for the algorithms using Multiple Instance data is provided below.  

\subsection{S-LE Algorithm Description}
The objective of S-LE \citep{Raducanu2012SupervisedNonlinearDimReduction} is to find low-dimensional representations $\bm{Z}$ of data $\bm{X}$, which promotes class separability in the embedding space.  Essentially, within-class and between class affinity matrices are defined by the nearest neighbors all of samples in the dataset.  An objective is defined using the graph Laplacians  which trades off between pulling samples of the same class close and pushing away samples of different classes.   A detailed discussion of S-LE is given in Section \ref{sec:Laplacian_Eigenmaps} and pseudo-code is provided in Algorithm \ref{alg:SLE}.

\begin{algorithm}[h!]
	\caption{S-LE}
	\label{alg:SLE}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $k, \beta,\gamma, d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {$\bm{X} \gets$ vector representations of bags in $\bm{B}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}_{w,mn} \gets \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 1$}
		\Else
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\EndIf
		\EndFor
		\State {Compute affinity matrix $\bm{W} \gets \bm{W}_{w} + \bm{W}_{b} $} 
		\State {Compute graph Laplacians $\bm{L}_{w} \gets \bm{D}_{w} - \bm{W}_{w}$, $\bm{L}_{b} \gets \bm{D}_{b} - \bm{W}_{b}$} 
		\State {Define $\bm{B} \gets \gamma \bm{L}_{b} + (1-\gamma)\bm{W}_{w}$}  
		\State {Solve the eigenvector problem $\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v}$}
		\State {Sort eigenvectors $\bm{v}$ and eigenvalues $\lambda$ in decreasing order}
		\State{$\bm{Z} \in \mathbb{R}^{N \times d} \gets $ top $d$ eigenvectors }        
	\end{algorithmic}
\end{algorithm}

\subsection{SE-Isomap Algorithm Description}
SE-Isomap \citep{Ribeiro2008SupervisedIsomap}, as discussed in Section \ref{sec:Isomap}, uses a specialized measure of similarity which considers class information in order to project samples of the same class closer and points in different classes away from each other in the embedding space. SE-Isomap first computes an affinity matrix between all pairs of samples in the dataset.  Samples from the same class have a higher affinity than samples in different classes, as controlled by the parameter $\alpha$.  After computing the affinity matrix, the shortest path distance matrix is constructed using Floyd's or Dijkstra's algorithms, and multidimensional  scaling is applied to find the low-dimensional data representations $\bm{Z}$ of input data $\bm{X}$. Psuedo-code for both SE-Isomap and MDS are provided in Algorithms \ref{alg:SE_Isomap} and \ref{alg:MDS}, respectively.

\begin{algorithm}[h!]
	\caption{SE-Isomap}
	\label{alg:SE_Isomap}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $k, \beta,\alpha, d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {$\bm{X} \gets$ vector representations of bags in $\bm{B}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{1 - \exp{\frac{-||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}}$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{ \exp{\frac{||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}} - \alpha$}
		\Else
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets 0$}
		\EndIf
		\EndFor
		\State {$\bm{D} \gets $ squares of the shortest distances between all points using Dijkstra's or Floyd's algorithm on $\bm{W}$}
		\State {$\bm{Z} \in \mathbb{R}^{N \times d} \gets $ $\textsc{MDS}(\bm{D})$}         
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{MDS}
	\label{alg:MDS}
	\begin{algorithmic}[1]
		\Require {Distance matrix $\bm{D} \in \mathbb{R}^{N \times N}$, embedding space dimensionality $d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {Calculate  $\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}$, where $\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}$ is the centering matrix} 
		\State {Compute eigenvectors and eigenvalues $\bm{K}v = \lambda v$} 
		\State {$\bm{V}',\bm{\Lambda}' \gets $ $\textsc{SortDecreasing}(\bm{V},\bm{\Lambda})$}
		\State {$\bm{Z} \in \mathbb{R}^{N \times d} \gets \bm{V}'\bm{\Lambda}^{'\frac{1}{2}}$}     
	\end{algorithmic}
\end{algorithm}

\section{Nonlinear Manifold Learning for Instance-Level Classification}
While preliminary experiments used S-LE and SE-Isomap to embed data for bag-level classification, this work proposes the development of  non-linear dimensionality reduction techniques that promote instance-level classification.

\subsection{Instance Selection}

\subsection{Traditional Manifold Learning Based}

\subsection{Ranking-Loss Based}

The work by Wei et al. in \cite{Wei2016ImageBagGenerators} suggested that  for image classification tasks, certain formulations of MIL were better suited than others.  Algorithms such as miGraph, MIBoosting and miFV which assume non-i.i.d samples or take advantage of aggregating properties of bags tend to work better than those which adopt the standard assumption.  The authors of this work recommend miGraph with LBP bag generation or MIBoosting with Single Blob generation for image classification.  Additionally, classification performance tended to increase as the number of instances increased.

Choquet integral
Can we use to preference learning to discern instances which are likely to be positive and negative, then use these rankings for embedding?

Metric embedding is often realized through weakly-supervised learning, where instead of labels, the data is accompanied with sets of preferences. While this embedding could, conceptually be easy to implement at the bag level, enforcing instance embeddings is difficult due to the uncertainty on the labels.  However, one could (theoretically) use instance ranking as discussed in Section \ref{sec:MI_Ranking} to provide estimated preference sets on the instances for use in embedding.

\section{Datasets}
In this section, a table is given to provide a summary of the datasets proposed to test the developed approaches.  The table provides a brief summary of the datasets as well as their intended uses in testing.

\begin{longtable}{ |p{4cm}|p{6cm}|p{4cm}|  } 
	\caption{Summary of datasets.}
	\label{tab:Datasets}\\
	\hline
	\multicolumn{3}{|c|}{\textbf{Datasets}} \\
	\hline
	\textbf{Dataset} & \textbf{Summary} & \textbf{Justification/Use}\\
	\hline
	DSIAC MS-0003-DB   &  Publicly available MWIR videos of military vehicles. & Test classification at both the bag and instance level.   \\
	\hline
	MUUFL Gulfport &  Hyperspectral, LiDAR and GPS data of scenes taken over University of Mississippi Gulfport. & Test classification at the bag-level.   \\
	\hline
	LFW Faces in the Wild & Images of 5749 different faces labeled at the  image level. & Classification and ranking. \\
	\hline
	PASCAL VOC  & 2913 RGB images labeled at the pixel-level. & Test instance-level classification. \\
	\hline
\end{longtable}