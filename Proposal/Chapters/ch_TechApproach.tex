\chapter{Technical Approach}
In this chapter, we present the proposed method for discriminative dimensionality reduction using weakly-supervised learning. In the current state of the literature, few methods have been developed which address the problem of dimensionality reduction and manifold learning using weak labels under the multiple-instance learning framework.  Among the proposed methods, none address the possibility of nonlinearity in the underlying manifold representing bags or instances. To this end, this proposal presents a plan of research to fill this gap in the literature.  Specifically, we aim to provide a generalized framework for nonlinear dimensionality reduction under the multiple instance learning paradigm which promotes instance-level discriminability in the latent embedding space.  This proposal only considers the binary classification problem, which is motivated by target detection in remote  sensing. The approaches developed in this work will be directly compared to state-of-the-art approaches in the literature.  The remainder of this chapter is divided into three sections.  In the first section, we describe the initial work in investigating the efficacy of strictly supervised, nonlinear manifold learning methods for bag-level classification.  In the second section, we present the proposed approaches for instance-level classification.  Finally, the third section presents the proposed datasets for applying and testing the developed approaches. 

\section{Nonlinear Manifold Learning for Bag-Level Classification}
In order to show potential for the proposed work to advance SOA in the literature, initial experiments were conducted to demonstrate proof of concept for nonlinear dimensionality reduction on bag-level classification.  Specifically, two algorithms were evaluated across a range of test parameters.  While Supervised Laplacian Eigenmaps (S-LE) and Supervised Enhanced Isomap (SE-Isomap) were reviewed in sections \ref{sec:Laplacian_Eigenmaps} and \ref{sec:Isomap}, respectively, pseudo-code for the algorithms using Multiple Instance data is provided below.  

\subsection{S-LE Algorithm Description}
The objective of S-LE \citep{Raducanu2012SupervisedNonlinearDimReduction} is to find low-dimensional representations $\bm{Z}$ of data $\bm{X}$, which promote class separability in the embedding space.  Essentially, within-class and between class affinity matrices are defined by the nearest neighbors all of samples in the dataset.  An objective is constructed using graph Laplacians  which trades off between pulling samples of the same class close and pushing away samples of different classes.   A detailed discussion of S-LE is given in Section \ref{sec:Laplacian_Eigenmaps} and pseudo-code is provided in Algorithm \ref{alg:SLE}.

\begin{algorithm}[h!]
	\caption{S-LE}
	\label{alg:SLE}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $k, \beta,\gamma, d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {$\bm{X} \gets$ vector representations of bags in $\bm{B}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}_{w,mn} \gets \exp \left (- \frac{||\bm{x}_{n} - \bm{x}_{m} ||^{2}}{\beta} \right)$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 1$}
		\Else
		\State{$\bm{W}_{w,mn} \gets 0$}
		\State{$\bm{W}_{b,mn} \gets 0$}
		\EndIf
		\EndFor
		\State {Compute affinity matrix $\bm{W} \gets \bm{W}_{w} + \bm{W}_{b} $} 
		\State {Compute graph Laplacians $\bm{L}_{w} \gets \bm{D}_{w} - \bm{W}_{w}$, $\bm{L}_{b} \gets \bm{D}_{b} - \bm{W}_{b}$} 
		\State {Define $\bm{B} \gets \gamma \bm{L}_{b} + (1-\gamma)\bm{W}_{w}$}  
		\State {Solve the eigenvector problem $\bm{B}\bm{v} = \lambda \bm{D}_{w}\bm{v}$}
		\State {Sort eigenvectors $\bm{v}$ and eigenvalues $\lambda$ in decreasing order}
		\State{$\bm{Z} \in \mathbb{R}^{N \times d} \gets $ top $d$ eigenvectors }        
	\end{algorithmic}
\end{algorithm}

\subsection{SE-Isomap Algorithm Description}
SE-Isomap \citep{Ribeiro2008SupervisedIsomap}, as discussed in Section \ref{sec:Isomap}, uses a specialized measure of similarity which considers class information in order to project samples of the same class closer and points in different classes away from each other in the embedding space. SE-Isomap first computes a distance matrix between all pairs of samples in the dataset.  Samples from the same class have smaller pairwise distances than samples in different classes, as controlled by the parameter $\alpha$.  After computing the distance matrix $\bm{W}$, the shortest path distance matrix is constructed using Floyd's or Dijkstra's algorithms, and multidimensional  scaling is applied to find the low-dimensional data representations $\bm{Z}$ of input data $\bm{X}$. Psuedo-code for both SE-Isomap and MDS are provided in Algorithms \ref{alg:SE_Isomap} and \ref{alg:MDS}, respectively.

\begin{algorithm}[h!]
	\caption{SE-Isomap}
	\label{alg:SE_Isomap}
	\begin{algorithmic}[1]
		\Require {Multiple-instance dataset $\bm{B} = \{\bm{B}_{1}, \dots, \bm{B}_{K}\}$, $\{L_{1}, \dots, L_{K} \}$, $L_{k} \in \{-1, +1\}$, parameters $k, \beta,\alpha, d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {$\bm{X} \gets$ vector representations of bags in $\bm{B}$}
		\For {$m,n \in N$}
		\If {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} = L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{1 - \exp{\frac{-||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}}$}
		\ElsIf {$\bm{x}_{n}$ in $k$-NN of $\bm{x}_{m}$ and $L_{m} \neq L_{n}$}
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets \sqrt{ \exp{\frac{||(\bm{x}_{m},\bm{x}_{n}||^{2}}{\beta}}} - \alpha$}
		\Else
		\State{$\bm{W}(\bm{x}_{m},\bm{x}_{n}) \gets 0$}
		\EndIf
		\EndFor
		\State {$\bm{D} \gets $ squares of the shortest distances between all points using Dijkstra's or Floyd's algorithm on $\bm{W}$}
		\State {$\bm{Z} \in \mathbb{R}^{N \times d} \gets $ $\textsc{MDS}(\bm{D})$}         
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{MDS}
	\label{alg:MDS}
	\begin{algorithmic}[1]
		\Require {Distance matrix $\bm{D} \in \mathbb{R}^{N \times N}$, embedding space dimensionality $d$}
		\Ensure {Low-dimensional data representations $\bm{Z}$}
		\State {Calculate  $\bm{K}= -\frac{1}{2}\bm{H}\bm{D}\bm{H}$, where $\bm{H}= \mathbb{I}_{N} - \frac{1}{N}\bm{e}\bm{e}^{T}$ is the centering matrix} 
		\State {Compute eigenvectors and eigenvalues $\bm{K}v = \lambda v$} 
		\State {$\bm{V}',\bm{\Lambda}' \gets $ $\textsc{SortDecreasing}(\bm{V},\bm{\Lambda})$}
		\State {$\bm{Z} \in \mathbb{R}^{N \times d} \gets \bm{V}'\bm{\Lambda}^{'\frac{1}{2}}$}     
	\end{algorithmic}
\end{algorithm}

\section{Nonlinear Manifold Learning for Instance-Level Classification}
While preliminary experiments used S-LE and SE-Isomap to embed data for bag-level classification, this work proposes the development of  non-linear dimensionality reduction techniques that promote instance-level classification in the embedding space.  To develop these approaches, four specific tasks will be explored: instance selection, manifold learning, metric embedding and out-of-sample testing.

\subsection{Instance Selection}
Many feature learning approaches in the literature use supervised learning to obtain discriminative feature representations, or use supervised methods to fine-tune unsupervised  feature extraction.  However, this cannot be done directly in MIL because of the uncertainty on the labels \citep{Carbonneau2016MILSurvey}.  Therefore, in order to adapt supervised nonlinear manifold learning to the MIL framework, techniques must be explored to propose the most-likely positive instances.  While methods for this have been proposed in the literature \citep{Kim2010LocalDRMIL, Maron1998DiverseDensity, Bocinsky2019SPIEMIACEInitialization}, I propose to develop an iterative technique to learn the most-likely positive points through training.  A variety of techniques will be explored for this task, including: Multiple instance boosting, MI ranking/preference learning, maximum-likelihood and DD-based, and clustering.  

\subsection{Manifold Learning}
I will continue to explore the use of S-LE and SE-Isomap for use in instance-level classification.  However, in order to use these methods for optimal instance embedding, notion of which instances are positive and negative is needed.  To explore instance  embedding, I will first implement DD as a method to select positive and negative prototypes which can be used to optimize the embeddings.  Following, I will investigate incorporating a MI boosting or ranking procedure to iteratively adapt the samples which are selected as positive and negative.  An iterative approach to training will be used which alternates between instance selection and providing the optimal embeddings for instance-level classification.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{example-image-a}
		\caption[Proposed manifold learning]{Image showing the mapping of instances from bags to a manifold, where the instances are well-separated  on the manifold.}
		\label{fig:proposed_manifold_learning}
	\end{figure*}
\end{center}

\subsection{Metric Embedding}
Metric embedding is often realized through weakly-supervised learning, where instead of labels, the data is accompanied with sets of preferences \citep{Hermans2017DefenseTripletLoss,Koch2015SiameseNetworks,Schroff2015FaceNet}. While this embedding is straightforward to implement at the bag level, enforcing instance embeddings is difficult due to the uncertainty on the labels.  However, one could (potentially) use instance ranking as discussed in Section \ref{sec:MI_Ranking} to provide estimated preference sets on the instances for use in embedding.  In the proposed work, I will investigate MI ranking/preference learning as a way to discriminate between positive and negative instances.  Specifically, Siamese networks or ranking SVMs will be trained with contrastive and triplet loss in order to learn low-dimensional features that promote class separation in the embedding space.  Bag-level information will be utilized to ensure similarity between instances in negative bags, while enforcing dissimilarity between the instances in positive bags and negative bags.  Given a test bag, the algorithm should be able to project negative instances close to other negative instances and far from positive instances in the embedding space (and vice versa) or correctly rank instances as being more likely to be positive or negative.

\begin{center}
	\begin{figure*}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{example-image-a}
		\caption[Proposed metric embedding]{Image to show how instances are ranked and are easily separable in a learned metric space.}
		\label{fig:proposed_metric_embedding}
	\end{figure*}
\end{center}

\subsection{Out-of-Sample Testing}
Preliminary work used simple linear reconstructions of  samples' input space neighbors to embed out-of-sample points for testing.  However, this approach is likely sub-optimal as it does not utilize training class label information for a non-linear embedding.  For this reason, out-of-sample embedding approaches will be explored to project test points into the embedded space.  Specifically, I will explore the use of neural networks to learn nonlinear mapping functions between the input and embedding spaces.  I will also explore  approaches in the literature that consider class information of the training data, such as the technique in \citep{Vural2016OutOfSampleSupManifoldLearning}. 

\section{Datasets}
In this section, a table is given to provide a summary of the datasets proposed to test the developed approaches.  The table provides a brief summary of the datasets as well as their intended uses in testing.

\begin{longtable}{ |p{4cm}|p{6cm}|p{4cm}|  } 
	\caption{Summary of datasets.}
	\label{tab:Datasets}\\
	\hline
	\multicolumn{3}{|c|}{\textbf{Datasets}} \\
	\hline
	\textbf{Dataset} & \textbf{Summary} & \textbf{Justification/Use}\\
	\hline
	DSIAC MS-0003-DB   &  Publicly available MWIR videos of military vehicles. & Test classification at both the bag and instance level.   \\
	\hline
	MUUFL Gulfport &  Hyperspectral, LiDAR and GPS data of scenes taken over University of Mississippi Gulfport. & Test detection and classification at the bag-level.   \\
	\hline
	LFW Faces in the Wild & Images of 5749 different faces labeled at the  image level. & Classification and ranking. \\
	\hline
	PASCAL VOC  & 2913 RGB images labeled at the pixel-level. & Test instance-level classification. \\
	\hline
\end{longtable}