\documentclass[]{article}

\usepackage[margin=1in]{geometry}
\usepackage[round]{natbib}
\usepackage{indentfirst}
\usepackage[hidelinks,pdfnewwindow=true]{hyperref}
\usepackage[dvipsnames]{xcolor}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{array}
\usepackage{caption}
\usepackage{url}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{shortvrb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{commath}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



%\graphicspath{{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/DSRF/"}{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/"}{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/png_figures_squished/"}}


%the following allows 5 deep section headings (can be useful for dividing things up)
%section
%  subsection
%    subsubsection
%      paragraph
%        subparagraph
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

%defines list object
%inputs are
%[1] bibtex reference label
%[2] Paper title
%[3] pdf file name (folder is hard coded as ../References)
%[4] abstract or any text you want to add
\newcommand{\paperentry}[4]{
            \hangindent=1cm
            \cite{#1} - \href{run:../References/#3}{\textcolor{ForestGreen}{\textit{#2}}}
            
            \noindent            
            \begin{minipage}[t]{0.1\linewidth}\hfill\end{minipage}
            \begin{minipage}[t]{0.8\linewidth}\textcolor{NavyBlue}{{\textit{Summary:}}}#4\end{minipage}
            \vspace{.25cm}
          }

%opening
\title{List of References}

\author{Connor H. McCurley}

\date{}

\begin{document}

\maketitle

\tableofcontents

\newpage

      
%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold/ Representation Learning}

	\subsection{Classic Methods}

	\paperentry{VanDerMaaten2009DRReview}
	{Dimensionality Reduction: A Comparative Review}
	{Manifold_Representation_Learning/Reviews/VanDerMaaten2009DRReview.pdf}
	{}
	
	\paperentry{Jindal2017ReviewDRTechniques}
	{A Review on Dimensionality Reduction Techniques}
	{Manifold_Representation_Learning/Reviews/Jindal2017ReviewDRTechniques.pdf}
	{}

	\paperentry{Bengio2014RepLearningReview}
	{Unsupervised Feature Learning and Deep Learning: {A} Review and New Perspectives}
	{Manifold_Representation_Learning/Reviews/Bengio2014RepLearningReview.pdf}
	{}
	
	\paperentry{Tenenbaum2000Isomap}
	{A Global Geometric Framework for Nonlinear Dimensionality Reduction}
	{Manifold_Representation_Learning/Manifold/Tenenbaum2000Isomap.pdf}
	{}
	
	\paperentry{Roweis2000LLE}
	{Nonlinear Dimensionality Reduction by Locally Linear Embedding}
	{Manifold_Representation_Learning/Manifold/Roweis2000LLE.pdf}
	{}
	
	\paperentry{Saul2001LLEIntro}
	{An introduction to locally linear embedding}
	{Manifold_Representation_Learning/Manifold/Saul2001LLEIntro.pdf}
	{}
	
	\paperentry{Belkin2003LaplacianEigenmaps}
	{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}
	{Manifold_Representation_Learning/Manifold/Belkin2003LaplacianEigenmaps.pdf}
	{}
	
	
	\paperentry{Bishop1998GTM}
	{GTM: The Generative Topographic Mapping}
	{Manifold_Representation_Learning/Manifold/Bishop1998GTM.pdf}
	{}
	
	\paperentry{Delaporte2008DiffusionMaps}
	{An introduction to diffusion maps}
	{Manifold_Representation_Learning/Manifold/Delaporte2008DiffusionMaps}
	{}
	
	\paperentry{Theodoris2008PCA}
	{The Karhunen-Loeve Transform}
	{}
	{}
	
	\paperentry{Theodoris2008KPCA}
	{Kernel PCA}
	{}
	{}
	
	\paperentry{Tipping1999PPCA}
	{Probabilistic Principal Component Analysis}
	{Manifold_Representation_Learning/Manifold/Tipping1999PPCA.pdf}
	{}
	
	\paperentry{Lawrence2003GPLVM}
	{Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data}
	{Manifold_Representation_Learning/Manifold/Lawrence2003GPLVM.pdf}
	{}
	
	\paperentry{Lawrence2005PPCAGPLVModels}
	{Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models}
	{Manifold_Representation_Learning/Manifold/Lawrence2005PPCAGPLVModels.pdf}
	{}
	
	\paperentry{Gorban2007ElasticMaps}
	{Elastic Maps and Nets for Approximating Principal Manifolds and Their Application to Microarray Data Visualization}
	{Manifold_Representation_Learning/Manifold/Gorban2007ElasticMaps.pdf}
	{}
	
	\paperentry{Lee2015MultipleManifolds}
	{Learning Representations from Multiple Manifolds}
	{Manifold_Representation_Learning/Manifold/Lee2015MultipleManifolds.pdf}
	{}
	
	\paperentry{Kokiopoulou2007OrthoNeighborhoodPreservingProjections}
	{Orthogonal Neighborhood Preserving Projections: A Projection-Based Dimensionality Reduction Technique}
	{Manifold_Representation_Learning/Manifold/Kokiopoulou2007OrthoNeighborhoodPreservingProjections.pdf}
	{}
	
	\paperentry{Talmon2015ManifoldLearningInDynamicalSystems}
	{Manifold Learning for Latent Variable Inference in Dynamical Systems}
	{Manifold_Representation_Learning/Manifold/Talmon2015ManifoldLearningInDynamicalSystems.pdf}
	{}
	
	\paperentry{Nickel2017PoincareEmbeddings}
	{Poincar\'{e} Embeddings for Learning Hierarchical Representations}
	{Manifold_Representation_Learning/Manifold/Nickel2017PoincareEmbeddings.pdf}
	{}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CHL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Competitive Hebbian Learning}
	
	\paperentry
	{Rumelhart1985CHL}
	{Feature Discovery by Competitive Learning}
	{Manifold_Representation_Learning/CHL/Rumelhart1985CHL.pdf}
	{}
	
	\paperentry{Kohonen1990SOM}
	{The self-organizing map}
	{Manifold_Representation_Learning/CHL/Kohonen1990SOM.pdf}
	{}
	\newline
	The self-organizing map (SOM) creates spatially organized intrinsic representations of features.  It belongs to the category of neural networks which use ``competitive learning", or ``self-organization".  It is a sheet-like artificial neural network in which the cells become tuned to various input patterns through an unsupervised learning process.  Only a neighborhood of cells give an active response to the current input sample.  The spatial location or coordinates of cells in the network correspond to different modes of the input distribution. The self-organizing map is also a form of vector quantization (VQ).  The purpose of VQ is to approximate a continuous probability density function $p(\bm{x})$ of input vectors $\bm{x}$ using a finite number of codebook vectors, $\bm{m}_i$, $i=1,2,\dots,k$.  After the ``codebook" is chosen, the approximation of $\bm{x}$ involves finding the reference vector, $\bm{m}_c$ closest to $\bm{x}$.  The ``winning" codebook vector for sample $\bm{x}$ satisfies the following:
	\begin{align*}
		|| \bm{x} - \bm{m}_c|| &= \min_{i}|| \bm{x} - \bm{m}_{i} ||
	\end{align*}	
	\noindent
	
	The algorithm operates by first initializing a spatial lattice of codebook elements (also called ``units"), where each unit's representative is in $\bm{m}_i \in \mathbb{R}^{D}$ where $D$ is the dimensionality of the input samples $\bm{x}$.  The training process proceeds as follows.  A random sample is selected and presented to the network and each unit determines its activation by computing dissimilarity.	 The unit who's codebook vector provides the smallest dissimilarity is referred to as the \textit{winner}.
	
	\begin{align*}
		c(t) = \argmin_{i} d(\bm{x}(t),\bm{m}_{i}(t))
	\end{align*}
	\noindent
	
	Both the winning vector and all vectors within a neighborhood of the winner are updated toward the sample by 
	
	\begin{align*}
		\bm{m}_{i}(t+1) = \bm{m}_{i}(t) + \alpha(t) \cdot h_{ci}(t) \cdot [ \bm{x}(t) - \bm{m}_{i}(t) ] 
	\end{align*}
	\noindent
	where $\alpha(t)$ is a learning rate which decreases over time and $h_{ci}(t)$ is a neighborhood function which is typically unimodal and symmetric around the location of the winner which monotonically decreases with increasing distance from the winner.  A radial basis kernel is typically chosen for the neighborhood function as 
	
	\begin{align*}
		 h_{ci}(t) = \exp{\left( -\frac{||\bm{r}_{c} - \bm{r}_i ||^{2}}{2 \sigma^{2}(t)} \right)}
	\end{align*}
	\noindent
	where the top expression represents the Euclidean distance between units $c$ and $i$ with $\bm{r}_{i}$ representing the 2-D location of unit $i$ in the lattice.  The neighborhood kernel's bandwidth is typically initialized to a value which covers a majority of the input space and decreases over time such that solely the winner is adapted toward the end of the training procedure. \\
	
	\noindent
	The SOM essentially performs density estimation of high-dimensional data and represents it in a 2 or 3-D representation.  At test time, the dissimilarity between each unit in the map and an input sample are computed.  This dissimilarity can be used to effectively detect outliers, thus making the SOM a robust method which can provide confidence values for it's representation abilities. \\
	
	In this paper, the SOM was applied to speech recognition, but made note of previous uses in robotics, control of diffusion processes, optimization problems, adaptive telecommunications, image compression, sentence understanding, and radar classification of sea-ice. \\
	
 	
 
	 \paperentry{Rauber2002GHSOM}
	 {The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data}
	 {Manifold_Representation_Learning/CHL/Rauber2002GHSOM.pdf}
	 {The Growing Hierarchical Self-organizing Map (GHSOM) is an extension of the classical SOM.  It is an artificial neural network with a hierarchical architecture, composed of individually growing SOMs.  Layer 0 is composed of a single neuron representing the mean of the training data.  A global stopping criteria is developed as a fraction of the mean quantization error.  This means that all units must represent their respective subsets of data an a MQE smaller than a fraction of the 0 layer mean quantization error.  For all units not satisfying this criteria, more representation is required for that  area of the feature  space and additional units are added.  After a particular number of training iterations, the quantization errors are computed and the unit with the highest error is selected as the \textit{error unit}. The most dissimilar neighbor of the error unit is chosen is and a row/ column of nodes is injected between them.  The growth process continues until a second stopping criteria is met.  Any units still not satisfying the global criteria are deemed to need extra representation.  Child map are initialized below these units and trained with the subset of data mapped to its parent node.\\
	 	
 	\noindent
 	In conclusion, the GHSOM is a growing self-organizing map architecture which has the ability to grow itself until the feature space  is adequately represented.  For areas of the space needing a more specific level of granularity, a hierarchical structure is imposed to ``fill-in" areas of high density. \\
 	
 	\noindent
 	The GHSOM has been applied to the areas of finance, computer network traffic analysis, manufacturing and image analysis (Palomo 2017).
	}
 
	 \paperentry{Chiang1997HandWrittenWords}
	 {Hybrid fuzzy-neural systems in handwritten word recognition}
	 {Manifold_Representation_Learning/CHL/Chiang1997HandWrittenWords.pdf}
	 {}
	
	\paperentry{Frigui2009LandmineSOM}
	{Detection and Discrimination of Land Mines in Ground-Penetrating Radar Based on Edge Histogram Descriptors and a Possibilistic $K$-Nearest Neighbor Classifier}
	{Manifold_Representation_Learning/CHL/Frigui2009LandmineSOM.pdf}
	{}
	
	\paperentry{Fritzke1995GrowingNeuralGas}
	{A Growing Neural Gas Network Learns Topologies}
	{Manifold_Representation_Learning/CHL/Fritzke1995GrowingNeuralGas.pdf}
	{Abstract: An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the "neural gas" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation. \\
		
	\noindent
	In contrast to SOMs and ``growing cell structures", which can project data onto non-linear subspaces which are chosen \textit{a priori}, the GNG is able to adapt its topology to match that of the input data distribution.  The growing process continues until a pre-defined level of quantization error has been reached. \\
	
	\noindent
	The base algorithm is outlined in Palomo (2017), \textit{Growing Hierarchical Neural Gas Self-Organizing Network}.		
	}
	
	\paperentry{Palomo2017GHNG}
	{The Growing Hierarchical Neural Gas Self-Organizing Neural Network}
	{Manifold_Representation_Learning/CHL/Palomo2017GHNG.pdf}
	{}
	\newline
	Abstract: The growing neural gas (GNG) self-organizing neural network stands as one of the most successful examples of unsupervised learning of a graph of processing units. Despite its success, little attention has been devoted to its extension to a hierarchical model, unlike other models such as the self-organizing	map, which has many hierarchical versions. Here, a hierarchical GNG is presented, which is designed to learn a tree of graphs.  Moreover, the original GNG algorithm is improved by a distinction between a growth phase where more units are added until no significant improvement in the quantization error is obtained, and a convergence phase where no unit creation is	allowed. This means that a principled mechanism is established	to control the growth of the structure. Experiments are reported, which demonstrate the self-organization and hierarchy learning abilities of our approach and its performance for vector quantization	applications.  Experiments were performed in structure learning, color quantization, and video sequence clustering. \\
		
	\noindent
	The aim of this method was to improve the adaptation ability of the Growing Hierarchical Self-Organizing Map proposed by Rauber (2002).  This was to be done through the extension of the Growing Neural Gas, which disposes of the fixed lattice topology enforced by the SOM.  Addtionally, the GNG learns a dynamic graph with variable numbers of neurons and connections.  The graph represents the input data in a more plastic and flexible way than the fixed-topology map.  	\\
	
	\noindent
	All clustering methods that learn a hierarchical structure have advantages even when used for non-hierarchical data. The learned hierarchical structure can be pruned at several levels, which yields alternative representations of the input data set at different levels of detail. This can be used to visualize a data set in coarser	or more detailed way. For vector quantization applications, the different pruning levels correspond to smaller or larger codebooks, so that a balance can be attained between the size of the codebook and the quantization error within the same hierarchical structure.\\
	
	\noindent
	The growing hierarchical neural gas (GHNG) model is defined as a tree of self-organizing graphs.  Each graph is made of a variable number of neurons or processing units, so that its size can grow or shrink during learning. In addition, each graph is the child of a unit in the upper level, except for the top	level (root) graph.  The training procedure is described by the following: \\
	
	\noindent
	Each graph begins with $H \geq 2$ units and one or more undirected connections between them.  Both the units and connections can be created and destroyed during the learning process. It is also not necessary that the graph is connected.  Let the training set be denoted as $\mathcal{S}$ with $\mathcal{S} \subset \mathbb{R}^{D}$, where $D$ is the dimensionality of the input space.  Each unit $i\in \{1, \dots, H \}$ has an associated prototype $\bm{w}_{i} \in \mathbb{R}^{D}$ and an error variable $e_i \in \mathbb{R}$, $e_i \geq 0$.  Each connection has an associated age, which is a nonnegative integer.  The set of connections will be notetd as $A \subseteq \{1, \dots,H \} \times \{1, \dots, H \}$.  The learning mechanism for the GHNG is based on the original GNG, but includes a novel procedure to control the growth of the graph.  First, a growth phase is performed where the graph is allowed to enlarge until a condition is met, which indicates that further growing would provide no significant improvement in the quantization error.  After that, a convergence phase is executed where no unit creation is allowed in order to carry out a fine tuning of the graph.  the leraning algorithm is provided in the following steps. \\
	
	\begin{enumerate}
		\item Start with two units ($H=2$) joined by a connection.  Each prototype is initialized to a sample drawn at random from $\mathcal{S}$.  The error variables are initialized to zero.  The age of the connection is initialized to zero.
		\item Draw a training sample $\bm{x}_{t} \in \mathbb{R}^{D}$ at random from $\mathcal{S}$.
		\item Find the nearest unit $q$ and second nearest unit $s$ in terms of Euclidean distance
		\begin{align*}	
		q &= \argmin_{i\in \{1,\dots,H \}} ||\bm{w}_{i}(t) - \bm{x}(t)  || \\
		s &= \argmin_{i\in \{1,\dots,H \} - \{q\} } ||\bm{w}_{i}(t) - \bm{x}(t)  ||
		\end{align*}
		\item Increment the age of all edges departing from $q$
		\item  Update the winning unit's error variable, $e_{q}$
		\begin{align*}
		e_{q}(t+1) = e_{q}(t) + || \bm{w}_q(t) - \bm{x}_{t} ||
		\end{align*}
	\end{enumerate}

	\noindent
	I believe the author's experimental approach did not take advantage of the method's strengths.  The author's only demonstrated experiments in vector quantization, and used corresponding metrics.  This method could be used to represent manifold  topology of differing dimensionality. This could be useful in HSI imagery, for example where different environment patches require manifold representations of various dimensionality.  Additionally, this could potentially be used to handle the sensor fusion problem with sensor loss/ drop-out. \\
	
	\paperentry{Sun2016GNGMotionDetection}
	{Online growing neural gas for anomaly detection in changing surveillance scenes}
	{Manifold_Representation_Learning/CHL/Sun2016GNGMotionDetection.pdf}
	{}
	
	\paperentry{LopezRubio2011GHPGraphs}
	{Growing Hierarchical Probabilistic Self-Organizing Graphs}
	{Manifold_Representation_Learning/CHL/LopezRubio2011GHPGraphs.pdf}
	{}
	
	
	\paperentry{Palomo2016GrowingNeuralForest}
	{Learning Topologies with the Growing Neural Forest}
	{Manifold_Representation_Learning/CHL/Palomo2016GrowingNeuralForest.pdf}
	{}
	
	\subsection{Deep Learning}
	
		\paperentry{Goodfellow2016DeepLearning}
		{Deep Learning}
		{Manifold_Representation_Learning/Autoencoders/Goodfellow2016DeepLearning.pdf}
		{}
		
		\paperentry{Haykin2009NeuralNetworks}
		{Neural networks and learning machines}
		{Manifold_Representation_Learning/Autoencoders/Haykin2009NeuralNetworks.pdf}
		{}
		
		\paperentry{Dai2017VariationalAutoencoder}
		{dden Talents of the Variational Autoencoder}
		{Manifold_Representation_Learning/Autoencoders/Dai2017VariationalAutoencoder.pdf}
		{}
		
		\paperentry{Rojas1996AssociativeNetworks}
		{Associative Networks}
		{Manifold_Representation_Learning/Autoencoders/Rojas1996AssociativeNetworks.pdf}
		{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Dissimilarities %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Information Measures}

	\paperentry{Arandjelovic2005FaceRecManifoldDensityDivergence}
	{Face recognition with image sets using manifold density divergence}
	{Manifold_Representation_Learning/Information/Arandjelovic2005FaceRecManifoldDensityDivergence.pdf}
	{}
	
	\paperentry{Wang2008ManifoldManifoldDistance}
	{Manifold–Manifold Distance and its Application to Face Recognition With Image Sets}
	{Manifold_Representation_Learning/Information/Wang2008ManifoldManifoldDistance.pdf}
	{}
	

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Regularization %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Regularization}
	\paperentry{Tsang2007ManifoldRegularization}
	{Large-Scale Sparsified Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Tsang2007ManifoldRegularization.pdf}
	{}
	
	\paperentry{Ren2017ManRegSAR}
	{Unsupervised Classification of Polarimetirc SAR Image Via Improved Manifold Regularized Low-Rank Representation With Multiple Features}
	{Manifold_Representation_Learning/ManifoldRegularization/Ren2017ManRegSAR.pdf}
	{}
	
	\paperentry{Belkin2006ManReg}
	{Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples}
	{Manifold_Representation_Learning/ManifoldRegularization/Belkin2006ManReg.pdf}
	{}
	
	\paperentry{Ratle2010ManRegHSI}
	{Semisupervised Neural Networks for Efficient Hyperspectral Image Classification}
	{Manifold_Representation_Learning/ManifoldRegularization/Ratle2010ManRegHSI.pdf}
	{}
	
	\paperentry{Li2015ManRegReinforcementLearning}
	{Approximate Policy Iteration with Unsupervised Feature Learning based on Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Li2015ManRegReinforcementLearning.pdf}
	{}
	
	\paperentry{Meng2018ManRegZeroShot}
	{Zero-Shot Learning via Low-Rank-Representation Based Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Meng2018ManRegZeroShot.pdf}
	{}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

	\subsection{Multiple Instance Concept Learning}
	
		\paperentry{Bocinsky2019Thesis}
		{Learning Multiple Target Concepts from Uncertain, Ambiguous Data Using the Adaptive Cosine Estimator and Spectral Match Filter}
		{Multiple_Instance_Learning/Bocinsky2019Thesis.pdf}
		{}
		
		\paperentry{Jiao2017Thesis}
		{Target Concept Learning From Ambiguously Labeled Data}
		{Multiple_Instance_Learning/Jiao2017MIHE_Thesis.pdf}
		{}
		
		\paperentry{Mccurley2019SPIEWEMIComparison}
		{Comparison of hand-held WEMI target detection algorithms}
		{Multiple_Instance_Learning/Mccurley2019SPIEWEMIComparison.pdf}
		{}
		
		\paperentry{Bocinsky2019SPIEMIACEInitialization}
		{Investigation of initialization strategies for the Multiple Instance Adaptive Cosine Estimator}
		{Multiple_Instance_Learning/Bocinsky2019SPIEMIACEInitialization.pdf}
		{}
		
		\paperentry{Zare2015MILLandmineEMI}
		{Multiple instance dictionary learning for subsurface object detection using handheld EMI}
		{Multiple_Instance_Learning/Zare2015MILLandmineEMI.pdf}
		{}
		
		\paperentry{Cook2015Thesis}
		{Task driven extended functions of multiple instances (TD-eFUMI)}
		{Multiple_Instance_Learning/Cook2015Thesis.pdf}
		{}
		
		\paperentry{Cook2016LandmineTaskDriveneFUMI}
		{Buried object detection using handheld WEMI with task-driven extended functions of multiple instances}
		{Multiple_Instance_Learning/Cook2016LandmineTaskDriveneFUMI.pdf}
		{}
		
		\paperentry{Zare2016MIACE}
		{Multiple Instance Hyperspectral Target Characterization}
		{Multiple_Instance_Learning/Zare2016MIACE.pdf}
		{}
		
		\paperentry{Jiao2017MIHE}
		{Multiple instance hybrid estimator for learning target signatures}
		{Multiple_Instance_Learning/Jiao2017MIHE.pdf}
		{}
		
		\paperentry{Xiao2017SphereMIL}
		{A Sphere-Description-Based Approach for Multiple-Instance Learning}
		{Multiple_Instance_Learning/Xiao2017SphereMIL.pdf}
		{}
		
		\paperentry{Cheplygina2019MILSurvey}
		{Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis}
		{Multiple_Instance_Learning/Cheplygina2019MILSurvey.pdf}
		{}
		
		\paperentry{Li2017SmoothMIL}
		{Cross-validated smooth multi-instance learning}
		{Multiple_Instance_Learning/Li2017SmoothMIL.pdf}
		{}
		
		\paperentry{Cheplygina2016DissimilarityEnsemblesMIL}
		{Dissimilarity-Based Ensembles for Multiple Instance Learning}
		{Multiple_Instance_Learning/Cheplygina2016DissimilarityEnsemblesMIL.pdf}
		{}
		
		\paperentry{Wang2017DiversityMILActiveLearning}
		{Incorporating Diversity and Informativeness in Multiple-Instance Active Learning}
		{Multiple_Instance_Learning/Wang2017DiversityMILActiveLearning.pdf}
		{}
		
		\paperentry{Hajimirsadeghi2017MIClassificationMarkovNetworks}
		{Multi-Instance Classification by Max-Margin Training of Cardinality-Based Markov Networks}
		{Multiple_Instance_Learning/Hajimirsadeghi2017MIClassificationMarkovNetworks.pdf}
		{}
		
		\paperentry{Du2016MIChoquetIntegralFusion}
		{Multiple Instance Choquet integral for classifier fusion}
		{Multiple_Instance_Learning/Du2016MIChoquetIntegralFusion.pdf}
		{}
		
		\paperentry{Ilse2018AttentionBasedDeepMIL}
		{Attention-based Deep Multiple Instance Learning}
		{Multiple_Instance_Learning/Ilse2018AttentionBasedDeepMIL.pdf}
		{}
		
		
		\paperentry{Karem2016MILMultiplePositiveAndNegativeConcepts}
		{Multiple Instance Learning with multiple positive and negative target concepts}
		{Multiple_Instance_Learning/Karem2016MILMultiplePositiveAndNegativeConcepts.pdf}
		{}
		
		\paperentry{Xiao2017MIOrdinalRegression}
		{Multiple-Instance Ordinal Regression}
		{Multiple_Instance_Learning/Xiao2017MIOrdinalRegression.pdf}
		{}
		
		\paperentry{Gao2017CountGuidedWeaklySupervisedLocalization}
		{{C-WSL:} Count-guided Weakly Supervised Localization}
		{Multiple_Instance_Learning/Gao2017CountGuidedWeaklySupervisedLocalization.pdf}
		{}
		
		\paperentry{Li2017MultiviewMIL}
		{Multi-View Multi-Instance Learning Based on Joint Sparse Representation and Multi-View Dictionary Learning}
		{Multiple_Instance_Learning/Li2017MultiviewMIL.pdf}
		{}
		
		\paperentry{Cao2016VehicleDetectionMIL}
		{Weakly Supervised Vehicle Detection in Satellite Images via Multi-Instance Discriminative Learning}
		{Multiple_Instance_Learning/Cao2016VehicleDetectionMIL.pdf}
		{}
		
		\paperentry{Dietterich1996AxisParallelRectangles}
		{Solving the multiple instance problem with axis-parallel rectangles}
		{Multiple_Instance_Learning/Dietterich1996AxisParallelRectangles.pdf}
		{}
		
		\paperentry{Maron1998DiverseDensity}
		{A Framework for Multiple-instance Learning}
		{Multiple_Instance_Learning/Maron1998DiverseDensity.pdf}
		{}
		
		\paperentry{Maron1998MILSceneClassification}
		{Multiple-Instance Learning for Natural Scene Classification}
		{Multiple_Instance_Learning/Maron1998MILSceneClassification.pdf}
		{}
		
		\paperentry{Carbonneau2016MILSurvey}
		{Multiple Instance Learning: {A} Survey of Problem Characteristics and Applications}
		{Multiple_Instance_Learning/Carbonneau2016MILSurvey.pdf}
		{}
		
		\paperentry{Zhang2002EMDD}
		{EM-DD: An Improved Multiple-Instance Learning Technique}
		{Multiple_Instance_Learning/Zhang2002EMDD.pdf}
		{}
		
		\paperentry{Zare2015eFUMI}
		{Extended Functions of Multiple Instances for target characterization}
		{Multiple_Instance_Learning/Zare2015eFUMI.pdf}
		{}
		
		\paperentry{Jiao2018MIHE2}
		{Multiple instance hybrid estimator for hyperspectral target characterization and sub-pixel target detection}
		{Multiple_Instance_Learning/Jiao2018MIHE2.pdf}
		{}
		
		
		
		
	
	\subsection{Multiple Instance Classification}
	
		\paperentry{Cao2016VehicleDetectionMIL}
		{Weakly Supervised Vehicle Detection in Satellite Images via Multi-Instance Discriminative Learning}
		{Multiple_Instance_Learning/Cao2016VehicleDetectionMIL.pdf}
		{}
		
	
	\subsection{Multiple Instance Regression}
	
		\paperentry{Trabelsi2018FuzzyClusteringMILRegression}
		{Fuzzy and Possibilistic Clustering for Multiple Instance Linear Regression}
		{Multiple_Instance_Learning/Trabelsi2018FuzzyClusteringMILRegression.pdf}
		{}
		
		\paperentry{Ruiz2018MIDynamicOrdinalRegression}
		{Multi-Instance Dynamic Ordinal Random Fields for Weakly Supervised Facial Behavior Analysis}
		{Multiple_Instance_Learning/Ruiz2018MIDynamicOrdinalRegression.pdf}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}


	\subsection{Applications}
	
		\paperentry{}
		{}
		{}
		{}
		


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fusion}
	
	\subsection{Classical Approaches}
		\subsubsection{General Approach}
			\paperentry{Mohandes2018ClassifierCombinationTechniquesReview}
			{Classifiers Combination Techniques: A Comprehensive Review}
			{Fusion/Reviews/Mohandes2018ClassifierCombinationTechniquesReview.pdf}
			{}
			
			\paperentry{Ruta2000OverviewClassifierFusionMethods}
			{An Overview of Classifier Fusion Methods}
			{Fusion/Reviews/Ruta2000OverviewClassifierFusionMethods.pdf}
			{}
			\newline
			``The objective of all decision support systems (DSS) is to create a model, which given a minimum amount of input data/information, is able to produce correct decisions."  ``the solution might be just to combine existing,  well performing methods, hoping that better results will be achieved.  Such fusion of information seems to be worth applying in terms of uncertainty reduction. Each of individual methods produces some errors, not mentioning that the input information	might be corrupted and incomplete. However, different methods performing on different data should	produce different errors, and assuming that all	individual methods perform well, combination of such multiple experts should reduce overall classification	error and as a consequence emphasize correct outputs."  ``Fusion of data/information can	be carried out on three levels of abstraction closely connected with the flow of the classification process: data level fusion, feature level fusion, and classifier fusion"  This paper focused on the later method of classifier fusion.  This process can essentially be categorized into two eruditions.  The first methods put emphasis on the classifier structure and do not do anything with the outputs until the combination process finds the best classifier or a selected group of classifiers.  Then their outputs are taken as a final decision or used for further processing.  The second category operates primarily on classifier outputs and can be further divided.  \newline  There are three possibles types of output labels generated by individual classifiers.  Crisp labels provide the lowest amount of information for fusion, as no information about potential alternatives is available.  Some additional information can be gleaned from labels in the form of class rankings.  However, fusion methods operating on classifiers with soft/fuzzy outputs can be expected to produce the greatest improvement in classification performance.  (Connor Note: This is valuable in terms of outlier rejection as well!).  The following explains an overview of classifier fusion methods operating on single class labels, class rankings, and fuzzy measures, respectively. \\ 
			\noindent
			\textbf{Methods operating on classifiers:}  \newline \textit{Dynamic Classifier Selection} (DCS) methods replect the tendency to extract  a single best classifier instead of mixing many different classifiers, by attempting to determing the single classifier which is most likely to produce the correct classification label for an input sample.  Only the output of the selected classifier is taken as a final decision.  The classifier selection process includes a partitioning of the input samples.  A classifier is is for each partition is selected locally.  All DCS methods rely on strong training data and by choosing only locally best classifier.  \textbf{They potentially lose some useful information from other well-performing classifiers.} Classifiers and their combination functions are typically organized in parallel and simultaneously and separately get their outputs as in input for a combination function.  A more reasonable approach, however, is \textbf{to organize all classifiers into groups and to apply different fusion methods for each group.}  A very important factor for the success of this method is the diversity of classifier types, training data, and methods involved.  \textbf{Any classification improvement may only be achieved if the total information uncertainty is reduced.}   This in turn depends on the diversity of information supporting different classification methods. \textbf{The same goal can be achieved by reduction of errors produced by individual classifiers.}  \textit{Hierarchical Mixture of Experts} (HME) is an example  of a fusion method whose strength comes from classifier's structure.  It is a supervised learning method based on the \textit{divide-and-conquer} principle.  It is organized as a tree-like structure of leaves.  Each leaf represents an individual expert in the network, each of which tries to solve a local supervised learning problem.  The outputs of the elements of the same node are partitioned and combined by the gating network and the total output of the node is given as a convex combination.  The expert networks are trained to increase the posterior probability according to Bayes rule.  A number  of learning algorithms can be applied to tune the mixture model.  \textit{Expectation-Maximization} (EM) is often used to learn the model parameters.  \textit{The HME technique does not seem to be applicable to large-dimensional datasets.} \newline \textbf{Fusing Single Class Labels:} Classifiers producing crisp, single-class labels (SCL) provide the least amount of useful information for the combination process.  The two most common techniques for fusing SCL classifiers are \textit{Generalized Voting} and \textit{Knowledge-Behavior Space} methods.  \\
			\noindent
			\textbf{Voting Methods:} \newline Voting strategies can be applied to a multiple classifier system assuming that each classifier gives a single class label as as output and no training data are available.  While there are many methods for combining these labels, they all lead to the following generalized voting definition.  Let the output of the classifiers form the decision vector $\bm{d} = [ \bm{d}_1, \bm{d}_2, \dots, \bm{d}_n ]^{T}$ where $\bm{d}_i \in \{ c_1, c_2, \dots, c_m, r \} $, $c_i$ denotes the class label of the i-th class and $r$ the rejection of assigning the input sample to any classes.  The binary characteristic function is defined as follows:
			
			\begin{align*}
				B_{j}(c_i) = \begin{cases}
				1 \text{ if } \bm{d}_j = c_i \\
				0 \text{ if } \bm{d}_j \neq c_i 
				\end{cases}
			\end{align*}
			
			
			
			
			\paperentry{Tulyakov2008ReviewClassifierCombinationMethods}
			{Review of Classifier Combination Methods}
			{Fusion/Reviews/Tulyakov2008ReviewClassifierCombinationMethods.pdf}
			{}
			
			\paperentry{hackett1990multisensorfusion}
			{Multi-sensor fusion: a perspective}
			{Fusion/Reviews/hackett1990multisensorfusion.pdf}
			{}
			
			\paperentry{zhang2010multisourceremotingsensingfusion}
			{Multi-source remote sensing data fusion: Status and trends}
			{Fusion/Reviews/zhang2010multisourceremotingsensingfusion.pdf}
			{}
		
	
		\subsubsection{Hierarchical Mixture of Experts}
		
			\paperentry{Jordan1993HME}
			{Hierarchical mixtures of experts and the EM algorithm}
			{Fusion/HME/Jordan1993HME.pdf}
			{}
		
			\paperentry{Yuksel2012TwentyYearsMixtureofExperts}
			{Twenty Years of Mixture of Experts}
			{Fusion/HME/Yuksel2012TwentyYearsMixtureofExperts.pdf}
			{}
			
			\paperentry{Beyer2009HeterogeneousMixtureOfExperts}
			{Heterogeneous mixture-of-experts for fusion of locally valid knowledge-based submodels}
			{Fusion/HME/Beyer2009HeterogeneousMixtureOfExperts.pdf}
			{}
			
			\paperentry{Shazeer2017SparselyGatedMixtureOfExperts}
			{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}
			{Fusion/HME/Shazeer2017SparselyGatedMixtureOfExperts.pdf}
			{}
		
		
		\subsubsection{Choquet Integral}
		
			\paperentry{Du2017Thesis}
			{Multiple Instance Choquet Integral For MultiResolution Sensor Fusion}
			{Fusion/Du2017Thesis}
			{}
		
			\paperentry{Smith2017ChoquetIntegralLandmine}
			{Aggregation of Choquet integrals in GPR and EMI for handheld platform-based explosive hazard detection}
			{Fusion/Choquet/Smith2017ChoquetIntegralLandmine.pdf}
			{}
			
			\paperentry{Smith2017GeneticProgrammingChoquetIntegral}
			{Genetic programming based Choquet integral for multi-source fusion}
			{Fusion/Choquet/Smith2017GeneticProgrammingChoquetIntegral.pdf}
			{}
		
			\paperentry{Du2019MIChoquetIntegral}
			{Multiple Instance Choquet Integral Classifier Fusion and Regression for Remote Sensing Applications}
			{Fusion/Choquet/Du2019MIChoquetIntegral.pdf}
			{}
		
			\paperentry{Anderson2017BinaryFuzzyMeasureChoquetIntegral}
			{Binary fuzzy measures and Choquet integration for multi-source fusion}
			{Fusion/Choquet/Anderson2017BinaryFuzzyMeasureChoquetIntegral.pdf}
			{}
			
			\paperentry{Du2018MultiResolutionSensorFusion}
			{Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With Label Uncertainty}
			{Fusion/Choquet/Du2018MultiResolutionSensorFusion.pdf}
			{}
			
			\paperentry{Gader2004ChoquetIntegralLandmine}
			{Multi-sensor and algorithm fusion with the Choquet integral: applications to landmine detection}
			{Fusion/Choquet/Gader2004ChoquetIntegralLandmine.pdf}
			{}
			
			
			
		
		\subsubsection{Deep Learning}
		
			\paperentry{Jian2019AEInfraredVisibleFusion}
			{A Symmetric Encoder-Decoder with Residual Block for Infrared and Visible Image Fusion}
			{Fusion/DeepLearning/Jian2019AEInfraredVisibleFusion.pdf}
			{}
		
		\subsubsection{Graph-Based}
		
			\paperentry{Vivar2019MultiModalGraphFusion}
			{Multi-modal Graph Fusion for Inductive Disease Classification in Incomplete Datasets}
			{Fusion/GraphBased/Vivar2019MultiModalGraphFusion.pdf}
			{}

	\subsection{Co-registration}
	
		\paperentry{Dawn2010SurveyRemoteSensingImageRegistration}
		{Remote Sensing Image Registration Techniques: A Survey}
		{Fusion/Dawn2010SurveyRemoteSensingImageRegistration.pdf}
		{}
	
		\paperentry{Brigot2016CoregistrationForestRemoteSensingImages}
		{Adaptation and Evaluation of an Optical Flow Method Applied to Coregistration of Forest Remote Sensing Images}
		{Fusion/Brigot2016CoregistrationForestRemoteSensingImages.pdf}
		{}
		
		\paperentry{Zitova2003SurveyImageRegistrationMethods}
		{Image registration methods: a survey}
		{Fusion/Reviews/Zitova2003SurveyImageRegistrationMethods.pdf}
		{}
	
		\subsubsection{Geocoding}
	
		\subsubsection{Similarity Measures}
	
		\subsubsection{Transformation, Interpolation, Re-sampling}
	
		\subsubsection{Conflation}
		
	\subsection{Multi-resolution Fusion}
	
	\subsection{Fusion of Mixed Data Types}
		
		\paperentry{Butenuth2007HeterogeneousGeospatialData}
		{Integration of heterogeneous geospatial data in a federated database}
		{Fusion/Butenuth2007HeterogeneousGeospatialData.pdf}
		{}
		
			
		\paperentry{Guo2019LVAforMultimodalLearningandSensorFusion}
		{Latent Variable Algorithms for Multimodal Learning and Sensor Fusion}
		{Fusion/Guo2019LVAforMultimodalLearningandSensorFusion.pdf}
		{}
		
		\paperentry{Zhang2019FusionHeteroEarthObsClimateZones}
		{Fusion of Heterogeneous Earth Observation Data for the Classification of Local Climate Zones}
		{Fusion/Zhang2019FusionHeteroEarthObsClimateZones.pdf}
		{}
	
	\subsection{Unsorted}
	
	\paperentry{Shen2016SpatioTemporalSpectralFusion}
	{An Integrated Framework for the Spatio–Temporal–Spectral Fusion of Remote Sensing Images}
	{Fusion/Shen2016SpatioTemporalSpectralFusion.pdf}
	{}
	
	
	\paperentry{}
	{}
	{}
	{}
	
	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Outlier Detection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outlier/ Adversarial Detection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Army %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Army}

\paperentry{Hall2019ProbabilisticObjectDetection}
{Probabilistic Object Detection: Definition and Evaluation}
{Army/Hall2019ProbabilisticObjectDetection.pdf}
{A probabilistic object detection metric (PDQ - Probability-based Detection Quality) was proposed, thus defining the new task of defining probabilistic object detection metrics.  The ability of deep CNNs to quantify both \textit{epistemic} and \textit{aleatoric uncertainty} is paramount for deployment safety-critical applications.  PDQ aims to measure the accuracy of an image object detector in terms of its label uncertainty and spatial quality.  This is achieved through two steps.  First, a detector must reliably quantify its \textit{semantic uncertainty} by providing full probability distributions over known classes for each detection.  Next, the detectors must quantify spatial uncertainty by reporting \textit{probabilistic bounding boxes}, where the box corners are modeled as normally distributed.  A loss function was constructed to consider both label and spatial quality when providing a final detection measure.  The primary benefit of this method is that it provides a measure for the level of uncertainty in a detection. \\ \\ Is it possible to replace the probabilistic metric with a possibilistic one?  Could this be more effective at handling outlying cases?} 


\paperentry{Mahalanobis2019DSIACCharacterization}
{A comparison of target detection algorithms using DSIAC ATR algorithm development data set}
{Army/Mahalanobis2019DSIACCharacterization.pdf}
{The authors provided an initial characterization of detection performance on the DSIAC dataset using the \textit{Faster R-CNN} algorithm and \textit{Quadratic Correlation Filter (QCF)}.  Performance was evaluated on two datasets, ``easy'' and ``difficult'', where the difficulty was determined by number of pixels on target and local contrast.  Under difficult conditions, the Faster R-CNN algorithm achieved noteworthy performance, detecting as much as 80\% of the targets at a low false alarm rate of 0.01 FA/Square degree.  The dataset was limited by a lack of background diversity. }

\paperentry{Tanner2019DSIACNeuralNet}
{Fundamentals of Target Classification Using Deep Learning}
{Army/Tanner2019DSIACNeuralNet.pdf}
{A shallow CNN was utilized for ATR on the DSIAC MWIR dataset.  The goal of the study was to determine the range of optimal thresholds which would optimally separate the target and clutter class distributions defined by the CNN predictions (output of softmax), as well as determine an upper bound on the number of training images required for optimizing performance.  The shallow CNN (5 layers) and a Difference of Gaussians (DoG), which finds regions of high intensity on dark backgrounds were used to detect and classify targets.  The CNN could correctly classify 96\% of targets as targets and as few as 4\% of clutter as targets.  It was found that the DoG detector failed when the targets were small (long range) or if the overall image was bright (infrared taken during the daytime).  It was also determined that guessing the bright pixels were at the center of the targets was a bad assumption. (The brightest part of a target is not necessarily at its center.)}

\paperentry{Li2018CollaborativeSparsePriorsMultiViewATR}
{Collaborative sparse priors for multi-view ATR}
{Army/Li2018CollaborativeSparsePriorsMultiViewATR.pdf}
{}

\paperentry{Kokiopoulou2009GraphBasedClassificationMultipleObsSets}
{Graph-based classification of multiple observation sets}
{Army/Kokiopoulou2009GraphBasedClassificationMultipleObsSets.pdf}
{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Segmentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Segmentation}
	\paperentry{Caselles1997GeodesicActiveContours}
	{Geodesic Active Contours}
	{Segmentation/Caselles1997GeodesicActiveContours.pdf}
	{}
	
	\paperentry{Alvarez2010MorphologicalSnakes}
	{Morphological Snakes}
	{Segmentation/Alvarez2010MorphologicalSnakes.pdf}
	{The authors introduce a morphological approach to curve evolution.  Snakes or curves iteratively solve partial differential equations (PDEs).  By doing so, the shape of the snake deforms to minimize the internal and external energies along its boundary.  The internal component keeps the curve smooth, while the external component attaches the curve to image structures such as edges, lines, etc.  Curve evolution is one of the most widely used image segmentation/ object tracking algorithms.  The main contribution of the paper is a new morphological approach to the solution of the PDE associated with snake model evolution.  They approach the solution using only inf-sup operators which has the main benefit of providing simpler level sets (0 outside the contours and 1 inside).}
	
	\paperentry{Marquez_Neila2014MorphologicalCurveBasedEvolution}
	{A Morphological Approach to Curvature-Based Evolution of Curves and Surfaces}
	{Segmentation/Marquez_Neila2014MorphologicalCurveBasedEvolution.pdf}
	{}

\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
