\documentclass[]{article}

\usepackage[margin=1in]{geometry}
\usepackage[round]{natbib}
\usepackage{indentfirst}
\usepackage[hidelinks,pdfnewwindow=true]{hyperref}
\usepackage[dvipsnames]{xcolor}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{array}
\usepackage{caption}
\usepackage{url}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{shortvrb}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{commath}
\usepackage{bm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



%\graphicspath{{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/DSRF/"}{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/"}{"C:/Users/Conma/Documents/2019_SPIE/Paper/Images/png_figures_squished/"}}


%the following allows 5 deep section headings (can be useful for dividing things up)
%section
%  subsection
%    subsubsection
%      paragraph
%        subparagraph
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

%defines list object
%inputs are
%[1] bibtex reference label
%[2] Paper title
%[3] pdf file name (folder is hard coded as ../References)
%[4] abstract or any text you want to add
\newcommand{\paperentry}[4]{
            \hangindent=1cm
            \cite{#1} - \href{run:../References/#3}{\textcolor{ForestGreen}{\textit{#2}}}
            
            \noindent            
            \begin{minipage}[t]{0.1\linewidth}\hfill\end{minipage}
            \begin{minipage}[t]{0.8\linewidth}\textcolor{NavyBlue}{{\textit{Summary:}}}#4\end{minipage}
            \vspace{.25cm}
          }

%opening
\title{List of References}

\author{Connor H. McCurley}

\date{}

\begin{document}

\maketitle

\tableofcontents

\newpage

      
%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold/ Representation Learning}

	\subsection{Classic Methods}

	\paperentry{VanDerMaaten2009DRReview}
	{Dimensionality Reduction: A Comparative Review}
	{Manifold_Representation_Learning/Reviews/VanDerMaaten2009DRReview.pdf}
	{}
	
	\paperentry{Jindal2017ReviewDRTechniques}
	{A Review on Dimensionality Reduction Techniques}
	{Manifold_Representation_Learning/Reviews/Jindal2017ReviewDRTechniques.pdf}
	{}

	\paperentry{Bengio2014RepLearningReview}
	{Unsupervised Feature Learning and Deep Learning: {A} Review and New Perspectives}
	{Manifold_Representation_Learning/Reviews/Bengio2014RepLearningReview.pdf}
	{}
	
	\paperentry{Tenenbaum2000Isomap}
	{A Global Geometric Framework for Nonlinear Dimensionality Reduction}
	{Manifold_Representation_Learning/Manifold/Tenenbaum2000Isomap.pdf}
	{}
	
	\paperentry{Roweis2000LLE}
	{Nonlinear Dimensionality Reduction by Locally Linear Embedding}
	{Manifold_Representation_Learning/Manifold/Roweis2000LLE.pdf}
	{}
	
	\paperentry{Saul2001LLEIntro}
	{An introduction to locally linear embedding}
	{Manifold_Representation_Learning/Manifold/Saul2001LLEIntro.pdf}
	{}
	
	\paperentry{Belkin2003LaplacianEigenmaps}
	{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}
	{Manifold_Representation_Learning/Manifold/Belkin2003LaplacianEigenmaps.pdf}
	{}
	
	
	\paperentry{Bishop1998GTM}
	{GTM: The Generative Topographic Mapping}
	{Manifold_Representation_Learning/Manifold/Bishop1998GTM.pdf}
	{}
	
	\paperentry{Delaporte2008DiffusionMaps}
	{An introduction to diffusion maps}
	{Manifold_Representation_Learning/Manifold/Delaporte2008DiffusionMaps}
	{}
	
	\paperentry{Theodoris2008PCA}
	{The Karhunen-Loeve Transform}
	{}
	{}
	
	\paperentry{Theodoris2008KPCA}
	{Kernel PCA}
	{}
	{}
	
	\paperentry{Tipping1999PPCA}
	{Probabilistic Principal Component Analysis}
	{Manifold_Representation_Learning/Manifold/Tipping1999PPCA.pdf}
	{}
	
	\paperentry{Lawrence2003GPLVM}
	{Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data}
	{Manifold_Representation_Learning/Manifold/Lawrence2003GPLVM.pdf}
	{}
	
	\paperentry{Lawrence2005PPCAGPLVModels}
	{Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models}
	{Manifold_Representation_Learning/Manifold/Lawrence2005PPCAGPLVModels.pdf}
	{}
	
	\paperentry{Gorban2007ElasticMaps}
	{Elastic Maps and Nets for Approximating Principal Manifolds and Their Application to Microarray Data Visualization}
	{Manifold_Representation_Learning/Manifold/Gorban2007ElasticMaps.pdf}
	{}
	
	\paperentry{Lee2015MultipleManifolds}
	{Learning Representations from Multiple Manifolds}
	{Manifold_Representation_Learning/Manifold/Lee2015MultipleManifolds.pdf}
	{}
	
	\paperentry{Kokiopoulou2007OrthoNeighborhoodPreservingProjections}
	{Orthogonal Neighborhood Preserving Projections: A Projection-Based Dimensionality Reduction Technique}
	{Manifold_Representation_Learning/Manifold/Kokiopoulou2007OrthoNeighborhoodPreservingProjections.pdf}
	{}
	
	\paperentry{Talmon2015ManifoldLearningInDynamicalSystems}
	{Manifold Learning for Latent Variable Inference in Dynamical Systems}
	{Manifold_Representation_Learning/Manifold/Talmon2015ManifoldLearningInDynamicalSystems.pdf}
	{}
	
	\paperentry{Nickel2017PoincareEmbeddings}
	{Poincar\'{e} Embeddings for Learning Hierarchical Representations}
	{Manifold_Representation_Learning/Manifold/Nickel2017PoincareEmbeddings.pdf}
	{}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CHL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Competitive Hebbian Learning}
	
	\paperentry
	{Rumelhart1985CHL}
	{Feature Discovery by Competitive Learning}
	{Manifold_Representation_Learning/CHL/Rumelhart1985CHL.pdf}
	{}
	
	\paperentry{Kohonen1990SOM}
	{The self-organizing map}
	{Manifold_Representation_Learning/CHL/Kohonen1990SOM.pdf}
	{}
	\newline
	The self-organizing map (SOM) creates spatially organized intrinsic representations of features.  It belongs to the category of neural networks which use ``competitive learning", or ``self-organization".  It is a sheet-like artificial neural network in which the cells become tuned to various input patterns through an unsupervised learning process.  Only a neighborhood of cells give an active response to the current input sample.  The spatial location or coordinates of cells in the network correspond to different modes of the input distribution. The self-organizing map is also a form of vector quantization (VQ).  The purpose of VQ is to approximate a continuous probability density function $p(\bm{x})$ of input vectors $\bm{x}$ using a finite number of codebook vectors, $\bm{m}_i$, $i=1,2,\dots,k$.  After the ``codebook" is chosen, the approximation of $\bm{x}$ involves finding the reference vector, $\bm{m}_c$ closest to $\bm{x}$.  The ``winning" codebook vector for sample $\bm{x}$ satisfies the following:
	\begin{align*}
		|| \bm{x} - \bm{m}_c|| &= \min_{i}|| \bm{x} - \bm{m}_{i} ||
	\end{align*}	
	\noindent
	
	The algorithm operates by first initializing a spatial lattice of codebook elements (also called ``units"), where each unit's representative is in $\bm{m}_i \in \mathbb{R}^{D}$ where $D$ is the dimensionality of the input samples $\bm{x}$.  The training process proceeds as follows.  A random sample is selected and presented to the network and each unit determines its activation by computing dissimilarity.	 The unit who's codebook vector provides the smallest dissimilarity is referred to as the \textit{winner}.
	
	\begin{align*}
		c(t) = \argmin_{i} d(\bm{x}(t),\bm{m}_{i}(t))
	\end{align*}
	\noindent
	
	Both the winning vector and all vectors within a neighborhood of the winner are updated toward the sample by 
	
	\begin{align*}
		\bm{m}_{i}(t+1) = \bm{m}_{i}(t) + \alpha(t) \cdot h_{ci}(t) \cdot [ \bm{x}(t) - \bm{m}_{i}(t) ] 
	\end{align*}
	\noindent
	where $\alpha(t)$ is a learning rate which decreases over time and $h_{ci}(t)$ is a neighborhood function which is typically unimodal and symmetric around the location of the winner which monotonically decreases with increasing distance from the winner.  A radial basis kernel is typically chosen for the neighborhood function as 
	
	\begin{align*}
		 h_{ci}(t) = \exp{\left( -\frac{||\bm{r}_{c} - \bm{r}_i ||^{2}}{2 \sigma^{2}(t)} \right)}
	\end{align*}
	\noindent
	where the top expression represents the Euclidean distance between units $c$ and $i$ with $\bm{r}_{i}$ representing the 2-D location of unit $i$ in the lattice.  The neighborhood kernel's bandwidth is typically initialized to a value which covers a majority of the input space and decreases over time such that solely the winner is adapted toward the end of the training procedure. \\
	
	\noindent
	The SOM essentially performs density estimation of high-dimensional data and represents it in a 2 or 3-D representation.  At test time, the dissimilarity between each unit in the map and an input sample are computed.  This dissimilarity can be used to effectively detect outliers, thus making the SOM a robust method which can provide confidence values for it's representation abilities. \\
	
	In this paper, the SOM was applied to speech recognition, but made note of previous uses in robotics, control of diffusion processes, optimization problems, adaptive telecommunications, image compression, sentence understanding, and radar classification of sea-ice. \\
	
 	
 
	 \paperentry{Rauber2002GHSOM}
	 {The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data}
	 {Manifold_Representation_Learning/CHL/Rauber2002GHSOM.pdf}
	 {The Growing Hierarchical Self-organizing Map (GHSOM) is an extension of the classical SOM.  It is an artificial neural network with a hierarchical architecture, composed of individually growing SOMs.  Layer 0 is composed of a single neuron representing the mean of the training data.  A global stopping criteria is developed as a fraction of the mean quantization error.  This means that all units must represent their respective subsets of data an a MQE smaller than a fraction of the 0 layer mean quantization error.  For all units not satisfying this criteria, more representation is required for that  area of the feature  space and additional units are added.  After a particular number of training iterations, the quantization errors are computed and the unit with the highest error is selected as the \textit{error unit}. The most dissimilar neighbor of the error unit is chosen is and a row/ column of nodes is injected between them.  The growth process continues until a second stopping criteria is met.  Any units still not satisfying the global criteria are deemed to need extra representation.  Child map are initialized below these units and trained with the subset of data mapped to its parent node.\\
	 	
 	\noindent
 	In conclusion, the GHSOM is a growing self-organizing map architecture which has the ability to grow itself until the feature space  is adequately represented.  For areas of the space needing a more specific level of granularity, a hierarchical structure is imposed to ``fill-in" areas of high density. \\
 	
 	\noindent
 	The GHSOM has been applied to the areas of finance, computer network traffic analysis, manufacturing and image analysis (Palomo 2017).
	}
 
	 \paperentry{Chiang1997HandWrittenWords}
	 {Hybrid fuzzy-neural systems in handwritten word recognition}
	 {Manifold_Representation_Learning/CHL/Chiang1997HandWrittenWords.pdf}
	 {}
	
	\paperentry{Frigui2009LandmineSOM}
	{Detection and Discrimination of Land Mines in Ground-Penetrating Radar Based on Edge Histogram Descriptors and a Possibilistic $K$-Nearest Neighbor Classifier}
	{Manifold_Representation_Learning/CHL/Frigui2009LandmineSOM.pdf}
	{}
	
	\paperentry{Fritzke1995GrowingNeuralGas}
	{A Growing Neural Gas Network Learns Topologies}
	{Manifold_Representation_Learning/CHL/Fritzke1995GrowingNeuralGas.pdf}
	{Abstract: An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the "neural gas" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation. \\
		
	\noindent
	In contrast to SOMs and ``growing cell structures", which can project data onto non-linear subspaces which are chosen \textit{a priori}, the GNG is able to adapt its topology to match that of the input data distribution.  The growing process continues until a pre-defined level of quantization error has been reached. \\
	
	\noindent
	The base algorithm is outlined in Palomo (2017), \textit{Growing Hierarchical Neural Gas Self-Organizing Network}.		
	}
	
	\paperentry{Palomo2017GHNG}
	{The Growing Hierarchical Neural Gas Self-Organizing Neural Network}
	{Manifold_Representation_Learning/CHL/Palomo2017GHNG.pdf}
	{}
	\newline
	Abstract: The growing neural gas (GNG) self-organizing neural network stands as one of the most successful examples of unsupervised learning of a graph of processing units. Despite its success, little attention has been devoted to its extension to a hierarchical model, unlike other models such as the self-organizing	map, which has many hierarchical versions. Here, a hierarchical GNG is presented, which is designed to learn a tree of graphs.  Moreover, the original GNG algorithm is improved by a distinction between a growth phase where more units are added until no significant improvement in the quantization error is obtained, and a convergence phase where no unit creation is	allowed. This means that a principled mechanism is established	to control the growth of the structure. Experiments are reported, which demonstrate the self-organization and hierarchy learning abilities of our approach and its performance for vector quantization	applications.  Experiments were performed in structure learning, color quantization, and video sequence clustering. \\
		
	\noindent
	The aim of this method was to improve the adaptation ability of the Growing Hierarchical Self-Organizing Map proposed by Rauber (2002).  This was to be done through the extension of the Growing Neural Gas, which disposes of the fixed lattice topology enforced by the SOM.  Addtionally, the GNG learns a dynamic graph with variable numbers of neurons and connections.  The graph represents the input data in a more plastic and flexible way than the fixed-topology map.  	\\
	
	\noindent
	All clustering methods that learn a hierarchical structure have advantages even when used for non-hierarchical data. The learned hierarchical structure can be pruned at several levels, which yields alternative representations of the input data set at different levels of detail. This can be used to visualize a data set in coarser	or more detailed way. For vector quantization applications, the different pruning levels correspond to smaller or larger codebooks, so that a balance can be attained between the size of the codebook and the quantization error within the same hierarchical structure.\\
	
	\noindent
	The growing hierarchical neural gas (GHNG) model is defined as a tree of self-organizing graphs.  Each graph is made of a variable number of neurons or processing units, so that its size can grow or shrink during learning. In addition, each graph is the child of a unit in the upper level, except for the top	level (root) graph.  The training procedure is described by the following: \\
	
	\noindent
	Each graph begins with $H \geq 2$ units and one or more undirected connections between them.  Both the units and connections can be created and destroyed during the learning process. It is also not necessary that the graph is connected.  Let the training set be denoted as $\mathcal{S}$ with $\mathcal{S} \subset \mathbb{R}^{D}$, where $D$ is the dimensionality of the input space.  Each unit $i\in \{1, \dots, H \}$ has an associated prototype $\bm{w}_{i} \in \mathbb{R}^{D}$ and an error variable $e_i \in \mathbb{R}$, $e_i \geq 0$.  Each connection has an associated age, which is a nonnegative integer.  The set of connections will be notetd as $A \subseteq \{1, \dots,H \} \times \{1, \dots, H \}$.  The learning mechanism for the GHNG is based on the original GNG, but includes a novel procedure to control the growth of the graph.  First, a growth phase is performed where the graph is allowed to enlarge until a condition is met, which indicates that further growing would provide no significant improvement in the quantization error.  After that, a convergence phase is executed where no unit creation is allowed in order to carry out a fine tuning of the graph.  the leraning algorithm is provided in the following steps. \\
	
	\begin{enumerate}
		\item Start with two units ($H=2$) joined by a connection.  Each prototype is initialized to a sample drawn at random from $\mathcal{S}$.  The error variables are initialized to zero.  The age of the connection is initialized to zero.
		\item Draw a training sample $\bm{x}_{t} \in \mathbb{R}^{D}$ at random from $\mathcal{S}$.
		\item Find the nearest unit $q$ and second nearest unit $s$ in terms of Euclidean distance
		\begin{align*}	
		q &= \argmin_{i\in \{1,\dots,H \}} ||\bm{w}_{i}(t) - \bm{x}(t)  || \\
		s &= \argmin_{i\in \{1,\dots,H \} - \{q\} } ||\bm{w}_{i}(t) - \bm{x}(t)  ||
		\end{align*}
		\item Increment the age of all edges departing from $q$
		\item  Update the winning unit's error variable, $e_{q}$
		\begin{align*}
		e_{q}(t+1) = e_{q}(t) + || \bm{w}_q(t) - \bm{x}_{t} ||
		\end{align*}
	\end{enumerate}

	\noindent
	I believe the author's experimental approach did not take advantage of the method's strengths.  The author's only demonstrated experiments in vector quantization, and used corresponding metrics.  This method could be used to represent manifold  topology of differing dimensionality. This could be useful in HSI imagery, for example where different environment patches require manifold representations of various dimensionality.  Additionally, this could potentially be used to handle the sensor fusion problem with sensor loss/ drop-out. \\
	
	\paperentry{Sun2016GNGMotionDetection}
	{Online growing neural gas for anomaly detection in changing surveillance scenes}
	{Manifold_Representation_Learning/CHL/Sun2016GNGMotionDetection.pdf}
	{}
	
	\paperentry{LopezRubio2011GHPGraphs}
	{Growing Hierarchical Probabilistic Self-Organizing Graphs}
	{Manifold_Representation_Learning/CHL/LopezRubio2011GHPGraphs.pdf}
	{}
	
	
	\paperentry{Palomo2016GrowingNeuralForest}
	{Learning Topologies with the Growing Neural Forest}
	{Manifold_Representation_Learning/CHL/Palomo2016GrowingNeuralForest.pdf}
	{}
	
	\subsection{Deep Learning}
	
		\paperentry{Goodfellow2016DeepLearning}
		{Deep Learning}
		{Manifold_Representation_Learning/Autoencoders/Goodfellow2016DeepLearning.pdf}
		{}
		
		\paperentry{Haykin2009NeuralNetworks}
		{Neural networks and learning machines}
		{Manifold_Representation_Learning/Autoencoders/Haykin2009NeuralNetworks.pdf}
		{}
		
		\paperentry{Dai2017VariationalAutoencoder}
		{dden Talents of the Variational Autoencoder}
		{Manifold_Representation_Learning/Autoencoders/Dai2017VariationalAutoencoder.pdf}
		{}
		
		\paperentry{Rojas1996AssociativeNetworks}
		{Associative Networks}
		{Manifold_Representation_Learning/Autoencoders/Rojas1996AssociativeNetworks.pdf}
		{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Dissimilarities %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Information Measures}

	\paperentry{Arandjelovic2005FaceRecManifoldDensityDivergence}
	{Face recognition with image sets using manifold density divergence}
	{Manifold_Representation_Learning/Information/Arandjelovic2005FaceRecManifoldDensityDivergence.pdf}
	{}
	
	\paperentry{Wang2008ManifoldManifoldDistance}
	{Manifoldâ€“Manifold Distance and its Application to Face Recognition With Image Sets}
	{Manifold_Representation_Learning/Information/Wang2008ManifoldManifoldDistance.pdf}
	{}
	

	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Manifold Regularization %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Manifold Regularization}
	\paperentry{Tsang2007ManifoldRegularization}
	{Large-Scale Sparsified Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Tsang2007ManifoldRegularization.pdf}
	{}
	
	\paperentry{Ren2017ManRegSAR}
	{Unsupervised Classification of Polarimetirc SAR Image Via Improved Manifold Regularized Low-Rank Representation With Multiple Features}
	{Manifold_Representation_Learning/ManifoldRegularization/Ren2017ManRegSAR.pdf}
	{}
	
	\paperentry{Belkin2006ManReg}
	{Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples}
	{Manifold_Representation_Learning/ManifoldRegularization/Belkin2006ManReg.pdf}
	{}
	
	\paperentry{Ratle2010ManRegHSI}
	{Semisupervised Neural Networks for Efficient Hyperspectral Image Classification}
	{Manifold_Representation_Learning/ManifoldRegularization/Ratle2010ManRegHSI.pdf}
	{}
	
	\paperentry{Li2015ManRegReinforcementLearning}
	{Approximate Policy Iteration with Unsupervised Feature Learning based on Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Li2015ManRegReinforcementLearning.pdf}
	{}
	
	\paperentry{Meng2018ManRegZeroShot}
	{Zero-Shot Learning via Low-Rank-Representation Based Manifold Regularization}
	{Manifold_Representation_Learning/ManifoldRegularization/Meng2018ManRegZeroShot.pdf}
	{}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%% Multiple Instance Learning %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Instance Learning}

	\subsection{Multiple Instance Concept Learning}
	
		\paperentry{Bocinsky2019Thesis}
		{Learning Multiple Target Concepts from Uncertain, Ambiguous Data Using the Adaptive Cosine Estimator and Spectral Match Filter}
		{Multiple_Instance_Learning/Bocinsky2019Thesis.pdf}
		{}
		
		\paperentry{Jiao2017Thesis}
		{Target Concept Learning From Ambiguously Labeled Data}
		{Multiple_Instance_Learning/Jiao2017MIHE_Thesis.pdf}
		{}
		
		\paperentry{Mccurley2019SPIEWEMIComparison}
		{Comparison of hand-held WEMI target detection algorithms}
		{Multiple_Instance_Learning/Mccurley2019SPIEWEMIComparison.pdf}
		{}
		
		\paperentry{Bocinsky2019SPIEMIACEInitialization}
		{Investigation of initialization strategies for the Multiple Instance Adaptive Cosine Estimator}
		{Multiple_Instance_Learning/Bocinsky2019SPIEMIACEInitialization.pdf}
		{}
		
		\paperentry{Zare2015MILLandmineEMI}
		{Multiple instance dictionary learning for subsurface object detection using handheld EMI}
		{Multiple_Instance_Learning/Zare2015MILLandmineEMI.pdf}
		{}
		
		\paperentry{Cook2015Thesis}
		{Task driven extended functions of multiple instances (TD-eFUMI)}
		{Multiple_Instance_Learning/Cook2015Thesis.pdf}
		{}
		
		\paperentry{Cook2016LandmineTaskDriveneFUMI}
		{Buried object detection using handheld WEMI with task-driven extended functions of multiple instances}
		{Multiple_Instance_Learning/Cook2016LandmineTaskDriveneFUMI.pdf}
		{}
		
		\paperentry{Zare2016MIACE}
		{Multiple Instance Hyperspectral Target Characterization}
		{Multiple_Instance_Learning/Zare2016MIACE.pdf}
		{}
		
		\paperentry{Jiao2017MIHE}
		{Multiple instance hybrid estimator for learning target signatures}
		{Multiple_Instance_Learning/Jiao2017MIHE.pdf}
		{}
		
		\paperentry{Xiao2017SphereMIL}
		{A Sphere-Description-Based Approach for Multiple-Instance Learning}
		{Multiple_Instance_Learning/Xiao2017SphereMIL.pdf}
		{}
		
		\paperentry{Cheplygina2019MILSurvey}
		{Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis}
		{Multiple_Instance_Learning/Cheplygina2019MILSurvey.pdf}
		{}
		
		\paperentry{Li2017SmoothMIL}
		{Cross-validated smooth multi-instance learning}
		{Multiple_Instance_Learning/Li2017SmoothMIL.pdf}
		{}
		
		\paperentry{Cheplygina2016DissimilarityEnsemblesMIL}
		{Dissimilarity-Based Ensembles for Multiple Instance Learning}
		{Multiple_Instance_Learning/Cheplygina2016DissimilarityEnsemblesMIL.pdf}
		{}
		
		\paperentry{Wang2017DiversityMILActiveLearning}
		{Incorporating Diversity and Informativeness in Multiple-Instance Active Learning}
		{Multiple_Instance_Learning/Wang2017DiversityMILActiveLearning.pdf}
		{}
		
		\paperentry{Hajimirsadeghi2017MIClassificationMarkovNetworks}
		{Multi-Instance Classification by Max-Margin Training of Cardinality-Based Markov Networks}
		{Multiple_Instance_Learning/Hajimirsadeghi2017MIClassificationMarkovNetworks.pdf}
		{}
		
		\paperentry{Du2016MIChoquetIntegralFusion}
		{Multiple Instance Choquet integral for classifier fusion}
		{Multiple_Instance_Learning/Du2016MIChoquetIntegralFusion.pdf}
		{}
		
		\paperentry{Ilse2018AttentionBasedDeepMIL}
		{Attention-based Deep Multiple Instance Learning}
		{Multiple_Instance_Learning/Ilse2018AttentionBasedDeepMIL.pdf}
		{}
		
		
		\paperentry{Karem2016MILMultiplePositiveAndNegativeConcepts}
		{Multiple Instance Learning with multiple positive and negative target concepts}
		{Multiple_Instance_Learning/Karem2016MILMultiplePositiveAndNegativeConcepts.pdf}
		{}
		
		\paperentry{Xiao2017MIOrdinalRegression}
		{Multiple-Instance Ordinal Regression}
		{Multiple_Instance_Learning/Xiao2017MIOrdinalRegression.pdf}
		{}
		
		\paperentry{Gao2017CountGuidedWeaklySupervisedLocalization}
		{{C-WSL:} Count-guided Weakly Supervised Localization}
		{Multiple_Instance_Learning/Gao2017CountGuidedWeaklySupervisedLocalization.pdf}
		{}
		
		\paperentry{Li2017MultiviewMIL}
		{Multi-View Multi-Instance Learning Based on Joint Sparse Representation and Multi-View Dictionary Learning}
		{Multiple_Instance_Learning/Li2017MultiviewMIL.pdf}
		{}
		
		\paperentry{Cao2016VehicleDetectionMIL}
		{Weakly Supervised Vehicle Detection in Satellite Images via Multi-Instance Discriminative Learning}
		{Multiple_Instance_Learning/Cao2016VehicleDetectionMIL.pdf}
		{}
		
		\paperentry{Dietterich1996AxisParallelRectangles}
		{Solving the multiple instance problem with axis-parallel rectangles}
		{Multiple_Instance_Learning/Dietterich1996AxisParallelRectangles.pdf}
		{}
		
		\paperentry{Maron1998DiverseDensity}
		{A Framework for Multiple-instance Learning}
		{Multiple_Instance_Learning/Maron1998DiverseDensity.pdf}
		{}
		
		\paperentry{Maron1998MILSceneClassification}
		{Multiple-Instance Learning for Natural Scene Classification}
		{Multiple_Instance_Learning/Maron1998MILSceneClassification.pdf}
		{}
		
		\paperentry{Carbonneau2016MILSurvey}
		{Multiple Instance Learning: {A} Survey of Problem Characteristics and Applications}
		{Multiple_Instance_Learning/Carbonneau2016MILSurvey.pdf}
		{}
		
		\paperentry{Zhang2002EMDD}
		{EM-DD: An Improved Multiple-Instance Learning Technique}
		{Multiple_Instance_Learning/Zhang2002EMDD.pdf}
		{}
		
		\paperentry{Zare2015eFUMI}
		{Extended Functions of Multiple Instances for target characterization}
		{Multiple_Instance_Learning/Zare2015eFUMI.pdf}
		{}
		
		\paperentry{Jiao2018MIHE2}
		{Multiple instance hybrid estimator for hyperspectral target characterization and sub-pixel target detection}
		{Multiple_Instance_Learning/Jiao2018MIHE2.pdf}
		{}
		
		
		
		
	
	\subsection{Multiple Instance Classification}
	
		\paperentry{Cao2016VehicleDetectionMIL}
		{Weakly Supervised Vehicle Detection in Satellite Images via Multi-Instance Discriminative Learning}
		{Multiple_Instance_Learning/Cao2016VehicleDetectionMIL.pdf}
		{}
		
	
	\subsection{Multiple Instance Regression}
	
		\paperentry{Trabelsi2018FuzzyClusteringMILRegression}
		{Fuzzy and Possibilistic Clustering for Multiple Instance Linear Regression}
		{Multiple_Instance_Learning/Trabelsi2018FuzzyClusteringMILRegression.pdf}
		{}
		
		\paperentry{Ruiz2018MIDynamicOrdinalRegression}
		{Multi-Instance Dynamic Ordinal Random Fields for Weakly Supervised Facial Behavior Analysis}
		{Multiple_Instance_Learning/Ruiz2018MIDynamicOrdinalRegression.pdf}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}
		
		\paperentry{}
		{}
		{Multiple_Instance_Learning/}
		{}


	\subsection{Applications}
	
		\paperentry{}
		{}
		{}
		{}
		


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fusion}
	
	\subsection{Classical Approaches}
		\subsubsection{General Approach}
			\paperentry{Mohandes2018ClassifierCombinationTechniquesReview}
			{Classifiers Combination Techniques: A Comprehensive Review}
			{Fusion/Reviews/Mohandes2018ClassifierCombinationTechniquesReview.pdf}
			{} \newline
			\textbf{Should reference this paper for hierarchical representations of classifier combination methods.  Many good diagrams.  Combining expert opinions before making decisions can substantially increase the reliability of critical application systems such as medical diagnosis, security, and so on.}  Evidence from multiple classifiers can be combined on the data, feature, or decision level.  Classifier ensemble combination methods are known under many different names: multi-classifier combination, multi-classifier fusion, mixture of experts, and ensemble based classification, to name a few.  Since the most recent review, 8 years earlier, a few more methods for classifier combination were introduced, including: a signal strength based combination approach, a novel Bayes voting strategy, a modified weighted averaging technique using graph-theoretic clustering, a neural network based approach for training combination rules, weighted feature  combination, and hierarchical fuzzy stack generalization. \\
			\noindent
			Typical classifier combination algorithms begin with a set of scores from individual classifiers and produce a combined score for each class along with a final class label.  The problem then generalizes to finding a combination function with accepts a K dimensional score vector from each of the M classifiers, then produces a single, final classification score representing the selected class.  The M classifiers could be identical but use different feature sets as inputs, or use different parameter sets.  Alternatively, the classifiers could be different by nature but use the same set of input features.  The important distinction is that individual classifiers should not make identical erroneous decisions on the same observation set, i.e. they should provide complementary information. \\
			\noindent
			Most classifier combination techniques assume independence between features. \\
			\noindent Adaptive techniques for classifier fusion are mainly based on evolution or artificial intelligence algorithms.  They include neural network combination strategies and genetic algorithms as well as fuzzy set theory. Fusion using ANNs allows for non-linear combination of classifier outputs.  Adaptive methods also include adaptive weighting, associative switching, adaptive fuzzy integrals, mixture of local experts and hierarchical MLE.  Adaptive classifiers tend to do better than the non-adaptive type.  \\ 
			\noindent
			Describes the differences between bagging, boosting, AdaBoost, and HME. \textit{Bagging} is creating different datasets by bootstrapped versions of the original dataset (sampling with replacement).  In \textit{boosting}, individual classifiers are trained hierarchically to discriminate more complex regions of the feature space.  \textit{AdaBoost} is a variation of boosting which combines the outputs of weak classifiers into a weighted sum representing the final decision.  However, it is sensitive to noisy data and outliers.  Additionally, AdaBoost on the feature level falls victim to the curse of dimensionality.\\  
			\noindent
			Classifier fusion methods have been used on HSI data, to improve  accuracy when sensor data is subjected to drift, handwritten word recognition, sequential data with HMM classifiers only,etc. \\
			\noindent
			\textbf{The literature still lacks a comprehensive performance analysis of techniques for a given application.}  Important research questions still include: classifier post-processing before combination, using meta-heuristic algorithms to improve performance, such as using optimization algorithms with majority voting, showing the advantages/ disadvantages of using different strategies such as probabilistic, learning, decision based, or evidence based, additionally, finding the optimal number/ type of classifiers to fuse is an open question. \\
			
			
			\paperentry{Ruta2000OverviewClassifierFusionMethods}
			{An Overview of Classifier Fusion Methods}
			{Fusion/Reviews/Ruta2000OverviewClassifierFusionMethods.pdf}
			{}
			\newline
			``The objective of all decision support systems (DSS) is to create a model, which given a minimum amount of input data/information, is able to produce correct decisions."  ``the solution might be just to combine existing,  well performing methods, hoping that better results will be achieved.  Such fusion of information seems to be worth applying in terms of uncertainty reduction. Each of individual methods produces some errors, not mentioning that the input information	might be corrupted and incomplete. However, different methods performing on different data should	produce different errors, and assuming that all	individual methods perform well, combination of such multiple experts should reduce overall classification	error and as a consequence emphasize correct outputs."  ``Fusion of data/information can	be carried out on three levels of abstraction closely connected with the flow of the classification process: data level fusion, feature level fusion, and classifier fusion"  This paper focused on the later method of classifier fusion.  This process can essentially be categorized into two eruditions.  The first methods put emphasis on the classifier structure and do not do anything with the outputs until the combination process finds the best classifier or a selected group of classifiers.  Then their outputs are taken as a final decision or used for further processing.  The second category operates primarily on classifier outputs and can be further divided.  \newline  There are three possibles types of output labels generated by individual classifiers.  Crisp labels provide the lowest amount of information for fusion, as no information about potential alternatives is available.  Some additional information can be gleaned from labels in the form of class rankings.  However, fusion methods operating on classifiers with soft/fuzzy outputs can be expected to produce the greatest improvement in classification performance.  (Connor Note: This is valuable in terms of outlier rejection as well!).  The following explains an overview of classifier fusion methods operating on single class labels, class rankings, and fuzzy measures, respectively. \\ 
			\noindent
			\textbf{Methods operating on classifiers:}  \newline \textit{Dynamic Classifier Selection} (DCS) methods replect the tendency to extract  a single best classifier instead of mixing many different classifiers, by attempting to determing the single classifier which is most likely to produce the correct classification label for an input sample.  Only the output of the selected classifier is taken as a final decision.  The classifier selection process includes a partitioning of the input samples.  A classifier is is for each partition is selected locally.  All DCS methods rely on strong training data and by choosing only locally best classifier.  \textbf{They potentially lose some useful information from other well-performing classifiers.} Classifiers and their combination functions are typically organized in parallel and simultaneously and separately get their outputs as in input for a combination function.  A more reasonable approach, however, is \textbf{to organize all classifiers into groups and to apply different fusion methods for each group.}  A very important factor for the success of this method is the diversity of classifier types, training data, and methods involved.  \textbf{Any classification improvement may only be achieved if the total information uncertainty is reduced.}   This in turn depends on the diversity of information supporting different classification methods. \textbf{The same goal can be achieved by reduction of errors produced by individual classifiers.}  \textit{Hierarchical Mixture of Experts} (HME) is an example  of a fusion method whose strength comes from classifier's structure.  It is a supervised learning method based on the \textit{divide-and-conquer} principle.  It is organized as a tree-like structure of leaves.  Each leaf represents an individual expert in the network, each of which tries to solve a local supervised learning problem.  The outputs of the elements of the same node are partitioned and combined by the gating network and the total output of the node is given as a convex combination.  The expert networks are trained to increase the posterior probability according to Bayes rule.  A number  of learning algorithms can be applied to tune the mixture model.  \textit{Expectation-Maximization} (EM) is often used to learn the model parameters.  \textit{The HME technique does not seem to be applicable to large-dimensional datasets.} \newline \textbf{Fusing Single Class Labels:} Classifiers producing crisp, single-class labels (SCL) provide the least amount of useful information for the combination process.  The two most common techniques for fusing SCL classifiers are \textit{Generalized Voting} and \textit{Knowledge-Behavior Space} methods.  \\
			\noindent
			\textbf{Voting Methods:} \newline Voting strategies can be applied to a multiple classifier system assuming that each classifier gives a single class label as as output and no training data are available.  While there are many methods for combining these labels, they all lead to the following generalized voting definition.  Let the output of the classifiers form the decision vector $\bm{d} = [ \bm{d}_1, \bm{d}_2, \dots, \bm{d}_n ]^{T}$ where $\bm{d}_i \in \{ c_1, c_2, \dots, c_m, r \} $, $c_i$ denotes the class label of the i-th class and $r$ the rejection of assigning the input sample to any classes.  The binary characteristic function is defined as follows: (Have not input math)
			
%			\begin{align*}
%				B_{j}(c_i) = \begin{cases}
%				1 \text{ if } \bm{d}_j = c_i \\
%				0 \text{ if } \bm{d}_j \neq c_i 
%				\end{cases}
%			\end{align*}

			\noindent
			\textbf{Class Ranking Based Techniques:} \newline
			There are two primary methods for fusion of class rankings.  \textit{Class set reduction} (CSR) attempts to reduce the number of eligible classes by compromising between minimizing the class set size and maximizing the likelihood of inclusion in the true class.  This is typically performed through the \textit{intersection or union of neighborhoods}.  The second popular CSR method is \textit{Class Set Reordering} (CSRR) which tries to improve  the overall rank of the true class through techniques such as the \textit{Highest Rank Method}, \textit{Borda Count}, or \textit{Logistic Regression}.  
			
			\noindent
			\textbf{Soft-Label Classifier Fusion:} \newline
			Soft labels are outputs in the range $[0,1]$ and are typically referred to  as \textit{fuzzy measures}, which cover all known measures of  evidence: probability, possibility, necessity, belief, and plausibility.  Each of these measures are used to describe different dimensions of information uncertainty.  This class of fusion attempts to reduce the level of uncertainty by maximizing suitable measures of evidence.  Common methods for this type of fusion include: Bayesian, Fuzzy Integrals, Dempster-Shaffer Combination, Fuzzy Templates, Product of  Experts, and Artificial Neural Networks.  \textit{Bayesian} methods can be applied under the condition that the outputs of the classifier are expressed as posterior probabilities.  Typical methods of Bayesian fusion include Bayes Average and Bayes Belief Integration. \textit{Fuzzy Integrals} aim at searching for the maximal agreement between  the real possibilities  relating to objective evidence and the expectation, $g$, which defines the level of importance of a subset of sources.  The concept of fuzzy integrals arises from the $\lambda$-fuzzy measure, $g$, developed by Sugeno.  Common methods for Fuzzy Integration include the Sugeno Fuzzy Integral, Choquet Fuzzy Integral, and Webster Fuzzy Integral.  \textit{Product of  Experts} combines different probabilistic models of the same data by performing a weighted average of individual probability distributions. \\
			
			
			
			
			\paperentry{Tulyakov2008ReviewClassifierCombinationMethods}
			{Review of Classifier Combination Methods}
			{Fusion/Reviews/Tulyakov2008ReviewClassifierCombinationMethods.pdf}
			{}\newline
			
			
			\paperentry{hackett1990multisensorfusion}
			{Multi-sensor fusion: a perspective}
			{Fusion/Reviews/hackett1990multisensorfusion.pdf}
			{}\newline
			\textbf{Multi-Sensor fusion deals with the combination of complementary and sometimes competing sensor data into a reliable estimate of the environment to achieve an output which is better than the modalities, individually.}  Multi-sensor fusion has been used  in target recognition, autonomous robot navigation, automatic manufacturing, scene segmentation, sensor modeling, and object recognition.  \textit{Sensor fusion combines the outputs from two or more devises that retrieve a particular property of the environment.}  Each sensor's measurements are, in general, imprecise and contain errors and uncertainties, so the consensus of multiple sensors measuring the same property can reduce uncertainty and reduce measurement ambiguity. Every sensor modality is sensitive to a different property of the environment; it is necessary to use multiple sensors in order to address these sensitivities. \textit{Sensor fusion deals with the selection of a proper model  for each sensor, and identification of an appropriate fusion method.}  There are several methods for combining multiple data sources.  A few are: deciding, guiding, averaging, Bayesian statistics, and integration.  Deciding is the use of a particular data source during a certain time of the fusion process, usually based on some confidence measure.  Averaging is the weighted combination of several data sources. This type of fusion ensures all sensors contribute to the fusion process, but not all to the same degree.  Guiding is the use of one or more sensors to focus the attention of another sensor on some part of the scene.  Integration is the  delegation of carious sensors to particular tasks, thus eliminating redundancy in sensor measurements.  The most simple method of fusion uses raw data of the same property obtained by multiple sensors of the same type.  Multi-sensor integration is the use of several sensors in a sequential manner. \\
			\noindent
			\textbf{Data from different sensors must be put into equivalent forms to allow for fusion.  In order for data from multiple sources to be fused, there must be some method to relate data points from one sensor with corresponding data points from the other sensors.}  The \textit{registered} data points allow for easy gathering of sensor information about one particular point in the scene.  \\
			\noindent
			Fusion methods can be broadly classified into two categories, \textit{direct} and \textit{indirect}.  Direct fusion combines raw sensor measurements while indirect methods transform the sensor data to be fused. \\
			\noindent
			\textbf{Before sensor measurements can be combined, we must ensure that the measurements represent the same physical entity. Therefore, we need to check the consistency of sensor measurements.}  One such method for checking measurement consistency is the \textit{Mahalanobis} distance.\\
			\noindent
			\textbf{Since each sensor is sensitive to a different modality, multiple sensors not only can provide multiple views of objects, but they can also impose more constraints to reduce the search space during matching.}\\
			
			\paperentry{zhang2010multisourceremotingsensingfusion}
			{Multi-source remote sensing data fusion: Status and trends}
			{Fusion/Reviews/zhang2010multisourceremotingsensingfusion.pdf}
			{}\\
			\textbf{Remote sensing data fusion, as one of the most commonly used techniques for fusion, aims to integrate the information acquired with different spatial and spectral resolutions from sensors mounted on satellites, aircraft and ground platforms to produce fused data that contains more detailed information than each of the sources, individually.}  \textbf{Fusing remotely sensed data, especially multi-source data, remains challenging due to reasons such as landscape complexity, temporal and spectral variations, and accurate data co-registration.} \textbf{\textit{Pixel level} fusion is the combination of raw data from multiple sources into single resolution data, which are expected to be more informative and synthetic than either of the input data or reveal the changes between data sets acquired at different times. \textit{Feature level} fusion extracts various features, e.g. edges, corners, lines, texture parameters, etc., from different data sources and then combines them into one or more feature maps that may be used instead of the original data for further processing. This is particularly important when the number of available spectral bands becomes so large that	it is impossible to analyze each band separately. Methods applied to extract features usually depend on the characteristics of the individual source data, and therefore may be different if the data sets used are heterogeneous. Typically, in image processing, such fusion requires a precise (pixel-level) registration of the available images. Feature maps thus obtained are then used as input to pre-processing for image segmentation or change detection. \textit{Decision level} fusion combines the results from multiple algorithms to yield a final fused decision. When the results from different algorithms are expressed as confidences	(or scores) rather than decisions, it is called soft fusion; otherwise, it is called hard fusion. Methods of decision fusion include voting methods, statistical methods and fuzzy logic based
methods.} \textbf{PROVIDES A GREAT DESCRIPTION OF LiDAR USE DESCRIPTION FROM XIAOXIAO'S DISSERTATION.} \\
			\noindent 
			An undesirable property when applying pixel-level fusion techniques to the fusion of SAR and optical images is that either spectral features of the optical imageery or the microwave backscattering information is destroyed, or both simultaneously.  \\
			\noindent
			Applications: satellite Earth observations, computer vision, medical image processing, defense security, land use classification, Digital Surface Modeling (DSM), Digital Elevation Modeling (DEM), environmental monitoring, road mapping. archeology, building detection and reconstruction, etc. \\
			\noindent
			\textbf{For specific purposes, ancillary and terrestrial meta-data such as laser-scanners, GIS data, web-sensors, field survey data, economic consensus data, and meteorological data me be combined with remote sensing data to improve the performance of data fusion. }
		
	
		\subsubsection{Hierarchical Mixture of Experts}
		
			\paperentry{Jordan1993HME}
			{Hierarchical mixtures of experts and the EM algorithm}
			{Fusion/HME/Jordan1993HME.pdf}
			{}
		
			\paperentry{Yuksel2012TwentyYearsMixtureofExperts}
			{Twenty Years of Mixture of Experts}
			{Fusion/HME/Yuksel2012TwentyYearsMixtureofExperts.pdf}
			{}
			
			\paperentry{Beyer2009HeterogeneousMixtureOfExperts}
			{Heterogeneous mixture-of-experts for fusion of locally valid knowledge-based submodels}
			{Fusion/HME/Beyer2009HeterogeneousMixtureOfExperts.pdf}
			{}
			
			\paperentry{Shazeer2017SparselyGatedMixtureOfExperts}
			{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}
			{Fusion/HME/Shazeer2017SparselyGatedMixtureOfExperts.pdf}
			{}
		
		
		\subsubsection{Choquet Integral}
		
			\paperentry{Du2017Thesis}
			{Multiple Instance Choquet Integral For MultiResolution Sensor Fusion}
			{Fusion/Du2017Thesis}
			{}
		
			\paperentry{Smith2017ChoquetIntegralLandmine}
			{Aggregation of Choquet integrals in GPR and EMI for handheld platform-based explosive hazard detection}
			{Fusion/Choquet/Smith2017ChoquetIntegralLandmine.pdf}
			{}
			
			\paperentry{Smith2017GeneticProgrammingChoquetIntegral}
			{Genetic programming based Choquet integral for multi-source fusion}
			{Fusion/Choquet/Smith2017GeneticProgrammingChoquetIntegral.pdf}
			{}
		
			\paperentry{Du2019MIChoquetIntegral}
			{Multiple Instance Choquet Integral Classifier Fusion and Regression for Remote Sensing Applications}
			{Fusion/Choquet/Du2019MIChoquetIntegral.pdf}
			{}
		
			\paperentry{Anderson2017BinaryFuzzyMeasureChoquetIntegral}
			{Binary fuzzy measures and Choquet integration for multi-source fusion}
			{Fusion/Choquet/Anderson2017BinaryFuzzyMeasureChoquetIntegral.pdf}
			{}
			
			\paperentry{Du2018MultiResolutionSensorFusion}
			{Multi-Resolution Multi-Modal Sensor Fusion For Remote Sensing Data With Label Uncertainty}
			{Fusion/Choquet/Du2018MultiResolutionSensorFusion.pdf}
			{}
			
			\paperentry{Gader2004ChoquetIntegralLandmine}
			{Multi-sensor and algorithm fusion with the Choquet integral: applications to landmine detection}
			{Fusion/Choquet/Gader2004ChoquetIntegralLandmine.pdf}
			{}
			
			
			
		
		\subsubsection{Deep Learning}
		
			\paperentry{Jian2019AEInfraredVisibleFusion}
			{A Symmetric Encoder-Decoder with Residual Block for Infrared and Visible Image Fusion}
			{Fusion/DeepLearning/Jian2019AEInfraredVisibleFusion.pdf}
			{}
		
		\subsubsection{Graph-Based}
		
			\paperentry{Vivar2019MultiModalGraphFusion}
			{Multi-modal Graph Fusion for Inductive Disease Classification in Incomplete Datasets}
			{Fusion/GraphBased/Vivar2019MultiModalGraphFusion.pdf}
			{}

	\subsection{Co-registration}
	
		\paperentry{Dawn2010SurveyRemoteSensingImageRegistration}
		{Remote Sensing Image Registration Techniques: A Survey}
		{Fusion/Dawn2010SurveyRemoteSensingImageRegistration.pdf}
		{}
	
		\paperentry{Brigot2016CoregistrationForestRemoteSensingImages}
		{Adaptation and Evaluation of an Optical Flow Method Applied to Coregistration of Forest Remote Sensing Images}
		{Fusion/Brigot2016CoregistrationForestRemoteSensingImages.pdf}
		{}
		
		\paperentry{Zitova2003SurveyImageRegistrationMethods}
		{Image registration methods: a survey}
		{Fusion/Reviews/Zitova2003SurveyImageRegistrationMethods.pdf}
		{}\\
		\textbf{Image registration is the process of overlaying images (two or more) of the same scene taken at different times, from different viewpoints, and/or by different sensors.}  Image registration can broadly be broken into two categories, \textit{area-based} and \textit{feature-based} and according to four basic steps of image registration: \textit{feature detection, feature matching, mapping function design, and image transformation and resampling.} \\
		\noindent
		Image acquisition can be divided into four methodologies.  \textit{Different viewpoints (multiview analysis)} involves collecting images of the same scene from different viewpoints.  \textit{Different times (multitemporal analysis)} collects images of the same scene acquired at different times, often on a regular basis.  The aim  is to find and  evaluate changes in the scene over time.  \textit{Differnt sensors (multimodal analysis)} collects images of the same scene through different sensors.  The aim is to integrate information obtained from different source streams to gain a more complex and detailed scene representation.  \textit{Scene to model registration} involves images of a scene and a corresponding model (such as digital elevation models (DEM)).  The objective is to localize the acquired image in the scene/ model and/ or compare them.\\
		\noindent
		\textit{Feature detection} involves finding salient and distinctive points of interest in an image (closed-boundary regions, edges, contours, corners, line intersections, etc.)  These features can be represented by their point representatives (centers of gravity, line endings, distinctive points) called \textit{control points} (CP).  Physically corresponding features can be dissimilar due to different imaging conditions and/or due to differing spectral sensitivities among sensors.  Features should be sufficiently robust and stable as to not be influenced by unexpected variations a nd noise. \textit{Feature  matching} involves pairing corresponding features  between the query and reference image.  Various features and (dis)similarity measures along with spatial relationships are employed during this process.  Classical area-based feature matching involves methods such as cross-correlation (CC) (which does not incorporate any structural analysis) and mutual information (MI) which measures the statistical dependency between two datasets.  Other feature-based matching methods include graph matching, clustering, Iterative Closest Points (ICP) and more.  Features should be invariant , unique, stable, and independent.  Area-based matching is preferable when images do not have many prominent details/ distinctive information.  However, they rely on the images having similar intensity functions.  Feature-based matching are typically applied when local structure information is more significant than the information carried by the image intensities.  These methods allow images of completely different natures to be registered (i.e. multi-modal).  However, the drawback of this class of matching is that the respective featues might be difficult to detect between images.   \textit{Transform model estimation} involves selecting the type and parameters of a \textit{mapping function} to align the sensed image with the reference. Mapping  models are either Global (which use all CPs) or Local (which decompose the image into patches and the function parameters are locally dependent to a patch.)  Global methods preserve shape (curvatures, angles, etc), while local methods allow for local image transformations, thus addressing local deformations.  Types of mapping functions include: bivariate polynomials, radial basis functions, elastic, fluid, diffusion based, level sets, and optical flow registration.  \textit{Image resampling and transformation} involves transforming an image by means of the mapping function.  The main limitation of image resampling is computational complexity, especially for high dimensional images. \\
		\noindent
		\textbf{Accuracy evaluation is a non-trivial problem, partially because errors can evolve into the registration process in each of its stages and partially because it is difficult to distinguish between registration inaccuracies and actual physical differences.}  Accuracy error is usually measured by local \textit{local error} (displacement of CP coordinates due to inaccurate detection), \textit{matching error} (measured by the number of false matches when establishing the correspondence between CP candidates), and \textit{alignment error} (the difference between the mapping model used for registration and the actual between-image geometric distortion).
		
		\noindent
		Applications: Image mosaicing, creating super-resolution images, integrating information into geographic information systems (GIS), in medicine, cartography, computer vision, classification, environmental monitoring \\
		
	
		\subsubsection{Geocoding}
	
		\subsubsection{Similarity Measures}
	
		\subsubsection{Transformation, Interpolation, Re-sampling}
	
		\subsubsection{Conflation}
		
	\subsection{Multi-resolution Fusion}
	
	\subsection{Fusion of Mixed Data Types}
		
		\paperentry{Butenuth2007HeterogeneousGeospatialData}
		{Integration of heterogeneous geospatial data in a federated database}
		{Fusion/Butenuth2007HeterogeneousGeospatialData.pdf}
		{}
		
			
		\paperentry{Guo2019LVAforMultimodalLearningandSensorFusion}
		{Latent Variable Algorithms for Multimodal Learning and Sensor Fusion}
		{Fusion/Guo2019LVAforMultimodalLearningandSensorFusion.pdf}
		{}
		
		\paperentry{Zhang2019FusionHeteroEarthObsClimateZones}
		{Fusion of Heterogeneous Earth Observation Data for the Classification of Local Climate Zones}
		{Fusion/Zhang2019FusionHeteroEarthObsClimateZones.pdf}
		{}
	
	\subsection{Unsorted}
	
	\paperentry{Shen2016SpatioTemporalSpectralFusion}
	{An Integrated Framework for the Spatioâ€“Temporalâ€“Spectral Fusion of Remote Sensing Images}
	{Fusion/Shen2016SpatioTemporalSpectralFusion.pdf}
	{}
	
	
	\paperentry{}
	{}
	{}
	{}
	
	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Outlier Detection %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outlier/ Adversarial Detection}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Army %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Army}

\paperentry{Hall2019ProbabilisticObjectDetection}
{Probabilistic Object Detection: Definition and Evaluation}
{Army/Hall2019ProbabilisticObjectDetection.pdf}
{A probabilistic object detection metric (PDQ - Probability-based Detection Quality) was proposed, thus defining the new task of defining probabilistic object detection metrics.  The ability of deep CNNs to quantify both \textit{epistemic} and \textit{aleatoric uncertainty} is paramount for deployment safety-critical applications.  PDQ aims to measure the accuracy of an image object detector in terms of its label uncertainty and spatial quality.  This is achieved through two steps.  First, a detector must reliably quantify its \textit{semantic uncertainty} by providing full probability distributions over known classes for each detection.  Next, the detectors must quantify spatial uncertainty by reporting \textit{probabilistic bounding boxes}, where the box corners are modeled as normally distributed.  A loss function was constructed to consider both label and spatial quality when providing a final detection measure.  The primary benefit of this method is that it provides a measure for the level of uncertainty in a detection. \\ \\ Is it possible to replace the probabilistic metric with a possibilistic one?  Could this be more effective at handling outlying cases?} 


\paperentry{Mahalanobis2019DSIACCharacterization}
{A comparison of target detection algorithms using DSIAC ATR algorithm development data set}
{Army/Mahalanobis2019DSIACCharacterization.pdf}
{The authors provided an initial characterization of detection performance on the DSIAC dataset using the \textit{Faster R-CNN} algorithm and \textit{Quadratic Correlation Filter (QCF)}.  Performance was evaluated on two datasets, ``easy'' and ``difficult'', where the difficulty was determined by number of pixels on target and local contrast.  Under difficult conditions, the Faster R-CNN algorithm achieved noteworthy performance, detecting as much as 80\% of the targets at a low false alarm rate of 0.01 FA/Square degree.  The dataset was limited by a lack of background diversity. }

\paperentry{Tanner2019DSIACNeuralNet}
{Fundamentals of Target Classification Using Deep Learning}
{Army/Tanner2019DSIACNeuralNet.pdf}
{A shallow CNN was utilized for ATR on the DSIAC MWIR dataset.  The goal of the study was to determine the range of optimal thresholds which would optimally separate the target and clutter class distributions defined by the CNN predictions (output of softmax), as well as determine an upper bound on the number of training images required for optimizing performance.  The shallow CNN (5 layers) and a Difference of Gaussians (DoG), which finds regions of high intensity on dark backgrounds were used to detect and classify targets.  The CNN could correctly classify 96\% of targets as targets and as few as 4\% of clutter as targets.  It was found that the DoG detector failed when the targets were small (long range) or if the overall image was bright (infrared taken during the daytime).  It was also determined that guessing the bright pixels were at the center of the targets was a bad assumption. (The brightest part of a target is not necessarily at its center.)}

\paperentry{Li2018CollaborativeSparsePriorsMultiViewATR}
{Collaborative sparse priors for multi-view ATR}
{Army/Li2018CollaborativeSparsePriorsMultiViewATR.pdf}
{}

\paperentry{Kokiopoulou2009GraphBasedClassificationMultipleObsSets}
{Graph-based classification of multiple observation sets}
{Army/Kokiopoulou2009GraphBasedClassificationMultipleObsSets.pdf}
{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Segmentation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Segmentation}
	\paperentry{Caselles1997GeodesicActiveContours}
	{Geodesic Active Contours}
	{Segmentation/Caselles1997GeodesicActiveContours.pdf}
	{}
	
	\paperentry{Alvarez2010MorphologicalSnakes}
	{Morphological Snakes}
	{Segmentation/Alvarez2010MorphologicalSnakes.pdf}
	{The authors introduce a morphological approach to curve evolution.  Snakes or curves iteratively solve partial differential equations (PDEs).  By doing so, the shape of the snake deforms to minimize the internal and external energies along its boundary.  The internal component keeps the curve smooth, while the external component attaches the curve to image structures such as edges, lines, etc.  Curve evolution is one of the most widely used image segmentation/ object tracking algorithms.  The main contribution of the paper is a new morphological approach to the solution of the PDE associated with snake model evolution.  They approach the solution using only inf-sup operators which has the main benefit of providing simpler level sets (0 outside the contours and 1 inside).}
	
	\paperentry{Marquez_Neila2014MorphologicalCurveBasedEvolution}
	{A Morphological Approach to Curvature-Based Evolution of Curves and Surfaces}
	{Segmentation/Marquez_Neila2014MorphologicalCurveBasedEvolution.pdf}
	{}

\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
